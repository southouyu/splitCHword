2019-09-11 16:21:06,122  INFO org.ansj.util.MyStaticValue 106 - init version.number to env value is : 2.11.8
2019-09-11 16:21:06,141  INFO org.ansj.util.MyStaticValue 106 - init copyright.string to env value is : Copyright 2002-2016, LAMP/EPFL
2019-09-11 16:21:06,150  INFO org.ansj.util.MyStaticValue 106 - init osgi.version.number to env value is : 2.11.8.v20160304-115712-1706a37eb8
2019-09-11 16:21:06,151  INFO org.ansj.util.MyStaticValue 106 - init maven.version.number to env value is : 2.11.8
2019-09-11 16:21:06,168  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/ambiguity.dic
2019-09-11 16:21:06,170 ERROR org.ansj.library.AmbiguityLibrary 117 - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:D:\MyProject\splitCHword\library\ambiguity.dic not found or can not to read, path: library/ambiguity.dic
2019-09-11 16:21:06,314  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/default.dic
2019-09-11 16:21:06,314 ERROR org.ansj.library.DicLibrary 221 - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:D:\MyProject\splitCHword\library\default.dic not found or can not to read, path: library/default.dic
2019-09-11 16:21:07,998  INFO org.ansj.library.DATDictionary 90 - init core library ok use time : 1536
2019-09-11 16:21:08,916  INFO org.ansj.library.NgramLibrary 17 - init ngram ok use time :914
2019-09-11 16:30:11,449  INFO org.ansj.util.MyStaticValue 106 - init version.number to env value is : 2.11.8
2019-09-11 16:30:11,455  INFO org.ansj.util.MyStaticValue 106 - init copyright.string to env value is : Copyright 2002-2016, LAMP/EPFL
2019-09-11 16:30:11,459  INFO org.ansj.util.MyStaticValue 106 - init osgi.version.number to env value is : 2.11.8.v20160304-115712-1706a37eb8
2019-09-11 16:30:11,459  INFO org.ansj.util.MyStaticValue 106 - init maven.version.number to env value is : 2.11.8
2019-09-11 16:30:11,467  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/ambiguity.dic
2019-09-11 16:30:11,468 ERROR org.ansj.library.AmbiguityLibrary 117 - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:D:\MyProject\splitCHword\library\ambiguity.dic not found or can not to read, path: library/ambiguity.dic
2019-09-11 16:30:11,470  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/default.dic
2019-09-11 16:30:11,470 ERROR org.ansj.library.DicLibrary 221 - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:D:\MyProject\splitCHword\library\default.dic not found or can not to read, path: library/default.dic
2019-09-11 16:30:13,375  INFO org.ansj.library.DATDictionary 90 - init core library ok use time : 1701
2019-09-11 16:30:13,935  INFO org.ansj.library.NgramLibrary 17 - init ngram ok use time :552
2019-09-11 16:33:25,986  INFO org.ansj.util.MyStaticValue 106 - init version.number to env value is : 2.11.8
2019-09-11 16:33:25,993  INFO org.ansj.util.MyStaticValue 106 - init copyright.string to env value is : Copyright 2002-2016, LAMP/EPFL
2019-09-11 16:33:25,996  INFO org.ansj.util.MyStaticValue 106 - init osgi.version.number to env value is : 2.11.8.v20160304-115712-1706a37eb8
2019-09-11 16:33:25,996  INFO org.ansj.util.MyStaticValue 106 - init maven.version.number to env value is : 2.11.8
2019-09-11 16:33:26,005  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/ambiguity.dic
2019-09-11 16:33:26,005 ERROR org.ansj.library.AmbiguityLibrary 117 - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:D:\MyProject\splitCHword\library\ambiguity.dic not found or can not to read, path: library/ambiguity.dic
2019-09-11 16:33:26,008  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/default.dic
2019-09-11 16:33:26,008 ERROR org.ansj.library.DicLibrary 221 - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:D:\MyProject\splitCHword\library\default.dic not found or can not to read, path: library/default.dic
2019-09-11 16:33:27,706  INFO org.ansj.library.DATDictionary 90 - init core library ok use time : 1511
2019-09-11 16:33:28,898  INFO org.ansj.library.NgramLibrary 17 - init ngram ok use time :1185
2019-09-11 16:34:01,687  INFO org.ansj.util.MyStaticValue 106 - init version.number to env value is : 2.11.8
2019-09-11 16:34:01,695  INFO org.ansj.util.MyStaticValue 106 - init copyright.string to env value is : Copyright 2002-2016, LAMP/EPFL
2019-09-11 16:34:01,703  INFO org.ansj.util.MyStaticValue 106 - init osgi.version.number to env value is : 2.11.8.v20160304-115712-1706a37eb8
2019-09-11 16:34:01,704  INFO org.ansj.util.MyStaticValue 106 - init maven.version.number to env value is : 2.11.8
2019-09-11 16:34:01,715  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/ambiguity.dic
2019-09-11 16:34:01,716 ERROR org.ansj.library.AmbiguityLibrary 117 - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:D:\MyProject\splitCHword\library\ambiguity.dic not found or can not to read, path: library/ambiguity.dic
2019-09-11 16:34:01,720  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/default.dic
2019-09-11 16:34:01,720 ERROR org.ansj.library.DicLibrary 221 - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:D:\MyProject\splitCHword\library\default.dic not found or can not to read, path: library/default.dic
2019-09-11 16:34:03,116  INFO org.ansj.library.DATDictionary 90 - init core library ok use time : 1128
2019-09-11 16:34:03,716  INFO org.ansj.library.NgramLibrary 17 - init ngram ok use time :593
2019-09-11 17:14:19,535  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-11 17:18:11,231  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-11 17:20:52,465  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-11 17:26:06,875  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-11 17:26:07,608  WARN org.apache.hadoop.util.NativeCodeLoader 62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-11 17:26:07,712 ERROR org.apache.hadoop.util.Shell 396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2430)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
	at com.datamodel.SparkUtil.CreateSession(SparkUtil.java:20)
	at com.datamodel.SparkUtil.readDataFromCsv(SparkUtil.java:25)
	at com.datamodel.SparkUtil.main(SparkUtil.java:39)
2019-09-11 17:26:08,320  INFO org.apache.spark.SparkContext 54 - Submitted application: splitCHWord
2019-09-11 17:26:08,543  INFO org.apache.spark.SecurityManager 54 - Changing view acls to: OUY
2019-09-11 17:26:08,544  INFO org.apache.spark.SecurityManager 54 - Changing modify acls to: OUY
2019-09-11 17:26:08,545  INFO org.apache.spark.SecurityManager 54 - Changing view acls groups to: 
2019-09-11 17:26:08,545  INFO org.apache.spark.SecurityManager 54 - Changing modify acls groups to: 
2019-09-11 17:26:08,546  INFO org.apache.spark.SecurityManager 54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(OUY); groups with view permissions: Set(); users  with modify permissions: Set(OUY); groups with modify permissions: Set()
2019-09-11 17:26:10,280  INFO org.apache.spark.util.Utils 54 - Successfully started service 'sparkDriver' on port 14549.
2019-09-11 17:26:10,489  INFO org.apache.spark.SparkEnv 54 - Registering MapOutputTracker
2019-09-11 17:26:10,594  INFO org.apache.spark.SparkEnv 54 - Registering BlockManagerMaster
2019-09-11 17:26:10,600  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-09-11 17:26:10,601  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - BlockManagerMasterEndpoint up
2019-09-11 17:26:10,622  INFO org.apache.spark.storage.DiskBlockManager 54 - Created local directory at C:\Users\OUY\AppData\Local\Temp\blockmgr-aafabbe3-bab8-4909-a389-891581d5dfa7
2019-09-11 17:26:10,763  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore started with capacity 899.7 MB
2019-09-11 17:26:11,006  INFO org.apache.spark.SparkEnv 54 - Registering OutputCommitCoordinator
2019-09-11 17:26:11,331  INFO org.spark_project.jetty.util.log 192 - Logging initialized @7898ms
2019-09-11 17:26:11,579  INFO org.spark_project.jetty.server.Server 345 - jetty-9.3.z-SNAPSHOT
2019-09-11 17:26:11,600  INFO org.spark_project.jetty.server.Server 403 - Started @8168ms
2019-09-11 17:26:11,654  INFO org.spark_project.jetty.server.AbstractConnector 270 - Started ServerConnector@377fb945{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-09-11 17:26:11,654  INFO org.apache.spark.util.Utils 54 - Successfully started service 'SparkUI' on port 4040.
2019-09-11 17:26:11,692  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@42b64ab8{/jobs,null,AVAILABLE,@Spark}
2019-09-11 17:26:11,694  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2c1b9e4b{/jobs/json,null,AVAILABLE,@Spark}
2019-09-11 17:26:11,695  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@649725e3{/jobs/job,null,AVAILABLE,@Spark}
2019-09-11 17:26:11,696  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@52b56a3e{/jobs/job/json,null,AVAILABLE,@Spark}
2019-09-11 17:26:11,697  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4eed46ee{/stages,null,AVAILABLE,@Spark}
2019-09-11 17:26:11,699  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4fad94a7{/stages/json,null,AVAILABLE,@Spark}
2019-09-11 17:26:11,700  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6326d182{/stages/stage,null,AVAILABLE,@Spark}
2019-09-11 17:26:11,702  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@226642a5{/stages/stage/json,null,AVAILABLE,@Spark}
2019-09-11 17:26:11,704  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5cc126dc{/stages/pool,null,AVAILABLE,@Spark}
2019-09-11 17:26:11,706  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@72bd06ca{/stages/pool/json,null,AVAILABLE,@Spark}
2019-09-11 17:26:11,713  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5dbe30be{/storage,null,AVAILABLE,@Spark}
2019-09-11 17:26:11,714  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@55795845{/storage/json,null,AVAILABLE,@Spark}
2019-09-11 17:26:11,715  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@119f1f2a{/storage/rdd,null,AVAILABLE,@Spark}
2019-09-11 17:26:11,717  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5b970f7{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-09-11 17:26:11,719  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@165b8a71{/environment,null,AVAILABLE,@Spark}
2019-09-11 17:26:11,720  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2f058b8a{/environment/json,null,AVAILABLE,@Spark}
2019-09-11 17:26:11,722  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3f2ef586{/executors,null,AVAILABLE,@Spark}
2019-09-11 17:26:11,723  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@76c7beb3{/executors/json,null,AVAILABLE,@Spark}
2019-09-11 17:26:11,725  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2cf92cc7{/executors/threadDump,null,AVAILABLE,@Spark}
2019-09-11 17:26:11,726  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7b139eab{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-09-11 17:26:11,741  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@611df6e3{/static,null,AVAILABLE,@Spark}
2019-09-11 17:26:11,742  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6ffab045{/,null,AVAILABLE,@Spark}
2019-09-11 17:26:11,744  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3e2943ab{/api,null,AVAILABLE,@Spark}
2019-09-11 17:26:11,745  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6c67e137{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-09-11 17:26:11,746  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@191ae03f{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-09-11 17:26:11,777  INFO org.apache.spark.ui.SparkUI 54 - Bound SparkUI to 0.0.0.0, and started at http://10.6.1.14:4040
2019-09-11 17:26:12,828  INFO org.apache.spark.executor.Executor 54 - Starting executor ID driver on host localhost
2019-09-11 17:26:13,079  INFO org.apache.spark.util.Utils 54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 14562.
2019-09-11 17:26:13,156  INFO org.apache.spark.network.netty.NettyBlockTransferService 54 - Server created on 10.6.1.14:14562
2019-09-11 17:26:13,164  INFO org.apache.spark.storage.BlockManager 54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-09-11 17:26:13,172  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registering BlockManager BlockManagerId(driver, 10.6.1.14, 14562, None)
2019-09-11 17:26:13,189  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Registering block manager 10.6.1.14:14562 with 899.7 MB RAM, BlockManagerId(driver, 10.6.1.14, 14562, None)
2019-09-11 17:26:13,194  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registered BlockManager BlockManagerId(driver, 10.6.1.14, 14562, None)
2019-09-11 17:26:13,195  INFO org.apache.spark.storage.BlockManager 54 - Initialized BlockManager: BlockManagerId(driver, 10.6.1.14, 14562, None)
2019-09-11 17:26:13,714  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@30f4b1a6{/metrics/json,null,AVAILABLE,@Spark}
2019-09-11 17:26:14,180  INFO org.apache.spark.sql.internal.SharedState 54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/MyProject/splitCHword/spark-warehouse').
2019-09-11 17:26:14,182  INFO org.apache.spark.sql.internal.SharedState 54 - Warehouse path is 'file:/D:/MyProject/splitCHword/spark-warehouse'.
2019-09-11 17:26:14,201  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@d4602a{/SQL,null,AVAILABLE,@Spark}
2019-09-11 17:26:14,202  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@47dd778{/SQL/json,null,AVAILABLE,@Spark}
2019-09-11 17:26:14,205  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@11a82d0f{/SQL/execution,null,AVAILABLE,@Spark}
2019-09-11 17:26:14,206  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3ae66c85{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-09-11 17:26:14,232  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5a4ed68f{/static/sql,null,AVAILABLE,@Spark}
2019-09-11 17:26:16,358  INFO org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef 54 - Registered StateStoreCoordinator endpoint
2019-09-11 17:26:17,018  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0 stored as values in memory (estimated size 219.8 KB, free 899.5 MB)
2019-09-11 17:26:17,168  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-11 17:26:17,173  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_0_piece0 in memory on 10.6.1.14:14562 (size: 20.7 KB, free: 899.7 MB)
2019-09-11 17:26:17,180  INFO org.apache.spark.SparkContext 54 - Created broadcast 0 from load at SparkUtil.java:29
2019-09-11 17:26:17,703  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-11 17:26:17,706  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-11 17:26:17,740  INFO org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 413 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 243
2019-09-11 17:26:17,859  INFO org.apache.spark.SparkContext 54 - Starting job: load at SparkUtil.java:29
2019-09-11 17:26:17,896  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 0 (load at SparkUtil.java:29) with 1 output partitions
2019-09-11 17:26:17,897  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 0 (load at SparkUtil.java:29)
2019-09-11 17:26:17,898  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-11 17:26:17,904  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-11 17:26:17,969  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:29), which has no missing parents
2019-09-11 17:26:18,105  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1 stored as values in memory (estimated size 5.5 KB, free 899.5 MB)
2019-09-11 17:26:18,115  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 899.5 MB)
2019-09-11 17:26:18,117  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_1_piece0 in memory on 10.6.1.14:14562 (size: 3.4 KB, free: 899.7 MB)
2019-09-11 17:26:18,120  INFO org.apache.spark.SparkContext 54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-09-11 17:26:18,153  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:29) (first 15 tasks are for partitions Vector(0))
2019-09-11 17:26:18,155  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 0.0 with 1 tasks
2019-09-11 17:26:18,266  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes)
2019-09-11 17:26:18,283  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 0.0 (TID 0)
2019-09-11 17:26:18,408  INFO org.apache.spark.rdd.BinaryFileRDD 54 - Input split: Paths:/D:/MyProject/splitCHword/target/classes/test.csv:0+243
2019-09-11 17:26:18,717  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 0.0 (TID 0). 810 bytes result sent to driver
2019-09-11 17:26:18,740  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 0.0 (TID 0) in 512 ms on localhost (executor driver) (1/1)
2019-09-11 17:26:18,745  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-09-11 17:26:18,756  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 0 (load at SparkUtil.java:29) finished in 0.558 s
2019-09-11 17:26:18,787  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 0 finished: load at SparkUtil.java:29, took 0.926664 s
2019-09-11 17:26:19,242  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_1_piece0 on 10.6.1.14:14562 in memory (size: 3.4 KB, free: 899.7 MB)
2019-09-11 17:26:21,439  INFO org.apache.spark.SparkContext 54 - Invoking stop() from shutdown hook
2019-09-11 17:26:21,479  INFO org.spark_project.jetty.server.AbstractConnector 310 - Stopped Spark@377fb945{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-09-11 17:26:21,482  INFO org.apache.spark.ui.SparkUI 54 - Stopped Spark web UI at http://10.6.1.14:4040
2019-09-11 17:26:21,552  INFO org.apache.spark.MapOutputTrackerMasterEndpoint 54 - MapOutputTrackerMasterEndpoint stopped!
2019-09-11 17:26:21,619  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore cleared
2019-09-11 17:26:21,619  INFO org.apache.spark.storage.BlockManager 54 - BlockManager stopped
2019-09-11 17:26:21,621  INFO org.apache.spark.storage.BlockManagerMaster 54 - BlockManagerMaster stopped
2019-09-11 17:26:21,639  INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint 54 - OutputCommitCoordinator stopped!
2019-09-11 17:26:21,652  INFO org.apache.spark.SparkContext 54 - Successfully stopped SparkContext
2019-09-11 17:26:21,654  INFO org.apache.spark.util.ShutdownHookManager 54 - Shutdown hook called
2019-09-11 17:26:21,688  INFO org.apache.spark.util.ShutdownHookManager 54 - Deleting directory C:\Users\OUY\AppData\Local\Temp\spark-932b0c98-1042-4768-866a-5d373a957b5a
2019-09-11 17:27:03,316  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-11 17:27:04,332  WARN org.apache.hadoop.util.NativeCodeLoader 62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-11 17:27:04,486 ERROR org.apache.hadoop.util.Shell 396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2430)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
	at com.datamodel.SparkUtil.CreateSession(SparkUtil.java:20)
	at com.datamodel.SparkUtil.readDataFromCsv(SparkUtil.java:25)
	at com.datamodel.SparkUtil.main(SparkUtil.java:39)
2019-09-11 17:27:04,750  INFO org.apache.spark.SparkContext 54 - Submitted application: splitCHWord
2019-09-11 17:27:04,834  INFO org.apache.spark.SecurityManager 54 - Changing view acls to: OUY
2019-09-11 17:27:04,837  INFO org.apache.spark.SecurityManager 54 - Changing modify acls to: OUY
2019-09-11 17:27:04,842  INFO org.apache.spark.SecurityManager 54 - Changing view acls groups to: 
2019-09-11 17:27:04,845  INFO org.apache.spark.SecurityManager 54 - Changing modify acls groups to: 
2019-09-11 17:27:04,848  INFO org.apache.spark.SecurityManager 54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(OUY); groups with view permissions: Set(); users  with modify permissions: Set(OUY); groups with modify permissions: Set()
2019-09-11 17:27:05,917  INFO org.apache.spark.util.Utils 54 - Successfully started service 'sparkDriver' on port 14600.
2019-09-11 17:27:05,990  INFO org.apache.spark.SparkEnv 54 - Registering MapOutputTracker
2019-09-11 17:27:06,014  INFO org.apache.spark.SparkEnv 54 - Registering BlockManagerMaster
2019-09-11 17:27:06,018  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-09-11 17:27:06,019  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - BlockManagerMasterEndpoint up
2019-09-11 17:27:06,033  INFO org.apache.spark.storage.DiskBlockManager 54 - Created local directory at C:\Users\OUY\AppData\Local\Temp\blockmgr-208ea56d-6752-4fe0-9b35-c07b65c1ff6e
2019-09-11 17:27:06,065  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore started with capacity 899.7 MB
2019-09-11 17:27:06,181  INFO org.apache.spark.SparkEnv 54 - Registering OutputCommitCoordinator
2019-09-11 17:27:06,338  INFO org.spark_project.jetty.util.log 192 - Logging initialized @6280ms
2019-09-11 17:27:06,452  INFO org.spark_project.jetty.server.Server 345 - jetty-9.3.z-SNAPSHOT
2019-09-11 17:27:06,476  INFO org.spark_project.jetty.server.Server 403 - Started @6420ms
2019-09-11 17:27:06,515  INFO org.spark_project.jetty.server.AbstractConnector 270 - Started ServerConnector@6bcc39aa{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-09-11 17:27:06,515  INFO org.apache.spark.util.Utils 54 - Successfully started service 'SparkUI' on port 4040.
2019-09-11 17:27:06,562  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@42b64ab8{/jobs,null,AVAILABLE,@Spark}
2019-09-11 17:27:06,563  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2c1b9e4b{/jobs/json,null,AVAILABLE,@Spark}
2019-09-11 17:27:06,564  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@649725e3{/jobs/job,null,AVAILABLE,@Spark}
2019-09-11 17:27:06,566  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@52b56a3e{/jobs/job/json,null,AVAILABLE,@Spark}
2019-09-11 17:27:06,569  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4eed46ee{/stages,null,AVAILABLE,@Spark}
2019-09-11 17:27:06,570  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4fad94a7{/stages/json,null,AVAILABLE,@Spark}
2019-09-11 17:27:06,572  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6326d182{/stages/stage,null,AVAILABLE,@Spark}
2019-09-11 17:27:06,574  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@226642a5{/stages/stage/json,null,AVAILABLE,@Spark}
2019-09-11 17:27:06,575  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5cc126dc{/stages/pool,null,AVAILABLE,@Spark}
2019-09-11 17:27:06,577  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@72bd06ca{/stages/pool/json,null,AVAILABLE,@Spark}
2019-09-11 17:27:06,578  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5dbe30be{/storage,null,AVAILABLE,@Spark}
2019-09-11 17:27:06,579  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@55795845{/storage/json,null,AVAILABLE,@Spark}
2019-09-11 17:27:06,581  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@119f1f2a{/storage/rdd,null,AVAILABLE,@Spark}
2019-09-11 17:27:06,583  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5b970f7{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-09-11 17:27:06,586  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@165b8a71{/environment,null,AVAILABLE,@Spark}
2019-09-11 17:27:06,588  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2f058b8a{/environment/json,null,AVAILABLE,@Spark}
2019-09-11 17:27:06,591  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3f2ef586{/executors,null,AVAILABLE,@Spark}
2019-09-11 17:27:06,592  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@76c7beb3{/executors/json,null,AVAILABLE,@Spark}
2019-09-11 17:27:06,593  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2cf92cc7{/executors/threadDump,null,AVAILABLE,@Spark}
2019-09-11 17:27:06,594  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7b139eab{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-09-11 17:27:06,607  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@611df6e3{/static,null,AVAILABLE,@Spark}
2019-09-11 17:27:06,609  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6ffab045{/,null,AVAILABLE,@Spark}
2019-09-11 17:27:06,613  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3e2943ab{/api,null,AVAILABLE,@Spark}
2019-09-11 17:27:06,614  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6c67e137{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-09-11 17:27:06,616  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@191ae03f{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-09-11 17:27:06,619  INFO org.apache.spark.ui.SparkUI 54 - Bound SparkUI to 0.0.0.0, and started at http://10.6.1.14:4040
2019-09-11 17:27:06,888  INFO org.apache.spark.executor.Executor 54 - Starting executor ID driver on host localhost
2019-09-11 17:27:06,953  INFO org.apache.spark.util.Utils 54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 14613.
2019-09-11 17:27:06,954  INFO org.apache.spark.network.netty.NettyBlockTransferService 54 - Server created on 10.6.1.14:14613
2019-09-11 17:27:06,956  INFO org.apache.spark.storage.BlockManager 54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-09-11 17:27:06,960  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registering BlockManager BlockManagerId(driver, 10.6.1.14, 14613, None)
2019-09-11 17:27:06,976  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Registering block manager 10.6.1.14:14613 with 899.7 MB RAM, BlockManagerId(driver, 10.6.1.14, 14613, None)
2019-09-11 17:27:06,984  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registered BlockManager BlockManagerId(driver, 10.6.1.14, 14613, None)
2019-09-11 17:27:06,985  INFO org.apache.spark.storage.BlockManager 54 - Initialized BlockManager: BlockManagerId(driver, 10.6.1.14, 14613, None)
2019-09-11 17:27:07,252  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@30f4b1a6{/metrics/json,null,AVAILABLE,@Spark}
2019-09-11 17:27:07,356  INFO org.apache.spark.sql.internal.SharedState 54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/MyProject/splitCHword/spark-warehouse/').
2019-09-11 17:27:07,357  INFO org.apache.spark.sql.internal.SharedState 54 - Warehouse path is 'file:/D:/MyProject/splitCHword/spark-warehouse/'.
2019-09-11 17:27:07,372  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@d4602a{/SQL,null,AVAILABLE,@Spark}
2019-09-11 17:27:07,373  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@47dd778{/SQL/json,null,AVAILABLE,@Spark}
2019-09-11 17:27:07,375  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@11a82d0f{/SQL/execution,null,AVAILABLE,@Spark}
2019-09-11 17:27:07,376  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3ae66c85{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-09-11 17:27:07,379  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5a4ed68f{/static/sql,null,AVAILABLE,@Spark}
2019-09-11 17:27:09,140  INFO org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef 54 - Registered StateStoreCoordinator endpoint
2019-09-11 17:27:09,592  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0 stored as values in memory (estimated size 219.8 KB, free 899.5 MB)
2019-09-11 17:27:09,731  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-11 17:27:09,741  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_0_piece0 in memory on 10.6.1.14:14613 (size: 20.7 KB, free: 899.7 MB)
2019-09-11 17:27:09,875  INFO org.apache.spark.SparkContext 54 - Created broadcast 0 from load at SparkUtil.java:29
2019-09-11 17:27:10,347  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-11 17:27:10,351  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-11 17:27:10,372  INFO org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 413 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 243
2019-09-11 17:27:10,442  INFO org.apache.spark.SparkContext 54 - Starting job: load at SparkUtil.java:29
2019-09-11 17:27:10,471  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 0 (load at SparkUtil.java:29) with 1 output partitions
2019-09-11 17:27:10,473  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 0 (load at SparkUtil.java:29)
2019-09-11 17:27:10,473  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-11 17:27:10,475  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-11 17:27:10,481  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:29), which has no missing parents
2019-09-11 17:27:10,516  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1 stored as values in memory (estimated size 5.5 KB, free 899.5 MB)
2019-09-11 17:27:10,543  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 899.5 MB)
2019-09-11 17:27:10,544  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_1_piece0 in memory on 10.6.1.14:14613 (size: 3.4 KB, free: 899.7 MB)
2019-09-11 17:27:10,546  INFO org.apache.spark.SparkContext 54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-09-11 17:27:10,613  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:29) (first 15 tasks are for partitions Vector(0))
2019-09-11 17:27:10,616  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 0.0 with 1 tasks
2019-09-11 17:27:10,729  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes)
2019-09-11 17:27:10,745  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 0.0 (TID 0)
2019-09-11 17:27:10,946  INFO org.apache.spark.rdd.BinaryFileRDD 54 - Input split: Paths:/D:/MyProject/splitCHword/target/classes/test.csv:0+243
2019-09-11 17:27:11,143  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 0.0 (TID 0). 853 bytes result sent to driver
2019-09-11 17:27:11,154  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 0.0 (TID 0) in 449 ms on localhost (executor driver) (1/1)
2019-09-11 17:27:11,161  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-09-11 17:27:11,165  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 0 (load at SparkUtil.java:29) finished in 0.497 s
2019-09-11 17:27:11,173  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 0 finished: load at SparkUtil.java:29, took 0.730875 s
2019-09-11 17:27:13,617  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Pruning directories with: 
2019-09-11 17:27:13,624  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Post-Scan Filters: 
2019-09-11 17:27:13,627  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Output Data Schema: struct<label: string, feature: string>
2019-09-11 17:27:13,642  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Pushed Filters: 
2019-09-11 17:27:14,102  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_1_piece0 on 10.6.1.14:14613 in memory (size: 3.4 KB, free: 899.7 MB)
2019-09-11 17:27:14,108  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_0_piece0 on 10.6.1.14:14613 in memory (size: 20.7 KB, free: 899.7 MB)
2019-09-11 17:27:14,607  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 386.801755 ms
2019-09-11 17:27:14,620  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2 stored as values in memory (estimated size 220.5 KB, free 899.5 MB)
2019-09-11 17:27:14,641  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-11 17:27:14,653  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_2_piece0 in memory on 10.6.1.14:14613 (size: 20.7 KB, free: 899.7 MB)
2019-09-11 17:27:14,656  INFO org.apache.spark.SparkContext 54 - Created broadcast 2 from show at SparkUtil.java:40
2019-09-11 17:27:14,687  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2019-09-11 17:27:14,807  INFO org.apache.spark.SparkContext 54 - Starting job: show at SparkUtil.java:40
2019-09-11 17:27:14,809  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 1 (show at SparkUtil.java:40) with 1 output partitions
2019-09-11 17:27:14,810  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 1 (show at SparkUtil.java:40)
2019-09-11 17:27:14,810  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-11 17:27:14,812  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-11 17:27:14,813  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:40), which has no missing parents
2019-09-11 17:27:14,881  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3 stored as values in memory (estimated size 9.4 KB, free 899.5 MB)
2019-09-11 17:27:14,889  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.3 KB, free 899.5 MB)
2019-09-11 17:27:14,891  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_3_piece0 in memory on 10.6.1.14:14613 (size: 5.3 KB, free: 899.7 MB)
2019-09-11 17:27:14,892  INFO org.apache.spark.SparkContext 54 - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2019-09-11 17:27:14,893  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:40) (first 15 tasks are for partitions Vector(0))
2019-09-11 17:27:14,894  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 1.0 with 1 tasks
2019-09-11 17:27:14,907  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5293 bytes)
2019-09-11 17:27:14,908  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 1.0 (TID 1)
2019-09-11 17:27:14,938  INFO org.apache.spark.sql.execution.datasources.FileScanRDD 54 - Reading File path: file:///D:/MyProject/splitCHword/target/classes/test.csv, range: 0-243, partition values: [empty row]
2019-09-11 17:27:15,017  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 21.949431 ms
2019-09-11 17:27:15,127  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 1.0 (TID 1). 1403 bytes result sent to driver
2019-09-11 17:27:15,131  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 1.0 (TID 1) in 235 ms on localhost (executor driver) (1/1)
2019-09-11 17:27:15,131  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-09-11 17:27:15,132  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 1 (show at SparkUtil.java:40) finished in 0.237 s
2019-09-11 17:27:15,141  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 1 finished: show at SparkUtil.java:40, took 0.333945 s
2019-09-11 17:27:15,224  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 30.619293 ms
2019-09-11 17:27:15,289  INFO org.apache.spark.SparkContext 54 - Invoking stop() from shutdown hook
2019-09-11 17:27:15,298  INFO org.spark_project.jetty.server.AbstractConnector 310 - Stopped Spark@6bcc39aa{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-09-11 17:27:15,306  INFO org.apache.spark.ui.SparkUI 54 - Stopped Spark web UI at http://10.6.1.14:4040
2019-09-11 17:27:15,328  INFO org.apache.spark.MapOutputTrackerMasterEndpoint 54 - MapOutputTrackerMasterEndpoint stopped!
2019-09-11 17:27:15,376  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore cleared
2019-09-11 17:27:15,376  INFO org.apache.spark.storage.BlockManager 54 - BlockManager stopped
2019-09-11 17:27:15,379  INFO org.apache.spark.storage.BlockManagerMaster 54 - BlockManagerMaster stopped
2019-09-11 17:27:15,382  INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint 54 - OutputCommitCoordinator stopped!
2019-09-11 17:27:15,390  INFO org.apache.spark.SparkContext 54 - Successfully stopped SparkContext
2019-09-11 17:27:15,391  INFO org.apache.spark.util.ShutdownHookManager 54 - Shutdown hook called
2019-09-11 17:27:15,392  INFO org.apache.spark.util.ShutdownHookManager 54 - Deleting directory C:\Users\OUY\AppData\Local\Temp\spark-a2495971-ced3-4ef8-8e6a-a8248ce307eb
2019-09-11 17:31:54,501  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-11 17:31:55,196  WARN org.apache.hadoop.util.NativeCodeLoader 62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-11 17:31:55,323 ERROR org.apache.hadoop.util.Shell 396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2430)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
	at com.datamodel.SparkUtil.CreateSession(SparkUtil.java:20)
	at com.datamodel.SparkUtil.readDataFromCsv(SparkUtil.java:25)
	at com.datamodel.SparkUtil.main(SparkUtil.java:39)
2019-09-11 17:31:55,566  INFO org.apache.spark.SparkContext 54 - Submitted application: splitCHWord
2019-09-11 17:31:55,631  INFO org.apache.spark.SecurityManager 54 - Changing view acls to: OUY
2019-09-11 17:31:55,633  INFO org.apache.spark.SecurityManager 54 - Changing modify acls to: OUY
2019-09-11 17:31:55,634  INFO org.apache.spark.SecurityManager 54 - Changing view acls groups to: 
2019-09-11 17:31:55,636  INFO org.apache.spark.SecurityManager 54 - Changing modify acls groups to: 
2019-09-11 17:31:55,637  INFO org.apache.spark.SecurityManager 54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(OUY); groups with view permissions: Set(); users  with modify permissions: Set(OUY); groups with modify permissions: Set()
2019-09-11 17:31:56,642  INFO org.apache.spark.util.Utils 54 - Successfully started service 'sparkDriver' on port 14898.
2019-09-11 17:31:56,679  INFO org.apache.spark.SparkEnv 54 - Registering MapOutputTracker
2019-09-11 17:31:56,712  INFO org.apache.spark.SparkEnv 54 - Registering BlockManagerMaster
2019-09-11 17:31:56,719  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-09-11 17:31:56,720  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - BlockManagerMasterEndpoint up
2019-09-11 17:31:56,736  INFO org.apache.spark.storage.DiskBlockManager 54 - Created local directory at C:\Users\OUY\AppData\Local\Temp\blockmgr-aadcb057-6550-4c30-acdc-c0fbf4c2d1e9
2019-09-11 17:31:56,777  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore started with capacity 899.7 MB
2019-09-11 17:31:56,893  INFO org.apache.spark.SparkEnv 54 - Registering OutputCommitCoordinator
2019-09-11 17:31:57,023  INFO org.spark_project.jetty.util.log 192 - Logging initialized @5805ms
2019-09-11 17:31:57,126  INFO org.spark_project.jetty.server.Server 345 - jetty-9.3.z-SNAPSHOT
2019-09-11 17:31:57,147  INFO org.spark_project.jetty.server.Server 403 - Started @5931ms
2019-09-11 17:31:57,183  INFO org.spark_project.jetty.server.AbstractConnector 270 - Started ServerConnector@630be695{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-09-11 17:31:57,183  INFO org.apache.spark.util.Utils 54 - Successfully started service 'SparkUI' on port 4040.
2019-09-11 17:31:57,234  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@45bf6f39{/jobs,null,AVAILABLE,@Spark}
2019-09-11 17:31:57,235  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2e060819{/jobs/json,null,AVAILABLE,@Spark}
2019-09-11 17:31:57,237  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2184b4f4{/jobs/job,null,AVAILABLE,@Spark}
2019-09-11 17:31:57,241  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@316a598d{/jobs/job/json,null,AVAILABLE,@Spark}
2019-09-11 17:31:57,242  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6ba30587{/stages,null,AVAILABLE,@Spark}
2019-09-11 17:31:57,243  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5633dafd{/stages/json,null,AVAILABLE,@Spark}
2019-09-11 17:31:57,244  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2eadc9f6{/stages/stage,null,AVAILABLE,@Spark}
2019-09-11 17:31:57,247  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@59901c4d{/stages/stage/json,null,AVAILABLE,@Spark}
2019-09-11 17:31:57,248  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@d8d9199{/stages/pool,null,AVAILABLE,@Spark}
2019-09-11 17:31:57,249  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@602ae7b6{/stages/pool/json,null,AVAILABLE,@Spark}
2019-09-11 17:31:57,250  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@71ad3d8a{/storage,null,AVAILABLE,@Spark}
2019-09-11 17:31:57,251  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@700f518a{/storage/json,null,AVAILABLE,@Spark}
2019-09-11 17:31:57,252  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@13da7ab0{/storage/rdd,null,AVAILABLE,@Spark}
2019-09-11 17:31:57,254  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@260ff5b7{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-09-11 17:31:57,255  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@77eb5790{/environment,null,AVAILABLE,@Spark}
2019-09-11 17:31:57,257  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@319c3a25{/environment/json,null,AVAILABLE,@Spark}
2019-09-11 17:31:57,258  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@ef1695a{/executors,null,AVAILABLE,@Spark}
2019-09-11 17:31:57,259  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@81b5db0{/executors/json,null,AVAILABLE,@Spark}
2019-09-11 17:31:57,262  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7139bd31{/executors/threadDump,null,AVAILABLE,@Spark}
2019-09-11 17:31:57,268  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4b3fe06e{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-09-11 17:31:57,282  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3e17a0a1{/static,null,AVAILABLE,@Spark}
2019-09-11 17:31:57,283  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@55d9b8f0{/,null,AVAILABLE,@Spark}
2019-09-11 17:31:57,288  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@43d38654{/api,null,AVAILABLE,@Spark}
2019-09-11 17:31:57,289  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1d75e7af{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-09-11 17:31:57,290  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@34b27915{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-09-11 17:31:57,305  INFO org.apache.spark.ui.SparkUI 54 - Bound SparkUI to 0.0.0.0, and started at http://10.6.1.14:4040
2019-09-11 17:31:57,714  INFO org.apache.spark.executor.Executor 54 - Starting executor ID driver on host localhost
2019-09-11 17:31:57,773  INFO org.apache.spark.util.Utils 54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 14911.
2019-09-11 17:31:57,775  INFO org.apache.spark.network.netty.NettyBlockTransferService 54 - Server created on 10.6.1.14:14911
2019-09-11 17:31:57,781  INFO org.apache.spark.storage.BlockManager 54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-09-11 17:31:57,785  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registering BlockManager BlockManagerId(driver, 10.6.1.14, 14911, None)
2019-09-11 17:31:57,793  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Registering block manager 10.6.1.14:14911 with 899.7 MB RAM, BlockManagerId(driver, 10.6.1.14, 14911, None)
2019-09-11 17:31:57,801  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registered BlockManager BlockManagerId(driver, 10.6.1.14, 14911, None)
2019-09-11 17:31:57,804  INFO org.apache.spark.storage.BlockManager 54 - Initialized BlockManager: BlockManagerId(driver, 10.6.1.14, 14911, None)
2019-09-11 17:31:58,130  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@48904d5a{/metrics/json,null,AVAILABLE,@Spark}
2019-09-11 17:31:58,369  INFO org.apache.spark.sql.internal.SharedState 54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/MyProject/splitCHword/spark-warehouse/').
2019-09-11 17:31:58,371  INFO org.apache.spark.sql.internal.SharedState 54 - Warehouse path is 'file:/D:/MyProject/splitCHword/spark-warehouse/'.
2019-09-11 17:31:58,386  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1fc713c9{/SQL,null,AVAILABLE,@Spark}
2019-09-11 17:31:58,387  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@76b47204{/SQL/json,null,AVAILABLE,@Spark}
2019-09-11 17:31:58,388  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6848a051{/SQL/execution,null,AVAILABLE,@Spark}
2019-09-11 17:31:58,389  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5740ff5e{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-09-11 17:31:58,392  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1d408060{/static/sql,null,AVAILABLE,@Spark}
2019-09-11 17:32:00,711  INFO org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef 54 - Registered StateStoreCoordinator endpoint
2019-09-11 17:32:01,355  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0 stored as values in memory (estimated size 219.8 KB, free 899.5 MB)
2019-09-11 17:32:01,633  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-11 17:32:01,648  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_0_piece0 in memory on 10.6.1.14:14911 (size: 20.7 KB, free: 899.7 MB)
2019-09-11 17:32:01,664  INFO org.apache.spark.SparkContext 54 - Created broadcast 0 from load at SparkUtil.java:29
2019-09-11 17:32:02,325  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-11 17:32:02,327  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-11 17:32:02,347  INFO org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 413 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 326
2019-09-11 17:32:02,418  INFO org.apache.spark.SparkContext 54 - Starting job: load at SparkUtil.java:29
2019-09-11 17:32:02,448  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 0 (load at SparkUtil.java:29) with 1 output partitions
2019-09-11 17:32:02,448  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 0 (load at SparkUtil.java:29)
2019-09-11 17:32:02,449  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-11 17:32:02,451  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-11 17:32:02,461  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:29), which has no missing parents
2019-09-11 17:32:02,531  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1 stored as values in memory (estimated size 5.5 KB, free 899.5 MB)
2019-09-11 17:32:02,564  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 899.5 MB)
2019-09-11 17:32:02,568  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_1_piece0 in memory on 10.6.1.14:14911 (size: 3.4 KB, free: 899.7 MB)
2019-09-11 17:32:02,572  INFO org.apache.spark.SparkContext 54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-09-11 17:32:02,614  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:29) (first 15 tasks are for partitions Vector(0))
2019-09-11 17:32:02,616  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 0.0 with 1 tasks
2019-09-11 17:32:02,701  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes)
2019-09-11 17:32:02,717  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 0.0 (TID 0)
2019-09-11 17:32:02,885  INFO org.apache.spark.rdd.BinaryFileRDD 54 - Input split: Paths:/D:/MyProject/splitCHword/target/classes/test.csv:0+326
2019-09-11 17:32:03,118  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 0.0 (TID 0). 853 bytes result sent to driver
2019-09-11 17:32:03,131  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 0.0 (TID 0) in 449 ms on localhost (executor driver) (1/1)
2019-09-11 17:32:03,135  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-09-11 17:32:03,143  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 0 (load at SparkUtil.java:29) finished in 0.485 s
2019-09-11 17:32:03,153  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 0 finished: load at SparkUtil.java:29, took 0.735003 s
2019-09-11 17:32:04,210  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_1_piece0 on 10.6.1.14:14911 in memory (size: 3.4 KB, free: 899.7 MB)
2019-09-11 17:32:05,886  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Pruning directories with: 
2019-09-11 17:32:05,892  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Post-Scan Filters: 
2019-09-11 17:32:05,897  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Output Data Schema: struct<label: string, feature: string>
2019-09-11 17:32:05,911  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Pushed Filters: 
2019-09-11 17:32:06,087  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_0_piece0 on 10.6.1.14:14911 in memory (size: 20.7 KB, free: 899.7 MB)
2019-09-11 17:32:06,632  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 319.402957 ms
2019-09-11 17:32:06,643  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2 stored as values in memory (estimated size 220.5 KB, free 899.5 MB)
2019-09-11 17:32:06,669  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-11 17:32:06,672  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_2_piece0 in memory on 10.6.1.14:14911 (size: 20.7 KB, free: 899.7 MB)
2019-09-11 17:32:06,676  INFO org.apache.spark.SparkContext 54 - Created broadcast 2 from show at SparkUtil.java:40
2019-09-11 17:32:06,694  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2019-09-11 17:32:06,772  INFO org.apache.spark.SparkContext 54 - Starting job: show at SparkUtil.java:40
2019-09-11 17:32:06,773  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 1 (show at SparkUtil.java:40) with 1 output partitions
2019-09-11 17:32:06,773  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 1 (show at SparkUtil.java:40)
2019-09-11 17:32:06,773  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-11 17:32:06,775  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-11 17:32:06,776  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:40), which has no missing parents
2019-09-11 17:32:06,815  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3 stored as values in memory (estimated size 9.4 KB, free 899.5 MB)
2019-09-11 17:32:06,822  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.3 KB, free 899.5 MB)
2019-09-11 17:32:06,824  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_3_piece0 in memory on 10.6.1.14:14911 (size: 5.3 KB, free: 899.7 MB)
2019-09-11 17:32:06,825  INFO org.apache.spark.SparkContext 54 - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2019-09-11 17:32:06,826  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:40) (first 15 tasks are for partitions Vector(0))
2019-09-11 17:32:06,826  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 1.0 with 1 tasks
2019-09-11 17:32:06,841  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5293 bytes)
2019-09-11 17:32:06,842  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 1.0 (TID 1)
2019-09-11 17:32:06,868  INFO org.apache.spark.sql.execution.datasources.FileScanRDD 54 - Reading File path: file:///D:/MyProject/splitCHword/target/classes/test.csv, range: 0-326, partition values: [empty row]
2019-09-11 17:32:06,940  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 34.542066 ms
2019-09-11 17:32:06,983  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 1.0 (TID 1). 1478 bytes result sent to driver
2019-09-11 17:32:06,986  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 1.0 (TID 1) in 159 ms on localhost (executor driver) (1/1)
2019-09-11 17:32:06,986  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-09-11 17:32:06,988  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 1 (show at SparkUtil.java:40) finished in 0.160 s
2019-09-11 17:32:06,991  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 1 finished: show at SparkUtil.java:40, took 0.217912 s
2019-09-11 17:32:07,030  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 19.799885 ms
2019-09-11 17:32:07,069  INFO org.apache.spark.SparkContext 54 - Invoking stop() from shutdown hook
2019-09-11 17:32:07,082  INFO org.spark_project.jetty.server.AbstractConnector 310 - Stopped Spark@630be695{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-09-11 17:32:07,091  INFO org.apache.spark.ui.SparkUI 54 - Stopped Spark web UI at http://10.6.1.14:4040
2019-09-11 17:32:07,122  INFO org.apache.spark.MapOutputTrackerMasterEndpoint 54 - MapOutputTrackerMasterEndpoint stopped!
2019-09-11 17:32:07,167  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore cleared
2019-09-11 17:32:07,168  INFO org.apache.spark.storage.BlockManager 54 - BlockManager stopped
2019-09-11 17:32:07,172  INFO org.apache.spark.storage.BlockManagerMaster 54 - BlockManagerMaster stopped
2019-09-11 17:32:07,178  INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint 54 - OutputCommitCoordinator stopped!
2019-09-11 17:32:07,185  INFO org.apache.spark.SparkContext 54 - Successfully stopped SparkContext
2019-09-11 17:32:07,187  INFO org.apache.spark.util.ShutdownHookManager 54 - Shutdown hook called
2019-09-11 17:32:07,189  INFO org.apache.spark.util.ShutdownHookManager 54 - Deleting directory C:\Users\OUY\AppData\Local\Temp\spark-a82281ef-acab-47d9-8c29-d9621d9dad0a
2019-09-11 17:45:03,678  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-11 17:45:04,606  WARN org.apache.hadoop.util.NativeCodeLoader 62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-11 17:45:04,758 ERROR org.apache.hadoop.util.Shell 396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2430)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
	at com.datamodel.SparkUtil.CreateSession(SparkUtil.java:21)
	at com.datamodel.SparkUtil.readDataFromCsv(SparkUtil.java:26)
	at com.datamodel.SparkUtil.main(SparkUtil.java:39)
2019-09-11 17:45:05,027  INFO org.apache.spark.SparkContext 54 - Submitted application: splitCHWord
2019-09-11 17:45:05,075  INFO org.apache.spark.SecurityManager 54 - Changing view acls to: OUY
2019-09-11 17:45:05,077  INFO org.apache.spark.SecurityManager 54 - Changing modify acls to: OUY
2019-09-11 17:45:05,078  INFO org.apache.spark.SecurityManager 54 - Changing view acls groups to: 
2019-09-11 17:45:05,083  INFO org.apache.spark.SecurityManager 54 - Changing modify acls groups to: 
2019-09-11 17:45:05,086  INFO org.apache.spark.SecurityManager 54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(OUY); groups with view permissions: Set(); users  with modify permissions: Set(OUY); groups with modify permissions: Set()
2019-09-11 17:45:06,172  INFO org.apache.spark.util.Utils 54 - Successfully started service 'sparkDriver' on port 1391.
2019-09-11 17:45:06,210  INFO org.apache.spark.SparkEnv 54 - Registering MapOutputTracker
2019-09-11 17:45:06,249  INFO org.apache.spark.SparkEnv 54 - Registering BlockManagerMaster
2019-09-11 17:45:06,255  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-09-11 17:45:06,256  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - BlockManagerMasterEndpoint up
2019-09-11 17:45:06,276  INFO org.apache.spark.storage.DiskBlockManager 54 - Created local directory at C:\Users\OUY\AppData\Local\Temp\blockmgr-1a864f88-230a-4aea-8945-ee98ac070087
2019-09-11 17:45:06,324  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore started with capacity 899.7 MB
2019-09-11 17:45:06,443  INFO org.apache.spark.SparkEnv 54 - Registering OutputCommitCoordinator
2019-09-11 17:45:06,599  INFO org.spark_project.jetty.util.log 192 - Logging initialized @6822ms
2019-09-11 17:45:06,708  INFO org.spark_project.jetty.server.Server 345 - jetty-9.3.z-SNAPSHOT
2019-09-11 17:45:06,731  INFO org.spark_project.jetty.server.Server 403 - Started @6958ms
2019-09-11 17:45:06,774  INFO org.spark_project.jetty.server.AbstractConnector 270 - Started ServerConnector@b8178f9{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-09-11 17:45:06,774  INFO org.apache.spark.util.Utils 54 - Successfully started service 'SparkUI' on port 4040.
2019-09-11 17:45:06,827  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1aa6e3c0{/jobs,null,AVAILABLE,@Spark}
2019-09-11 17:45:06,828  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@35cd68d4{/jobs/json,null,AVAILABLE,@Spark}
2019-09-11 17:45:06,829  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@216914{/jobs/job,null,AVAILABLE,@Spark}
2019-09-11 17:45:06,831  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5d5160e6{/jobs/job/json,null,AVAILABLE,@Spark}
2019-09-11 17:45:06,832  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2903c6ff{/stages,null,AVAILABLE,@Spark}
2019-09-11 17:45:06,836  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@37af1f93{/stages/json,null,AVAILABLE,@Spark}
2019-09-11 17:45:06,837  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@408e96d9{/stages/stage,null,AVAILABLE,@Spark}
2019-09-11 17:45:06,840  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@10cd6753{/stages/stage/json,null,AVAILABLE,@Spark}
2019-09-11 17:45:06,843  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@47af099e{/stages/pool,null,AVAILABLE,@Spark}
2019-09-11 17:45:06,845  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@b835727{/stages/pool/json,null,AVAILABLE,@Spark}
2019-09-11 17:45:06,846  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2c8662ac{/storage,null,AVAILABLE,@Spark}
2019-09-11 17:45:06,850  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3724b43e{/storage/json,null,AVAILABLE,@Spark}
2019-09-11 17:45:06,853  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@68e7c8c3{/storage/rdd,null,AVAILABLE,@Spark}
2019-09-11 17:45:06,856  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@238bfd6c{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-09-11 17:45:06,859  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@58860997{/environment,null,AVAILABLE,@Spark}
2019-09-11 17:45:06,864  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7487b142{/environment/json,null,AVAILABLE,@Spark}
2019-09-11 17:45:06,867  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@199bc830{/executors,null,AVAILABLE,@Spark}
2019-09-11 17:45:06,869  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@27b45ea{/executors/json,null,AVAILABLE,@Spark}
2019-09-11 17:45:06,871  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@790a251b{/executors/threadDump,null,AVAILABLE,@Spark}
2019-09-11 17:45:06,881  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@150ede8b{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-09-11 17:45:06,900  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3e15bb06{/static,null,AVAILABLE,@Spark}
2019-09-11 17:45:06,902  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6ba7383d{/,null,AVAILABLE,@Spark}
2019-09-11 17:45:06,904  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@710d89e2{/api,null,AVAILABLE,@Spark}
2019-09-11 17:45:06,906  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5e048149{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-09-11 17:45:06,907  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3d5790ea{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-09-11 17:45:06,914  INFO org.apache.spark.ui.SparkUI 54 - Bound SparkUI to 0.0.0.0, and started at http://10.6.1.14:4040
2019-09-11 17:45:07,196  INFO org.apache.spark.executor.Executor 54 - Starting executor ID driver on host localhost
2019-09-11 17:45:07,249  INFO org.apache.spark.util.Utils 54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 1404.
2019-09-11 17:45:07,250  INFO org.apache.spark.network.netty.NettyBlockTransferService 54 - Server created on 10.6.1.14:1404
2019-09-11 17:45:07,253  INFO org.apache.spark.storage.BlockManager 54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-09-11 17:45:07,257  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registering BlockManager BlockManagerId(driver, 10.6.1.14, 1404, None)
2019-09-11 17:45:07,264  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Registering block manager 10.6.1.14:1404 with 899.7 MB RAM, BlockManagerId(driver, 10.6.1.14, 1404, None)
2019-09-11 17:45:07,272  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registered BlockManager BlockManagerId(driver, 10.6.1.14, 1404, None)
2019-09-11 17:45:07,273  INFO org.apache.spark.storage.BlockManager 54 - Initialized BlockManager: BlockManagerId(driver, 10.6.1.14, 1404, None)
2019-09-11 17:45:07,719  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@e76b097{/metrics/json,null,AVAILABLE,@Spark}
2019-09-11 17:45:08,269  INFO org.apache.spark.sql.internal.SharedState 54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/MyProject/splitCHword/spark-warehouse/').
2019-09-11 17:45:08,286  INFO org.apache.spark.sql.internal.SharedState 54 - Warehouse path is 'file:/D:/MyProject/splitCHword/spark-warehouse/'.
2019-09-11 17:45:08,305  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7c52fc81{/SQL,null,AVAILABLE,@Spark}
2019-09-11 17:45:08,308  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2806d6da{/SQL/json,null,AVAILABLE,@Spark}
2019-09-11 17:45:08,313  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4f4c88f9{/SQL/execution,null,AVAILABLE,@Spark}
2019-09-11 17:45:08,316  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@cb39552{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-09-11 17:45:08,327  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@261db982{/static/sql,null,AVAILABLE,@Spark}
2019-09-11 17:45:10,385  INFO org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef 54 - Registered StateStoreCoordinator endpoint
2019-09-11 17:45:11,199  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0 stored as values in memory (estimated size 219.8 KB, free 899.5 MB)
2019-09-11 17:45:11,316  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-11 17:45:11,323  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_0_piece0 in memory on 10.6.1.14:1404 (size: 20.7 KB, free: 899.7 MB)
2019-09-11 17:45:11,328  INFO org.apache.spark.SparkContext 54 - Created broadcast 0 from load at SparkUtil.java:30
2019-09-11 17:45:11,821  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-11 17:45:11,824  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-11 17:45:11,844  INFO org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 413 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 326
2019-09-11 17:45:11,913  INFO org.apache.spark.SparkContext 54 - Starting job: load at SparkUtil.java:30
2019-09-11 17:45:11,945  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 0 (load at SparkUtil.java:30) with 1 output partitions
2019-09-11 17:45:11,945  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 0 (load at SparkUtil.java:30)
2019-09-11 17:45:11,946  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-11 17:45:11,949  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-11 17:45:11,960  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:30), which has no missing parents
2019-09-11 17:45:12,022  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1 stored as values in memory (estimated size 5.5 KB, free 899.5 MB)
2019-09-11 17:45:12,045  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 899.5 MB)
2019-09-11 17:45:12,048  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_1_piece0 in memory on 10.6.1.14:1404 (size: 3.4 KB, free: 899.7 MB)
2019-09-11 17:45:12,051  INFO org.apache.spark.SparkContext 54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-09-11 17:45:12,103  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:30) (first 15 tasks are for partitions Vector(0))
2019-09-11 17:45:12,107  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 0.0 with 1 tasks
2019-09-11 17:45:12,197  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes)
2019-09-11 17:45:12,233  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 0.0 (TID 0)
2019-09-11 17:45:12,355  INFO org.apache.spark.rdd.BinaryFileRDD 54 - Input split: Paths:/D:/MyProject/splitCHword/target/classes/test.csv:0+326
2019-09-11 17:45:12,581  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 0.0 (TID 0). 896 bytes result sent to driver
2019-09-11 17:45:12,594  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 0.0 (TID 0) in 422 ms on localhost (executor driver) (1/1)
2019-09-11 17:45:12,598  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-09-11 17:45:12,608  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 0 (load at SparkUtil.java:30) finished in 0.463 s
2019-09-11 17:45:12,620  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 0 finished: load at SparkUtil.java:30, took 0.705292 s
2019-09-11 17:45:14,918  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_0_piece0 on 10.6.1.14:1404 in memory (size: 20.7 KB, free: 899.7 MB)
2019-09-11 17:45:14,931  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_1_piece0 on 10.6.1.14:1404 in memory (size: 3.4 KB, free: 899.7 MB)
2019-09-11 17:45:15,663  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Pruning directories with: 
2019-09-11 17:45:15,669  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Post-Scan Filters: 
2019-09-11 17:45:15,676  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Output Data Schema: struct<label: string, feature: string>
2019-09-11 17:45:15,688  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Pushed Filters: 
2019-09-11 17:45:16,312  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 333.597298 ms
2019-09-11 17:45:16,324  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2 stored as values in memory (estimated size 220.5 KB, free 899.5 MB)
2019-09-11 17:45:16,351  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-11 17:45:16,354  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_2_piece0 in memory on 10.6.1.14:1404 (size: 20.7 KB, free: 899.7 MB)
2019-09-11 17:45:16,356  INFO org.apache.spark.SparkContext 54 - Created broadcast 2 from show at SparkUtil.java:40
2019-09-11 17:45:16,376  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2019-09-11 17:45:16,461  INFO org.apache.spark.SparkContext 54 - Starting job: show at SparkUtil.java:40
2019-09-11 17:45:16,463  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 1 (show at SparkUtil.java:40) with 1 output partitions
2019-09-11 17:45:16,464  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 1 (show at SparkUtil.java:40)
2019-09-11 17:45:16,465  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-11 17:45:16,465  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-11 17:45:16,467  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:40), which has no missing parents
2019-09-11 17:45:16,509  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3 stored as values in memory (estimated size 10.5 KB, free 899.5 MB)
2019-09-11 17:45:16,556  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.7 KB, free 899.4 MB)
2019-09-11 17:45:16,557  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_3_piece0 in memory on 10.6.1.14:1404 (size: 5.7 KB, free: 899.7 MB)
2019-09-11 17:45:16,558  INFO org.apache.spark.SparkContext 54 - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2019-09-11 17:45:16,559  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:40) (first 15 tasks are for partitions Vector(0))
2019-09-11 17:45:16,559  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 1.0 with 1 tasks
2019-09-11 17:45:16,577  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5293 bytes)
2019-09-11 17:45:16,578  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 1.0 (TID 1)
2019-09-11 17:45:16,605  INFO org.apache.spark.sql.execution.datasources.FileScanRDD 54 - Reading File path: file:///D:/MyProject/splitCHword/target/classes/test.csv, range: 0-326, partition values: [empty row]
2019-09-11 17:45:16,649  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 36.482117 ms
2019-09-11 17:45:16,705  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 1.0 (TID 1). 1491 bytes result sent to driver
2019-09-11 17:45:16,709  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 1.0 (TID 1) in 149 ms on localhost (executor driver) (1/1)
2019-09-11 17:45:16,710  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-09-11 17:45:16,711  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 1 (show at SparkUtil.java:40) finished in 0.150 s
2019-09-11 17:45:16,714  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 1 finished: show at SparkUtil.java:40, took 0.251698 s
2019-09-11 17:45:16,779  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 33.719453 ms
2019-09-11 17:45:16,837  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Pruning directories with: 
2019-09-11 17:45:16,838  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Post-Scan Filters: 
2019-09-11 17:45:16,838  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Output Data Schema: struct<label: string, feature: string>
2019-09-11 17:45:16,838  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Pushed Filters: 
2019-09-11 17:45:16,862  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 14.389754 ms
2019-09-11 17:45:16,874  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_4 stored as values in memory (estimated size 220.5 KB, free 899.2 MB)
2019-09-11 17:45:16,905  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_4_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.2 MB)
2019-09-11 17:45:16,909  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_4_piece0 in memory on 10.6.1.14:1404 (size: 20.7 KB, free: 899.7 MB)
2019-09-11 17:45:16,912  INFO org.apache.spark.SparkContext 54 - Created broadcast 4 from show at SparkUtil.java:41
2019-09-11 17:45:16,914  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2019-09-11 17:45:16,929  INFO org.apache.spark.SparkContext 54 - Starting job: show at SparkUtil.java:41
2019-09-11 17:45:16,930  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 2 (show at SparkUtil.java:41) with 1 output partitions
2019-09-11 17:45:16,930  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 2 (show at SparkUtil.java:41)
2019-09-11 17:45:16,931  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-11 17:45:16,931  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-11 17:45:16,932  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 2 (MapPartitionsRDD[9] at show at SparkUtil.java:41), which has no missing parents
2019-09-11 17:45:16,936  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_5 stored as values in memory (estimated size 9.4 KB, free 899.2 MB)
2019-09-11 17:45:16,943  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_5_piece0 stored as bytes in memory (estimated size 5.3 KB, free 899.2 MB)
2019-09-11 17:45:16,947  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_5_piece0 in memory on 10.6.1.14:1404 (size: 5.3 KB, free: 899.6 MB)
2019-09-11 17:45:16,947  INFO org.apache.spark.SparkContext 54 - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
2019-09-11 17:45:16,949  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at show at SparkUtil.java:41) (first 15 tasks are for partitions Vector(0))
2019-09-11 17:45:16,949  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 2.0 with 1 tasks
2019-09-11 17:45:16,951  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 5293 bytes)
2019-09-11 17:45:16,952  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 2.0 (TID 2)
2019-09-11 17:45:16,962  INFO org.apache.spark.sql.execution.datasources.FileScanRDD 54 - Reading File path: file:///D:/MyProject/splitCHword/target/classes/test.csv, range: 0-326, partition values: [empty row]
2019-09-11 17:45:16,970  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 2.0 (TID 2). 1478 bytes result sent to driver
2019-09-11 17:45:16,974  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 2.0 (TID 2) in 23 ms on localhost (executor driver) (1/1)
2019-09-11 17:45:16,975  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-09-11 17:45:16,976  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 2 (show at SparkUtil.java:41) finished in 0.025 s
2019-09-11 17:45:16,976  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 2 finished: show at SparkUtil.java:41, took 0.046946 s
2019-09-11 17:45:16,992  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 14.306981 ms
2019-09-11 17:45:16,999  INFO org.apache.spark.SparkContext 54 - Invoking stop() from shutdown hook
2019-09-11 17:45:17,011  INFO org.spark_project.jetty.server.AbstractConnector 310 - Stopped Spark@b8178f9{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-09-11 17:45:17,018  INFO org.apache.spark.ui.SparkUI 54 - Stopped Spark web UI at http://10.6.1.14:4040
2019-09-11 17:45:17,042  INFO org.apache.spark.MapOutputTrackerMasterEndpoint 54 - MapOutputTrackerMasterEndpoint stopped!
2019-09-11 17:45:17,108  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore cleared
2019-09-11 17:45:17,108  INFO org.apache.spark.storage.BlockManager 54 - BlockManager stopped
2019-09-11 17:45:17,111  INFO org.apache.spark.storage.BlockManagerMaster 54 - BlockManagerMaster stopped
2019-09-11 17:45:17,118  INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint 54 - OutputCommitCoordinator stopped!
2019-09-11 17:45:17,124  INFO org.apache.spark.SparkContext 54 - Successfully stopped SparkContext
2019-09-11 17:45:17,124  INFO org.apache.spark.util.ShutdownHookManager 54 - Shutdown hook called
2019-09-11 17:45:17,126  INFO org.apache.spark.util.ShutdownHookManager 54 - Deleting directory C:\Users\OUY\AppData\Local\Temp\spark-d09f5f41-0f1d-425d-af5e-0bbab22cf1cb
2019-09-12 08:53:30,814  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-12 08:53:31,845  WARN org.apache.hadoop.util.NativeCodeLoader 62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-12 08:53:32,032 ERROR org.apache.hadoop.util.Shell 396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2430)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
	at com.datamodel.SparkUtil.CreateSession(SparkUtil.java:21)
	at com.datamodel.SparkUtil.readDataFromCsv(SparkUtil.java:26)
	at com.datamodel.SparkUtil.main(SparkUtil.java:39)
2019-09-12 08:53:32,610  INFO org.apache.spark.SparkContext 54 - Submitted application: splitCHWord
2019-09-12 08:53:32,704  INFO org.apache.spark.SecurityManager 54 - Changing view acls to: OUY
2019-09-12 08:53:32,704  INFO org.apache.spark.SecurityManager 54 - Changing modify acls to: OUY
2019-09-12 08:53:32,704  INFO org.apache.spark.SecurityManager 54 - Changing view acls groups to: 
2019-09-12 08:53:32,704  INFO org.apache.spark.SecurityManager 54 - Changing modify acls groups to: 
2019-09-12 08:53:32,704  INFO org.apache.spark.SecurityManager 54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(OUY); groups with view permissions: Set(); users  with modify permissions: Set(OUY); groups with modify permissions: Set()
2019-09-12 08:53:33,954  INFO org.apache.spark.util.Utils 54 - Successfully started service 'sparkDriver' on port 2365.
2019-09-12 08:53:34,094  INFO org.apache.spark.SparkEnv 54 - Registering MapOutputTracker
2019-09-12 08:53:34,235  INFO org.apache.spark.SparkEnv 54 - Registering BlockManagerMaster
2019-09-12 08:53:34,235  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-09-12 08:53:34,235  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - BlockManagerMasterEndpoint up
2019-09-12 08:53:34,266  INFO org.apache.spark.storage.DiskBlockManager 54 - Created local directory at C:\Users\OUY\AppData\Local\Temp\blockmgr-f3ae2501-ca60-4f31-9ee7-a9753ac77dca
2019-09-12 08:53:34,360  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore started with capacity 899.7 MB
2019-09-12 08:53:34,501  INFO org.apache.spark.SparkEnv 54 - Registering OutputCommitCoordinator
2019-09-12 08:53:34,797  INFO org.spark_project.jetty.util.log 192 - Logging initialized @8296ms
2019-09-12 08:53:34,922  INFO org.spark_project.jetty.server.Server 345 - jetty-9.3.z-SNAPSHOT
2019-09-12 08:53:34,954  INFO org.spark_project.jetty.server.Server 403 - Started @8455ms
2019-09-12 08:53:34,985  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2019-09-12 08:53:34,985  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
2019-09-12 08:53:35,001  INFO org.spark_project.jetty.server.AbstractConnector 270 - Started ServerConnector@f08fdce{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 08:53:35,001  INFO org.apache.spark.util.Utils 54 - Successfully started service 'SparkUI' on port 4042.
2019-09-12 08:53:35,063  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7fcf294e{/jobs,null,AVAILABLE,@Spark}
2019-09-12 08:53:35,079  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@216914{/jobs/json,null,AVAILABLE,@Spark}
2019-09-12 08:53:35,079  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@35764bef{/jobs/job,null,AVAILABLE,@Spark}
2019-09-12 08:53:35,079  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2903c6ff{/jobs/job/json,null,AVAILABLE,@Spark}
2019-09-12 08:53:35,079  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@37af1f93{/stages,null,AVAILABLE,@Spark}
2019-09-12 08:53:35,079  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@408e96d9{/stages/json,null,AVAILABLE,@Spark}
2019-09-12 08:53:35,079  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@168cd36b{/stages/stage,null,AVAILABLE,@Spark}
2019-09-12 08:53:35,079  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@47af099e{/stages/stage/json,null,AVAILABLE,@Spark}
2019-09-12 08:53:35,079  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@b835727{/stages/pool,null,AVAILABLE,@Spark}
2019-09-12 08:53:35,079  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2c8662ac{/stages/pool/json,null,AVAILABLE,@Spark}
2019-09-12 08:53:35,079  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3724b43e{/storage,null,AVAILABLE,@Spark}
2019-09-12 08:53:35,079  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@68e7c8c3{/storage/json,null,AVAILABLE,@Spark}
2019-09-12 08:53:35,079  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@238bfd6c{/storage/rdd,null,AVAILABLE,@Spark}
2019-09-12 08:53:35,079  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@58860997{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-09-12 08:53:35,094  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7487b142{/environment,null,AVAILABLE,@Spark}
2019-09-12 08:53:35,094  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@199bc830{/environment/json,null,AVAILABLE,@Spark}
2019-09-12 08:53:35,094  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@27b45ea{/executors,null,AVAILABLE,@Spark}
2019-09-12 08:53:35,094  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@790a251b{/executors/json,null,AVAILABLE,@Spark}
2019-09-12 08:53:35,094  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@150ede8b{/executors/threadDump,null,AVAILABLE,@Spark}
2019-09-12 08:53:35,094  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3e15bb06{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-09-12 08:53:35,125  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4e1ce44{/static,null,AVAILABLE,@Spark}
2019-09-12 08:53:35,125  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@710d89e2{/,null,AVAILABLE,@Spark}
2019-09-12 08:53:35,125  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4fc142ec{/api,null,AVAILABLE,@Spark}
2019-09-12 08:53:35,125  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3d5790ea{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-09-12 08:53:35,125  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@67a3bd51{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-09-12 08:53:35,125  INFO org.apache.spark.ui.SparkUI 54 - Bound SparkUI to 0.0.0.0, and started at http://10.6.1.14:4042
2019-09-12 08:53:35,844  INFO org.apache.spark.executor.Executor 54 - Starting executor ID driver on host localhost
2019-09-12 08:53:36,032  INFO org.apache.spark.util.Utils 54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 2379.
2019-09-12 08:53:36,032  INFO org.apache.spark.network.netty.NettyBlockTransferService 54 - Server created on 10.6.1.14:2379
2019-09-12 08:53:36,047  INFO org.apache.spark.storage.BlockManager 54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-09-12 08:53:36,094  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registering BlockManager BlockManagerId(driver, 10.6.1.14, 2379, None)
2019-09-12 08:53:36,110  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Registering block manager 10.6.1.14:2379 with 899.7 MB RAM, BlockManagerId(driver, 10.6.1.14, 2379, None)
2019-09-12 08:53:36,125  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registered BlockManager BlockManagerId(driver, 10.6.1.14, 2379, None)
2019-09-12 08:53:36,125  INFO org.apache.spark.storage.BlockManager 54 - Initialized BlockManager: BlockManagerId(driver, 10.6.1.14, 2379, None)
2019-09-12 08:53:36,547  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1491344a{/metrics/json,null,AVAILABLE,@Spark}
2019-09-12 08:53:36,844  INFO org.apache.spark.sql.internal.SharedState 54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/MyProject/splitCHword/spark-warehouse/').
2019-09-12 08:53:36,844  INFO org.apache.spark.sql.internal.SharedState 54 - Warehouse path is 'file:/D:/MyProject/splitCHword/spark-warehouse/'.
2019-09-12 08:53:36,859  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6a12c7a8{/SQL,null,AVAILABLE,@Spark}
2019-09-12 08:53:36,859  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@436bd4df{/SQL/json,null,AVAILABLE,@Spark}
2019-09-12 08:53:36,859  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@67f77f6e{/SQL/execution,null,AVAILABLE,@Spark}
2019-09-12 08:53:36,859  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7be1ce6a{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-09-12 08:53:36,859  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2ddb3ae8{/static/sql,null,AVAILABLE,@Spark}
2019-09-12 08:53:38,484  INFO org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef 54 - Registered StateStoreCoordinator endpoint
2019-09-12 08:53:39,640  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0 stored as values in memory (estimated size 219.8 KB, free 899.5 MB)
2019-09-12 08:53:39,921  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 08:53:39,937  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_0_piece0 in memory on 10.6.1.14:2379 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 08:53:39,937  INFO org.apache.spark.SparkContext 54 - Created broadcast 0 from load at SparkUtil.java:30
2019-09-12 08:53:40,640  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 08:53:40,640  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 08:53:40,702  INFO org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 413 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 326
2019-09-12 08:53:40,796  INFO org.apache.spark.SparkContext 54 - Starting job: load at SparkUtil.java:30
2019-09-12 08:53:40,890  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 0 (load at SparkUtil.java:30) with 1 output partitions
2019-09-12 08:53:40,890  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 0 (load at SparkUtil.java:30)
2019-09-12 08:53:40,890  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 08:53:40,890  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 08:53:40,921  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:30), which has no missing parents
2019-09-12 08:53:40,983  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1 stored as values in memory (estimated size 5.5 KB, free 899.5 MB)
2019-09-12 08:53:40,999  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 899.5 MB)
2019-09-12 08:53:40,999  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_1_piece0 in memory on 10.6.1.14:2379 (size: 3.4 KB, free: 899.7 MB)
2019-09-12 08:53:40,999  INFO org.apache.spark.SparkContext 54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-09-12 08:53:41,030  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:30) (first 15 tasks are for partitions Vector(0))
2019-09-12 08:53:41,030  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 0.0 with 1 tasks
2019-09-12 08:53:41,124  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes)
2019-09-12 08:53:41,140  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 0.0 (TID 0)
2019-09-12 08:53:41,405  INFO org.apache.spark.rdd.BinaryFileRDD 54 - Input split: Paths:/D:/MyProject/splitCHword/target/classes/test.csv:0+326
2019-09-12 08:53:41,765  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 0.0 (TID 0). 853 bytes result sent to driver
2019-09-12 08:53:41,780  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 0.0 (TID 0) in 687 ms on localhost (executor driver) (1/1)
2019-09-12 08:53:41,780  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-09-12 08:53:41,827  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 0 (load at SparkUtil.java:30) finished in 0.750 s
2019-09-12 08:53:41,859  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 0 finished: load at SparkUtil.java:30, took 1.059811 s
2019-09-12 08:53:43,077  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_1_piece0 on 10.6.1.14:2379 in memory (size: 3.4 KB, free: 899.7 MB)
2019-09-12 08:53:44,592  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Pruning directories with: 
2019-09-12 08:53:44,592  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Post-Scan Filters: 
2019-09-12 08:53:44,592  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Output Data Schema: struct<feature: string>
2019-09-12 08:53:44,733  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_0_piece0 on 10.6.1.14:2379 in memory (size: 20.7 KB, free: 899.7 MB)
2019-09-12 08:53:44,748  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Pushed Filters: 
2019-09-12 08:53:45,420  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 281.08904 ms
2019-09-12 08:53:45,436  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2 stored as values in memory (estimated size 220.5 KB, free 899.5 MB)
2019-09-12 08:53:45,451  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 08:53:45,451  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_2_piece0 in memory on 10.6.1.14:2379 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 08:53:45,451  INFO org.apache.spark.SparkContext 54 - Created broadcast 2 from show at SparkUtil.java:40
2019-09-12 08:53:45,482  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2019-09-12 08:53:45,607  INFO org.apache.spark.SparkContext 54 - Starting job: show at SparkUtil.java:40
2019-09-12 08:53:45,607  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 1 (show at SparkUtil.java:40) with 1 output partitions
2019-09-12 08:53:45,607  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 1 (show at SparkUtil.java:40)
2019-09-12 08:53:45,607  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 08:53:45,607  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 08:53:45,607  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:40), which has no missing parents
2019-09-12 08:53:45,670  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3 stored as values in memory (estimated size 9.3 KB, free 899.5 MB)
2019-09-12 08:53:45,670  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.3 KB, free 899.5 MB)
2019-09-12 08:53:45,670  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_3_piece0 in memory on 10.6.1.14:2379 (size: 5.3 KB, free: 899.7 MB)
2019-09-12 08:53:45,670  INFO org.apache.spark.SparkContext 54 - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2019-09-12 08:53:45,670  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:40) (first 15 tasks are for partitions Vector(0))
2019-09-12 08:53:45,670  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 1.0 with 1 tasks
2019-09-12 08:53:45,685  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5293 bytes)
2019-09-12 08:53:45,701  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 1.0 (TID 1)
2019-09-12 08:53:45,732  INFO org.apache.spark.sql.execution.datasources.FileScanRDD 54 - Reading File path: file:///D:/MyProject/splitCHword/target/classes/test.csv, range: 0-326, partition values: [empty row]
2019-09-12 08:53:45,795  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 16.732999 ms
2019-09-12 08:53:45,842  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 1.0 (TID 1). 1502 bytes result sent to driver
2019-09-12 08:53:45,842  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 1.0 (TID 1) in 172 ms on localhost (executor driver) (1/1)
2019-09-12 08:53:45,842  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-09-12 08:53:45,842  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 1 (show at SparkUtil.java:40) finished in 0.172 s
2019-09-12 08:53:45,842  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 1 finished: show at SparkUtil.java:40, took 0.243380 s
2019-09-12 08:53:45,904  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 23.654807 ms
2019-09-12 08:53:45,951  INFO org.apache.spark.SparkContext 54 - Invoking stop() from shutdown hook
2019-09-12 08:53:45,982  INFO org.spark_project.jetty.server.AbstractConnector 310 - Stopped Spark@f08fdce{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 08:53:45,982  INFO org.apache.spark.ui.SparkUI 54 - Stopped Spark web UI at http://10.6.1.14:4042
2019-09-12 08:53:46,014  INFO org.apache.spark.MapOutputTrackerMasterEndpoint 54 - MapOutputTrackerMasterEndpoint stopped!
2019-09-12 08:53:46,060  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore cleared
2019-09-12 08:53:46,060  INFO org.apache.spark.storage.BlockManager 54 - BlockManager stopped
2019-09-12 08:53:46,076  INFO org.apache.spark.storage.BlockManagerMaster 54 - BlockManagerMaster stopped
2019-09-12 08:53:46,092  INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint 54 - OutputCommitCoordinator stopped!
2019-09-12 08:53:46,107  INFO org.apache.spark.SparkContext 54 - Successfully stopped SparkContext
2019-09-12 08:53:46,107  INFO org.apache.spark.util.ShutdownHookManager 54 - Shutdown hook called
2019-09-12 08:53:46,107  INFO org.apache.spark.util.ShutdownHookManager 54 - Deleting directory C:\Users\OUY\AppData\Local\Temp\spark-a1cc18d1-8cdd-4cc6-940c-cb683be70ff3
2019-09-12 09:02:27,574  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-12 09:02:28,199  WARN org.apache.hadoop.util.NativeCodeLoader 62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-12 09:02:28,277 ERROR org.apache.hadoop.util.Shell 396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2430)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
	at com.datamodel.SparkUtil.CreateSession(SparkUtil.java:22)
	at com.datamodel.SparkUtil.readDataFromCsv(SparkUtil.java:27)
	at com.datamodel.SparkUtil.main(SparkUtil.java:40)
2019-09-12 09:02:28,402  INFO org.apache.spark.SparkContext 54 - Submitted application: splitCHWord
2019-09-12 09:02:28,449  INFO org.apache.spark.SecurityManager 54 - Changing view acls to: OUY
2019-09-12 09:02:28,449  INFO org.apache.spark.SecurityManager 54 - Changing modify acls to: OUY
2019-09-12 09:02:28,449  INFO org.apache.spark.SecurityManager 54 - Changing view acls groups to: 
2019-09-12 09:02:28,449  INFO org.apache.spark.SecurityManager 54 - Changing modify acls groups to: 
2019-09-12 09:02:28,449  INFO org.apache.spark.SecurityManager 54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(OUY); groups with view permissions: Set(); users  with modify permissions: Set(OUY); groups with modify permissions: Set()
2019-09-12 09:02:29,230  INFO org.apache.spark.util.Utils 54 - Successfully started service 'sparkDriver' on port 2562.
2019-09-12 09:02:29,293  INFO org.apache.spark.SparkEnv 54 - Registering MapOutputTracker
2019-09-12 09:02:29,308  INFO org.apache.spark.SparkEnv 54 - Registering BlockManagerMaster
2019-09-12 09:02:29,308  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-09-12 09:02:29,308  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - BlockManagerMasterEndpoint up
2019-09-12 09:02:29,324  INFO org.apache.spark.storage.DiskBlockManager 54 - Created local directory at C:\Users\OUY\AppData\Local\Temp\blockmgr-faea3596-c59f-45dd-adcb-20b2e32aeeb9
2019-09-12 09:02:29,340  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore started with capacity 899.7 MB
2019-09-12 09:02:29,402  INFO org.apache.spark.SparkEnv 54 - Registering OutputCommitCoordinator
2019-09-12 09:02:29,511  INFO org.spark_project.jetty.util.log 192 - Logging initialized @4534ms
2019-09-12 09:02:29,574  INFO org.spark_project.jetty.server.Server 345 - jetty-9.3.z-SNAPSHOT
2019-09-12 09:02:29,589  INFO org.spark_project.jetty.server.Server 403 - Started @4618ms
2019-09-12 09:02:29,605  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2019-09-12 09:02:29,605  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
2019-09-12 09:02:29,621  INFO org.spark_project.jetty.server.AbstractConnector 270 - Started ServerConnector@6914bc2c{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 09:02:29,621  INFO org.apache.spark.util.Utils 54 - Successfully started service 'SparkUI' on port 4042.
2019-09-12 09:02:29,636  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@d5ae57e{/jobs,null,AVAILABLE,@Spark}
2019-09-12 09:02:29,636  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@52b56a3e{/jobs/json,null,AVAILABLE,@Spark}
2019-09-12 09:02:29,652  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4eed46ee{/jobs/job,null,AVAILABLE,@Spark}
2019-09-12 09:02:29,652  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@475835b1{/jobs/job/json,null,AVAILABLE,@Spark}
2019-09-12 09:02:29,652  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5241cf67{/stages,null,AVAILABLE,@Spark}
2019-09-12 09:02:29,652  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@77192705{/stages/json,null,AVAILABLE,@Spark}
2019-09-12 09:02:29,652  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7e809b79{/stages/stage,null,AVAILABLE,@Spark}
2019-09-12 09:02:29,652  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@89c10b7{/stages/stage/json,null,AVAILABLE,@Spark}
2019-09-12 09:02:29,652  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4fe89c24{/stages/pool,null,AVAILABLE,@Spark}
2019-09-12 09:02:29,652  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3d08f3f5{/stages/pool/json,null,AVAILABLE,@Spark}
2019-09-12 09:02:29,652  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1a1da881{/storage,null,AVAILABLE,@Spark}
2019-09-12 09:02:29,652  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7fd4acee{/storage/json,null,AVAILABLE,@Spark}
2019-09-12 09:02:29,652  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6175619b{/storage/rdd,null,AVAILABLE,@Spark}
2019-09-12 09:02:29,652  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@756cf158{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-09-12 09:02:29,652  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@751d3241{/environment,null,AVAILABLE,@Spark}
2019-09-12 09:02:29,667  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@64337702{/environment/json,null,AVAILABLE,@Spark}
2019-09-12 09:02:29,667  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@30ea8c23{/executors,null,AVAILABLE,@Spark}
2019-09-12 09:02:29,667  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4e76dac{/executors/json,null,AVAILABLE,@Spark}
2019-09-12 09:02:29,667  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5f2f577{/executors/threadDump,null,AVAILABLE,@Spark}
2019-09-12 09:02:29,667  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5d465e4b{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-09-12 09:02:29,667  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@41a90fa8{/static,null,AVAILABLE,@Spark}
2019-09-12 09:02:29,667  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@35beb15e{/,null,AVAILABLE,@Spark}
2019-09-12 09:02:29,667  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5ac86ba5{/api,null,AVAILABLE,@Spark}
2019-09-12 09:02:29,683  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@63c5efee{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-09-12 09:02:29,683  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1c98290c{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-09-12 09:02:29,683  INFO org.apache.spark.ui.SparkUI 54 - Bound SparkUI to 0.0.0.0, and started at http://10.6.1.14:4042
2019-09-12 09:02:29,855  INFO org.apache.spark.executor.Executor 54 - Starting executor ID driver on host localhost
2019-09-12 09:02:29,902  INFO org.apache.spark.util.Utils 54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 2575.
2019-09-12 09:02:29,902  INFO org.apache.spark.network.netty.NettyBlockTransferService 54 - Server created on 10.6.1.14:2575
2019-09-12 09:02:29,902  INFO org.apache.spark.storage.BlockManager 54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-09-12 09:02:29,917  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registering BlockManager BlockManagerId(driver, 10.6.1.14, 2575, None)
2019-09-12 09:02:29,917  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Registering block manager 10.6.1.14:2575 with 899.7 MB RAM, BlockManagerId(driver, 10.6.1.14, 2575, None)
2019-09-12 09:02:29,917  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registered BlockManager BlockManagerId(driver, 10.6.1.14, 2575, None)
2019-09-12 09:02:29,917  INFO org.apache.spark.storage.BlockManager 54 - Initialized BlockManager: BlockManagerId(driver, 10.6.1.14, 2575, None)
2019-09-12 09:02:30,230  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@350b3a17{/metrics/json,null,AVAILABLE,@Spark}
2019-09-12 09:02:30,324  INFO org.apache.spark.sql.internal.SharedState 54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/MyProject/splitCHword/spark-warehouse/').
2019-09-12 09:02:30,324  INFO org.apache.spark.sql.internal.SharedState 54 - Warehouse path is 'file:/D:/MyProject/splitCHword/spark-warehouse/'.
2019-09-12 09:02:30,324  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@64a896b0{/SQL,null,AVAILABLE,@Spark}
2019-09-12 09:02:30,324  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@11a82d0f{/SQL/json,null,AVAILABLE,@Spark}
2019-09-12 09:02:30,324  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@73d6d0c{/SQL/execution,null,AVAILABLE,@Spark}
2019-09-12 09:02:30,324  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3961a41a{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-09-12 09:02:30,339  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1fd386c3{/static/sql,null,AVAILABLE,@Spark}
2019-09-12 09:02:31,464  INFO org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef 54 - Registered StateStoreCoordinator endpoint
2019-09-12 09:02:31,933  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0 stored as values in memory (estimated size 219.8 KB, free 899.5 MB)
2019-09-12 09:02:32,042  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 09:02:32,058  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_0_piece0 in memory on 10.6.1.14:2575 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 09:02:32,058  INFO org.apache.spark.SparkContext 54 - Created broadcast 0 from load at SparkUtil.java:31
2019-09-12 09:02:32,479  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 09:02:32,495  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 09:02:32,495  INFO org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 413 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 326
2019-09-12 09:02:32,557  INFO org.apache.spark.SparkContext 54 - Starting job: load at SparkUtil.java:31
2019-09-12 09:02:32,573  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 0 (load at SparkUtil.java:31) with 1 output partitions
2019-09-12 09:02:32,573  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 0 (load at SparkUtil.java:31)
2019-09-12 09:02:32,573  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 09:02:32,573  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 09:02:32,573  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:31), which has no missing parents
2019-09-12 09:02:32,604  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1 stored as values in memory (estimated size 5.5 KB, free 899.5 MB)
2019-09-12 09:02:32,620  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 899.5 MB)
2019-09-12 09:02:32,620  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_1_piece0 in memory on 10.6.1.14:2575 (size: 3.4 KB, free: 899.7 MB)
2019-09-12 09:02:32,636  INFO org.apache.spark.SparkContext 54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-09-12 09:02:32,651  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:31) (first 15 tasks are for partitions Vector(0))
2019-09-12 09:02:32,651  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 0.0 with 1 tasks
2019-09-12 09:02:32,714  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes)
2019-09-12 09:02:32,714  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 0.0 (TID 0)
2019-09-12 09:02:32,792  INFO org.apache.spark.rdd.BinaryFileRDD 54 - Input split: Paths:/D:/MyProject/splitCHword/target/classes/test.csv:0+326
2019-09-12 09:02:32,948  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 0.0 (TID 0). 853 bytes result sent to driver
2019-09-12 09:02:32,964  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 0.0 (TID 0) in 282 ms on localhost (executor driver) (1/1)
2019-09-12 09:02:32,964  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-09-12 09:02:32,964  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 0 (load at SparkUtil.java:31) finished in 0.282 s
2019-09-12 09:02:32,979  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 0 finished: load at SparkUtil.java:31, took 0.421031 s
2019-09-12 09:02:33,557  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_1_piece0 on 10.6.1.14:2575 in memory (size: 3.4 KB, free: 899.7 MB)
2019-09-12 09:02:35,260  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_0_piece0 on 10.6.1.14:2575 in memory (size: 20.7 KB, free: 899.7 MB)
2019-09-12 09:02:35,307  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Pruning directories with: 
2019-09-12 09:02:35,322  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Post-Scan Filters: 
2019-09-12 09:02:35,322  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Output Data Schema: struct<feature: string>
2019-09-12 09:02:35,322  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Pushed Filters: 
2019-09-12 09:02:35,900  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 286.991542 ms
2019-09-12 09:02:35,916  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2 stored as values in memory (estimated size 220.5 KB, free 899.5 MB)
2019-09-12 09:02:35,932  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 09:02:35,932  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_2_piece0 in memory on 10.6.1.14:2575 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 09:02:35,932  INFO org.apache.spark.SparkContext 54 - Created broadcast 2 from show at SparkUtil.java:43
2019-09-12 09:02:35,963  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2019-09-12 09:02:36,025  INFO org.apache.spark.SparkContext 54 - Starting job: show at SparkUtil.java:43
2019-09-12 09:02:36,025  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 1 (show at SparkUtil.java:43) with 1 output partitions
2019-09-12 09:02:36,025  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 1 (show at SparkUtil.java:43)
2019-09-12 09:02:36,025  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 09:02:36,025  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 09:02:36,025  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:43), which has no missing parents
2019-09-12 09:02:36,057  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3 stored as values in memory (estimated size 14.2 KB, free 899.5 MB)
2019-09-12 09:02:36,057  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.9 KB, free 899.4 MB)
2019-09-12 09:02:36,057  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_3_piece0 in memory on 10.6.1.14:2575 (size: 6.9 KB, free: 899.7 MB)
2019-09-12 09:02:36,057  INFO org.apache.spark.SparkContext 54 - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2019-09-12 09:02:36,057  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:43) (first 15 tasks are for partitions Vector(0))
2019-09-12 09:02:36,057  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 1.0 with 1 tasks
2019-09-12 09:02:36,072  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5293 bytes)
2019-09-12 09:02:36,072  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 1.0 (TID 1)
2019-09-12 09:02:36,088  INFO org.apache.spark.sql.execution.datasources.FileScanRDD 54 - Reading File path: file:///D:/MyProject/splitCHword/target/classes/test.csv, range: 0-326, partition values: [empty row]
2019-09-12 09:02:36,103  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 10.414924 ms
2019-09-12 09:02:38,260  INFO org.ansj.util.MyStaticValue 106 - init version.number to env value is : 2.11.8
2019-09-12 09:02:38,260  INFO org.ansj.util.MyStaticValue 106 - init copyright.string to env value is : Copyright 2002-2016, LAMP/EPFL
2019-09-12 09:02:38,260  INFO org.ansj.util.MyStaticValue 106 - init osgi.version.number to env value is : 2.11.8.v20160304-115712-1706a37eb8
2019-09-12 09:02:38,260  INFO org.ansj.util.MyStaticValue 106 - init maven.version.number to env value is : 2.11.8
2019-09-12 09:02:38,306  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/ambiguity.dic
2019-09-12 09:02:38,306 ERROR org.ansj.library.AmbiguityLibrary 117 - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:D:\MyProject\splitCHword\library\ambiguity.dic not found or can not to read, path: library/ambiguity.dic
2019-09-12 09:02:38,306  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/default.dic
2019-09-12 09:02:38,306 ERROR org.ansj.library.DicLibrary 221 - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:D:\MyProject\splitCHword\library\default.dic not found or can not to read, path: library/default.dic
2019-09-12 09:02:39,431  INFO org.ansj.library.DATDictionary 90 - init core library ok use time : 984
2019-09-12 09:02:39,962  INFO org.ansj.library.NgramLibrary 17 - init ngram ok use time :484
2019-09-12 09:02:39,993  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 1.0 (TID 1). 1280 bytes result sent to driver
2019-09-12 09:02:40,009  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 1.0 (TID 1) in 3952 ms on localhost (executor driver) (1/1)
2019-09-12 09:02:40,009  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 1 (show at SparkUtil.java:43) finished in 3.952 s
2019-09-12 09:02:40,009  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 1 finished: show at SparkUtil.java:43, took 3.979524 s
2019-09-12 09:02:40,009  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-09-12 09:02:40,040  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 11.752096 ms
2019-09-12 09:02:40,056  INFO org.apache.spark.SparkContext 54 - Invoking stop() from shutdown hook
2019-09-12 09:02:40,071  INFO org.spark_project.jetty.server.AbstractConnector 310 - Stopped Spark@6914bc2c{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 09:02:40,071  INFO org.apache.spark.ui.SparkUI 54 - Stopped Spark web UI at http://10.6.1.14:4042
2019-09-12 09:02:40,087  INFO org.apache.spark.MapOutputTrackerMasterEndpoint 54 - MapOutputTrackerMasterEndpoint stopped!
2019-09-12 09:02:40,118  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore cleared
2019-09-12 09:02:40,118  INFO org.apache.spark.storage.BlockManager 54 - BlockManager stopped
2019-09-12 09:02:40,118  INFO org.apache.spark.storage.BlockManagerMaster 54 - BlockManagerMaster stopped
2019-09-12 09:02:40,134  INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint 54 - OutputCommitCoordinator stopped!
2019-09-12 09:02:40,134  INFO org.apache.spark.SparkContext 54 - Successfully stopped SparkContext
2019-09-12 09:02:40,134  INFO org.apache.spark.util.ShutdownHookManager 54 - Shutdown hook called
2019-09-12 09:02:40,134  INFO org.apache.spark.util.ShutdownHookManager 54 - Deleting directory C:\Users\OUY\AppData\Local\Temp\spark-b1714c1a-9e41-44e9-9796-7210bc242ca9
2019-09-12 09:03:13,314  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-12 09:03:13,908  WARN org.apache.hadoop.util.NativeCodeLoader 62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-12 09:03:14,017 ERROR org.apache.hadoop.util.Shell 396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2430)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
	at com.datamodel.SparkUtil.CreateSession(SparkUtil.java:22)
	at com.datamodel.SparkUtil.readDataFromCsv(SparkUtil.java:27)
	at com.datamodel.SparkUtil.main(SparkUtil.java:40)
2019-09-12 09:03:14,267  INFO org.apache.spark.SparkContext 54 - Submitted application: splitCHWord
2019-09-12 09:03:14,330  INFO org.apache.spark.SecurityManager 54 - Changing view acls to: OUY
2019-09-12 09:03:14,330  INFO org.apache.spark.SecurityManager 54 - Changing modify acls to: OUY
2019-09-12 09:03:14,330  INFO org.apache.spark.SecurityManager 54 - Changing view acls groups to: 
2019-09-12 09:03:14,330  INFO org.apache.spark.SecurityManager 54 - Changing modify acls groups to: 
2019-09-12 09:03:14,330  INFO org.apache.spark.SecurityManager 54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(OUY); groups with view permissions: Set(); users  with modify permissions: Set(OUY); groups with modify permissions: Set()
2019-09-12 09:03:15,192  INFO org.apache.spark.util.Utils 54 - Successfully started service 'sparkDriver' on port 2599.
2019-09-12 09:03:15,239  INFO org.apache.spark.SparkEnv 54 - Registering MapOutputTracker
2019-09-12 09:03:15,286  INFO org.apache.spark.SparkEnv 54 - Registering BlockManagerMaster
2019-09-12 09:03:15,286  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-09-12 09:03:15,286  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - BlockManagerMasterEndpoint up
2019-09-12 09:03:15,301  INFO org.apache.spark.storage.DiskBlockManager 54 - Created local directory at C:\Users\OUY\AppData\Local\Temp\blockmgr-fcabd575-aa02-44b2-a66c-f03e47a7164f
2019-09-12 09:03:15,348  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore started with capacity 899.7 MB
2019-09-12 09:03:15,457  INFO org.apache.spark.SparkEnv 54 - Registering OutputCommitCoordinator
2019-09-12 09:03:15,567  INFO org.spark_project.jetty.util.log 192 - Logging initialized @4994ms
2019-09-12 09:03:15,645  INFO org.spark_project.jetty.server.Server 345 - jetty-9.3.z-SNAPSHOT
2019-09-12 09:03:15,661  INFO org.spark_project.jetty.server.Server 403 - Started @5090ms
2019-09-12 09:03:15,692  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2019-09-12 09:03:15,692  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
2019-09-12 09:03:15,707  INFO org.spark_project.jetty.server.AbstractConnector 270 - Started ServerConnector@438d4aa2{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 09:03:15,707  INFO org.apache.spark.util.Utils 54 - Successfully started service 'SparkUI' on port 4042.
2019-09-12 09:03:15,739  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5fe7f967{/jobs,null,AVAILABLE,@Spark}
2019-09-12 09:03:15,739  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5633dafd{/jobs/json,null,AVAILABLE,@Spark}
2019-09-12 09:03:15,739  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2eadc9f6{/jobs/job,null,AVAILABLE,@Spark}
2019-09-12 09:03:15,754  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@778d82e9{/jobs/job/json,null,AVAILABLE,@Spark}
2019-09-12 09:03:15,754  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@59901c4d{/stages,null,AVAILABLE,@Spark}
2019-09-12 09:03:15,754  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@d8d9199{/stages/json,null,AVAILABLE,@Spark}
2019-09-12 09:03:15,754  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@602ae7b6{/stages/stage,null,AVAILABLE,@Spark}
2019-09-12 09:03:15,754  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@13da7ab0{/stages/stage/json,null,AVAILABLE,@Spark}
2019-09-12 09:03:15,754  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@260ff5b7{/stages/pool,null,AVAILABLE,@Spark}
2019-09-12 09:03:15,754  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@77eb5790{/stages/pool/json,null,AVAILABLE,@Spark}
2019-09-12 09:03:15,754  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@319c3a25{/storage,null,AVAILABLE,@Spark}
2019-09-12 09:03:15,754  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@ef1695a{/storage/json,null,AVAILABLE,@Spark}
2019-09-12 09:03:15,754  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@81b5db0{/storage/rdd,null,AVAILABLE,@Spark}
2019-09-12 09:03:15,754  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7139bd31{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-09-12 09:03:15,754  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4b3fe06e{/environment,null,AVAILABLE,@Spark}
2019-09-12 09:03:15,754  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3e17a0a1{/environment/json,null,AVAILABLE,@Spark}
2019-09-12 09:03:15,754  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4d8286c4{/executors,null,AVAILABLE,@Spark}
2019-09-12 09:03:15,770  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@161f6623{/executors/json,null,AVAILABLE,@Spark}
2019-09-12 09:03:15,770  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6778aea6{/executors/threadDump,null,AVAILABLE,@Spark}
2019-09-12 09:03:15,770  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@69228e85{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-09-12 09:03:15,786  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5853495b{/static,null,AVAILABLE,@Spark}
2019-09-12 09:03:15,786  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@34b27915{/,null,AVAILABLE,@Spark}
2019-09-12 09:03:15,786  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1b9776f5{/api,null,AVAILABLE,@Spark}
2019-09-12 09:03:15,786  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@57402ba1{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-09-12 09:03:15,786  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@702b06fb{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-09-12 09:03:15,801  INFO org.apache.spark.ui.SparkUI 54 - Bound SparkUI to 0.0.0.0, and started at http://10.6.1.14:4042
2019-09-12 09:03:16,035  INFO org.apache.spark.executor.Executor 54 - Starting executor ID driver on host localhost
2019-09-12 09:03:16,082  INFO org.apache.spark.util.Utils 54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 2613.
2019-09-12 09:03:16,082  INFO org.apache.spark.network.netty.NettyBlockTransferService 54 - Server created on 10.6.1.14:2613
2019-09-12 09:03:16,098  INFO org.apache.spark.storage.BlockManager 54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-09-12 09:03:16,098  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registering BlockManager BlockManagerId(driver, 10.6.1.14, 2613, None)
2019-09-12 09:03:16,098  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Registering block manager 10.6.1.14:2613 with 899.7 MB RAM, BlockManagerId(driver, 10.6.1.14, 2613, None)
2019-09-12 09:03:16,114  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registered BlockManager BlockManagerId(driver, 10.6.1.14, 2613, None)
2019-09-12 09:03:16,114  INFO org.apache.spark.storage.BlockManager 54 - Initialized BlockManager: BlockManagerId(driver, 10.6.1.14, 2613, None)
2019-09-12 09:03:16,410  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5e922647{/metrics/json,null,AVAILABLE,@Spark}
2019-09-12 09:03:16,535  INFO org.apache.spark.sql.internal.SharedState 54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/MyProject/splitCHword/spark-warehouse/').
2019-09-12 09:03:16,535  INFO org.apache.spark.sql.internal.SharedState 54 - Warehouse path is 'file:/D:/MyProject/splitCHword/spark-warehouse/'.
2019-09-12 09:03:16,535  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@161aa04a{/SQL,null,AVAILABLE,@Spark}
2019-09-12 09:03:16,535  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6848a051{/SQL/json,null,AVAILABLE,@Spark}
2019-09-12 09:03:16,535  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2b0b7e5a{/SQL/execution,null,AVAILABLE,@Spark}
2019-09-12 09:03:16,551  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3119cf6f{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-09-12 09:03:16,551  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@79518e00{/static/sql,null,AVAILABLE,@Spark}
2019-09-12 09:03:17,707  INFO org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef 54 - Registered StateStoreCoordinator endpoint
2019-09-12 09:03:18,536  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0 stored as values in memory (estimated size 219.8 KB, free 899.5 MB)
2019-09-12 09:03:18,731  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 09:03:18,738  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_0_piece0 in memory on 10.6.1.14:2613 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 09:03:18,746  INFO org.apache.spark.SparkContext 54 - Created broadcast 0 from load at SparkUtil.java:31
2019-09-12 09:03:19,171  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 09:03:19,171  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 09:03:19,187  INFO org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 413 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 326
2019-09-12 09:03:19,281  INFO org.apache.spark.SparkContext 54 - Starting job: load at SparkUtil.java:31
2019-09-12 09:03:19,327  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 0 (load at SparkUtil.java:31) with 1 output partitions
2019-09-12 09:03:19,327  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 0 (load at SparkUtil.java:31)
2019-09-12 09:03:19,327  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 09:03:19,343  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 09:03:19,359  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:31), which has no missing parents
2019-09-12 09:03:19,562  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1 stored as values in memory (estimated size 5.5 KB, free 899.5 MB)
2019-09-12 09:03:19,577  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 899.5 MB)
2019-09-12 09:03:19,593  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_1_piece0 in memory on 10.6.1.14:2613 (size: 3.4 KB, free: 899.7 MB)
2019-09-12 09:03:19,593  INFO org.apache.spark.SparkContext 54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-09-12 09:03:19,655  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:31) (first 15 tasks are for partitions Vector(0))
2019-09-12 09:03:19,655  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 0.0 with 1 tasks
2019-09-12 09:03:19,749  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes)
2019-09-12 09:03:19,749  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 0.0 (TID 0)
2019-09-12 09:03:19,999  INFO org.apache.spark.rdd.BinaryFileRDD 54 - Input split: Paths:/D:/MyProject/splitCHword/target/classes/test.csv:0+326
2019-09-12 09:03:20,202  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 0.0 (TID 0). 853 bytes result sent to driver
2019-09-12 09:03:20,218  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 0.0 (TID 0) in 500 ms on localhost (executor driver) (1/1)
2019-09-12 09:03:20,218  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-09-12 09:03:20,233  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 0 (load at SparkUtil.java:31) finished in 0.531 s
2019-09-12 09:03:20,249  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 0 finished: load at SparkUtil.java:31, took 0.957160 s
2019-09-12 09:03:21,171  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_1_piece0 on 10.6.1.14:2613 in memory (size: 3.4 KB, free: 899.7 MB)
2019-09-12 09:03:23,061  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_0_piece0 on 10.6.1.14:2613 in memory (size: 20.7 KB, free: 899.7 MB)
2019-09-12 09:03:23,233  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Pruning directories with: 
2019-09-12 09:03:23,233  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Post-Scan Filters: 
2019-09-12 09:03:23,233  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Output Data Schema: struct<feature: string>
2019-09-12 09:03:23,248  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Pushed Filters: 
2019-09-12 09:03:23,889  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 314.907464 ms
2019-09-12 09:03:23,904  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2 stored as values in memory (estimated size 220.5 KB, free 899.5 MB)
2019-09-12 09:03:23,920  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 09:03:23,920  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_2_piece0 in memory on 10.6.1.14:2613 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 09:03:23,920  INFO org.apache.spark.SparkContext 54 - Created broadcast 2 from show at SparkUtil.java:43
2019-09-12 09:03:23,936  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2019-09-12 09:03:24,014  INFO org.apache.spark.SparkContext 54 - Starting job: show at SparkUtil.java:43
2019-09-12 09:03:24,014  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 1 (show at SparkUtil.java:43) with 1 output partitions
2019-09-12 09:03:24,014  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 1 (show at SparkUtil.java:43)
2019-09-12 09:03:24,014  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 09:03:24,014  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 09:03:24,014  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:43), which has no missing parents
2019-09-12 09:03:24,045  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3 stored as values in memory (estimated size 14.2 KB, free 899.5 MB)
2019-09-12 09:03:24,045  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.9 KB, free 899.4 MB)
2019-09-12 09:03:24,045  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_3_piece0 in memory on 10.6.1.14:2613 (size: 6.9 KB, free: 899.7 MB)
2019-09-12 09:03:24,045  INFO org.apache.spark.SparkContext 54 - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2019-09-12 09:03:24,045  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:43) (first 15 tasks are for partitions Vector(0))
2019-09-12 09:03:24,045  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 1.0 with 1 tasks
2019-09-12 09:03:24,061  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5293 bytes)
2019-09-12 09:03:24,061  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 1.0 (TID 1)
2019-09-12 09:03:24,076  INFO org.apache.spark.sql.execution.datasources.FileScanRDD 54 - Reading File path: file:///D:/MyProject/splitCHword/target/classes/test.csv, range: 0-326, partition values: [empty row]
2019-09-12 09:03:24,092  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 15.825907 ms
2019-09-12 09:03:24,373  INFO org.ansj.util.MyStaticValue 106 - init version.number to env value is : 2.11.8
2019-09-12 09:03:24,373  INFO org.ansj.util.MyStaticValue 106 - init copyright.string to env value is : Copyright 2002-2016, LAMP/EPFL
2019-09-12 09:03:24,373  INFO org.ansj.util.MyStaticValue 106 - init osgi.version.number to env value is : 2.11.8.v20160304-115712-1706a37eb8
2019-09-12 09:03:24,373  INFO org.ansj.util.MyStaticValue 106 - init maven.version.number to env value is : 2.11.8
2019-09-12 09:03:24,389  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/ambiguity.dic
2019-09-12 09:03:24,389 ERROR org.ansj.library.AmbiguityLibrary 117 - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:D:\MyProject\splitCHword\library\ambiguity.dic not found or can not to read, path: library/ambiguity.dic
2019-09-12 09:03:24,389  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/default.dic
2019-09-12 09:03:24,389 ERROR org.ansj.library.DicLibrary 221 - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:D:\MyProject\splitCHword\library\default.dic not found or can not to read, path: library/default.dic
2019-09-12 09:03:25,404  INFO org.ansj.library.DATDictionary 90 - init core library ok use time : 906
2019-09-12 09:03:25,920  INFO org.ansj.library.NgramLibrary 17 - init ngram ok use time :500
2019-09-12 09:03:25,951  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 1.0 (TID 1). 1237 bytes result sent to driver
2019-09-12 09:03:25,951  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 1.0 (TID 1) in 1906 ms on localhost (executor driver) (1/1)
2019-09-12 09:03:25,966  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 1 (show at SparkUtil.java:43) finished in 1.921 s
2019-09-12 09:03:25,966  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-09-12 09:03:25,966  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 1 finished: show at SparkUtil.java:43, took 1.956711 s
2019-09-12 09:03:25,998  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 12.36607 ms
2019-09-12 09:03:26,029  INFO org.apache.spark.SparkContext 54 - Invoking stop() from shutdown hook
2019-09-12 09:03:26,045  INFO org.spark_project.jetty.server.AbstractConnector 310 - Stopped Spark@438d4aa2{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 09:03:26,045  INFO org.apache.spark.ui.SparkUI 54 - Stopped Spark web UI at http://10.6.1.14:4042
2019-09-12 09:03:26,170  INFO org.apache.spark.MapOutputTrackerMasterEndpoint 54 - MapOutputTrackerMasterEndpoint stopped!
2019-09-12 09:03:26,232  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore cleared
2019-09-12 09:03:26,232  INFO org.apache.spark.storage.BlockManager 54 - BlockManager stopped
2019-09-12 09:03:26,232  INFO org.apache.spark.storage.BlockManagerMaster 54 - BlockManagerMaster stopped
2019-09-12 09:03:26,248  INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint 54 - OutputCommitCoordinator stopped!
2019-09-12 09:03:26,248  INFO org.apache.spark.SparkContext 54 - Successfully stopped SparkContext
2019-09-12 09:03:26,248  INFO org.apache.spark.util.ShutdownHookManager 54 - Shutdown hook called
2019-09-12 09:03:26,263  INFO org.apache.spark.util.ShutdownHookManager 54 - Deleting directory C:\Users\OUY\AppData\Local\Temp\spark-ee722a47-aef0-4229-9d69-5a2b81d4a167
2019-09-12 09:06:01,225  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-12 09:06:01,756  WARN org.apache.hadoop.util.NativeCodeLoader 62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-12 09:06:01,850 ERROR org.apache.hadoop.util.Shell 396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2430)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
	at com.datamodel.SparkUtil.CreateSession(SparkUtil.java:22)
	at com.datamodel.SparkUtil.readDataFromCsv(SparkUtil.java:27)
	at com.datamodel.SparkUtil.main(SparkUtil.java:40)
2019-09-12 09:06:02,038  INFO org.apache.spark.SparkContext 54 - Submitted application: splitCHWord
2019-09-12 09:06:02,100  INFO org.apache.spark.SecurityManager 54 - Changing view acls to: OUY
2019-09-12 09:06:02,100  INFO org.apache.spark.SecurityManager 54 - Changing modify acls to: OUY
2019-09-12 09:06:02,100  INFO org.apache.spark.SecurityManager 54 - Changing view acls groups to: 
2019-09-12 09:06:02,100  INFO org.apache.spark.SecurityManager 54 - Changing modify acls groups to: 
2019-09-12 09:06:02,100  INFO org.apache.spark.SecurityManager 54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(OUY); groups with view permissions: Set(); users  with modify permissions: Set(OUY); groups with modify permissions: Set()
2019-09-12 09:06:02,990  INFO org.apache.spark.util.Utils 54 - Successfully started service 'sparkDriver' on port 2652.
2019-09-12 09:06:03,022  INFO org.apache.spark.SparkEnv 54 - Registering MapOutputTracker
2019-09-12 09:06:03,053  INFO org.apache.spark.SparkEnv 54 - Registering BlockManagerMaster
2019-09-12 09:06:03,069  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-09-12 09:06:03,069  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - BlockManagerMasterEndpoint up
2019-09-12 09:06:03,084  INFO org.apache.spark.storage.DiskBlockManager 54 - Created local directory at C:\Users\OUY\AppData\Local\Temp\blockmgr-9d1e8806-3024-4c30-bc9f-72cc80898645
2019-09-12 09:06:03,115  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore started with capacity 899.7 MB
2019-09-12 09:06:03,209  INFO org.apache.spark.SparkEnv 54 - Registering OutputCommitCoordinator
2019-09-12 09:06:03,334  INFO org.spark_project.jetty.util.log 192 - Logging initialized @4847ms
2019-09-12 09:06:03,412  INFO org.spark_project.jetty.server.Server 345 - jetty-9.3.z-SNAPSHOT
2019-09-12 09:06:03,428  INFO org.spark_project.jetty.server.Server 403 - Started @4945ms
2019-09-12 09:06:03,459  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2019-09-12 09:06:03,459  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
2019-09-12 09:06:03,475  INFO org.spark_project.jetty.server.AbstractConnector 270 - Started ServerConnector@63a564f2{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 09:06:03,475  INFO org.apache.spark.util.Utils 54 - Successfully started service 'SparkUI' on port 4042.
2019-09-12 09:06:03,522  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5fe7f967{/jobs,null,AVAILABLE,@Spark}
2019-09-12 09:06:03,522  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5633dafd{/jobs/json,null,AVAILABLE,@Spark}
2019-09-12 09:06:03,522  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2eadc9f6{/jobs/job,null,AVAILABLE,@Spark}
2019-09-12 09:06:03,522  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@778d82e9{/jobs/job/json,null,AVAILABLE,@Spark}
2019-09-12 09:06:03,522  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@59901c4d{/stages,null,AVAILABLE,@Spark}
2019-09-12 09:06:03,522  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@d8d9199{/stages/json,null,AVAILABLE,@Spark}
2019-09-12 09:06:03,522  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@602ae7b6{/stages/stage,null,AVAILABLE,@Spark}
2019-09-12 09:06:03,522  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@13da7ab0{/stages/stage/json,null,AVAILABLE,@Spark}
2019-09-12 09:06:03,537  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@260ff5b7{/stages/pool,null,AVAILABLE,@Spark}
2019-09-12 09:06:03,537  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@77eb5790{/stages/pool/json,null,AVAILABLE,@Spark}
2019-09-12 09:06:03,537  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@319c3a25{/storage,null,AVAILABLE,@Spark}
2019-09-12 09:06:03,537  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@ef1695a{/storage/json,null,AVAILABLE,@Spark}
2019-09-12 09:06:03,537  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@81b5db0{/storage/rdd,null,AVAILABLE,@Spark}
2019-09-12 09:06:03,537  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7139bd31{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-09-12 09:06:03,537  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4b3fe06e{/environment,null,AVAILABLE,@Spark}
2019-09-12 09:06:03,537  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3e17a0a1{/environment/json,null,AVAILABLE,@Spark}
2019-09-12 09:06:03,537  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4d8286c4{/executors,null,AVAILABLE,@Spark}
2019-09-12 09:06:03,553  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@161f6623{/executors/json,null,AVAILABLE,@Spark}
2019-09-12 09:06:03,553  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6778aea6{/executors/threadDump,null,AVAILABLE,@Spark}
2019-09-12 09:06:03,553  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@69228e85{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-09-12 09:06:03,553  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5853495b{/static,null,AVAILABLE,@Spark}
2019-09-12 09:06:03,553  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@34b27915{/,null,AVAILABLE,@Spark}
2019-09-12 09:06:03,568  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1b9776f5{/api,null,AVAILABLE,@Spark}
2019-09-12 09:06:03,568  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@57402ba1{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-09-12 09:06:03,568  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@702b06fb{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-09-12 09:06:03,568  INFO org.apache.spark.ui.SparkUI 54 - Bound SparkUI to 0.0.0.0, and started at http://10.6.1.14:4042
2019-09-12 09:06:03,803  INFO org.apache.spark.executor.Executor 54 - Starting executor ID driver on host localhost
2019-09-12 09:06:03,850  INFO org.apache.spark.util.Utils 54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 2665.
2019-09-12 09:06:03,850  INFO org.apache.spark.network.netty.NettyBlockTransferService 54 - Server created on 10.6.1.14:2665
2019-09-12 09:06:03,850  INFO org.apache.spark.storage.BlockManager 54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-09-12 09:06:03,865  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registering BlockManager BlockManagerId(driver, 10.6.1.14, 2665, None)
2019-09-12 09:06:03,865  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Registering block manager 10.6.1.14:2665 with 899.7 MB RAM, BlockManagerId(driver, 10.6.1.14, 2665, None)
2019-09-12 09:06:03,865  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registered BlockManager BlockManagerId(driver, 10.6.1.14, 2665, None)
2019-09-12 09:06:03,865  INFO org.apache.spark.storage.BlockManager 54 - Initialized BlockManager: BlockManagerId(driver, 10.6.1.14, 2665, None)
2019-09-12 09:06:04,521  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5e922647{/metrics/json,null,AVAILABLE,@Spark}
2019-09-12 09:06:04,818  INFO org.apache.spark.sql.internal.SharedState 54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/MyProject/splitCHword/spark-warehouse/').
2019-09-12 09:06:04,834  INFO org.apache.spark.sql.internal.SharedState 54 - Warehouse path is 'file:/D:/MyProject/splitCHword/spark-warehouse/'.
2019-09-12 09:06:04,849  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6848a051{/SQL,null,AVAILABLE,@Spark}
2019-09-12 09:06:04,849  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5740ff5e{/SQL/json,null,AVAILABLE,@Spark}
2019-09-12 09:06:04,849  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3119cf6f{/SQL/execution,null,AVAILABLE,@Spark}
2019-09-12 09:06:04,849  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1d408060{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-09-12 09:06:04,865  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7d70638{/static/sql,null,AVAILABLE,@Spark}
2019-09-12 09:06:06,302  INFO org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef 54 - Registered StateStoreCoordinator endpoint
2019-09-12 09:06:06,802  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0 stored as values in memory (estimated size 219.8 KB, free 899.5 MB)
2019-09-12 09:06:06,974  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 09:06:06,974  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_0_piece0 in memory on 10.6.1.14:2665 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 09:06:06,989  INFO org.apache.spark.SparkContext 54 - Created broadcast 0 from load at SparkUtil.java:31
2019-09-12 09:06:07,536  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 09:06:07,536  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 09:06:07,567  INFO org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 413 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 326
2019-09-12 09:06:07,646  INFO org.apache.spark.SparkContext 54 - Starting job: load at SparkUtil.java:31
2019-09-12 09:06:07,692  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 0 (load at SparkUtil.java:31) with 1 output partitions
2019-09-12 09:06:07,692  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 0 (load at SparkUtil.java:31)
2019-09-12 09:06:07,692  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 09:06:07,708  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 09:06:07,708  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:31), which has no missing parents
2019-09-12 09:06:07,755  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1 stored as values in memory (estimated size 5.5 KB, free 899.5 MB)
2019-09-12 09:06:07,771  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 899.5 MB)
2019-09-12 09:06:07,771  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_1_piece0 in memory on 10.6.1.14:2665 (size: 3.4 KB, free: 899.7 MB)
2019-09-12 09:06:07,771  INFO org.apache.spark.SparkContext 54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-09-12 09:06:07,802  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:31) (first 15 tasks are for partitions Vector(0))
2019-09-12 09:06:07,817  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 0.0 with 1 tasks
2019-09-12 09:06:07,880  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes)
2019-09-12 09:06:07,896  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 0.0 (TID 0)
2019-09-12 09:06:07,989  INFO org.apache.spark.rdd.BinaryFileRDD 54 - Input split: Paths:/D:/MyProject/splitCHword/target/classes/test.csv:0+326
2019-09-12 09:06:08,239  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 0.0 (TID 0). 853 bytes result sent to driver
2019-09-12 09:06:08,255  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 0.0 (TID 0) in 375 ms on localhost (executor driver) (1/1)
2019-09-12 09:06:08,255  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-09-12 09:06:08,255  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 0 (load at SparkUtil.java:31) finished in 0.422 s
2019-09-12 09:06:08,286  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 0 finished: load at SparkUtil.java:31, took 0.638577 s
2019-09-12 09:06:09,333  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_1_piece0 on 10.6.1.14:2665 in memory (size: 3.4 KB, free: 899.7 MB)
2019-09-12 09:06:11,082  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_0_piece0 on 10.6.1.14:2665 in memory (size: 20.7 KB, free: 899.7 MB)
2019-09-12 09:06:11,270  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Pruning directories with: 
2019-09-12 09:06:11,270  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Post-Scan Filters: 
2019-09-12 09:06:11,270  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Output Data Schema: struct<feature: string>
2019-09-12 09:06:11,285  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Pushed Filters: 
2019-09-12 09:06:11,879  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 324.63887 ms
2019-09-12 09:06:11,879  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2 stored as values in memory (estimated size 220.5 KB, free 899.5 MB)
2019-09-12 09:06:11,910  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.6 KB, free 899.5 MB)
2019-09-12 09:06:11,910  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_2_piece0 in memory on 10.6.1.14:2665 (size: 20.6 KB, free: 899.7 MB)
2019-09-12 09:06:11,910  INFO org.apache.spark.SparkContext 54 - Created broadcast 2 from show at SparkUtil.java:43
2019-09-12 09:06:11,926  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2019-09-12 09:06:11,988  INFO org.apache.spark.SparkContext 54 - Starting job: show at SparkUtil.java:43
2019-09-12 09:06:11,988  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 1 (show at SparkUtil.java:43) with 1 output partitions
2019-09-12 09:06:11,988  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 1 (show at SparkUtil.java:43)
2019-09-12 09:06:11,988  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 09:06:11,988  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 09:06:11,988  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:43), which has no missing parents
2019-09-12 09:06:12,020  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3 stored as values in memory (estimated size 14.2 KB, free 899.5 MB)
2019-09-12 09:06:12,020  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.9 KB, free 899.4 MB)
2019-09-12 09:06:12,020  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_3_piece0 in memory on 10.6.1.14:2665 (size: 6.9 KB, free: 899.7 MB)
2019-09-12 09:06:12,020  INFO org.apache.spark.SparkContext 54 - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2019-09-12 09:06:12,020  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:43) (first 15 tasks are for partitions Vector(0))
2019-09-12 09:06:12,020  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 1.0 with 1 tasks
2019-09-12 09:06:12,035  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5293 bytes)
2019-09-12 09:06:12,035  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 1.0 (TID 1)
2019-09-12 09:06:12,051  INFO org.apache.spark.sql.execution.datasources.FileScanRDD 54 - Reading File path: file:///D:/MyProject/splitCHword/target/classes/test.csv, range: 0-326, partition values: [empty row]
2019-09-12 09:06:12,066  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 12.57471 ms
2019-09-12 09:06:12,316  INFO org.ansj.util.MyStaticValue 106 - init version.number to env value is : 2.11.8
2019-09-12 09:06:12,316  INFO org.ansj.util.MyStaticValue 106 - init copyright.string to env value is : Copyright 2002-2016, LAMP/EPFL
2019-09-12 09:06:12,316  INFO org.ansj.util.MyStaticValue 106 - init osgi.version.number to env value is : 2.11.8.v20160304-115712-1706a37eb8
2019-09-12 09:06:12,316  INFO org.ansj.util.MyStaticValue 106 - init maven.version.number to env value is : 2.11.8
2019-09-12 09:06:12,332  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/ambiguity.dic
2019-09-12 09:06:12,332 ERROR org.ansj.library.AmbiguityLibrary 117 - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:D:\MyProject\splitCHword\library\ambiguity.dic not found or can not to read, path: library/ambiguity.dic
2019-09-12 09:06:12,332  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/default.dic
2019-09-12 09:06:12,332 ERROR org.ansj.library.DicLibrary 221 - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:D:\MyProject\splitCHword\library\default.dic not found or can not to read, path: library/default.dic
2019-09-12 09:06:13,363  INFO org.ansj.library.DATDictionary 90 - init core library ok use time : 906
2019-09-12 09:06:13,785  INFO org.ansj.library.NgramLibrary 17 - init ngram ok use time :406
2019-09-12 09:06:13,816  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 1.0 (TID 1). 1343 bytes result sent to driver
2019-09-12 09:06:13,816  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 1.0 (TID 1) in 1796 ms on localhost (executor driver) (1/1)
2019-09-12 09:06:13,816  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-09-12 09:06:13,816  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 1 (show at SparkUtil.java:43) finished in 1.796 s
2019-09-12 09:06:13,816  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 1 finished: show at SparkUtil.java:43, took 1.831643 s
2019-09-12 09:06:13,847  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 8.347299 ms
2019-09-12 09:06:13,878  INFO org.apache.spark.SparkContext 54 - Invoking stop() from shutdown hook
2019-09-12 09:06:13,878  INFO org.spark_project.jetty.server.AbstractConnector 310 - Stopped Spark@63a564f2{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 09:06:13,878  INFO org.apache.spark.ui.SparkUI 54 - Stopped Spark web UI at http://10.6.1.14:4042
2019-09-12 09:06:13,972  INFO org.apache.spark.MapOutputTrackerMasterEndpoint 54 - MapOutputTrackerMasterEndpoint stopped!
2019-09-12 09:06:14,003  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore cleared
2019-09-12 09:06:14,003  INFO org.apache.spark.storage.BlockManager 54 - BlockManager stopped
2019-09-12 09:06:14,003  INFO org.apache.spark.storage.BlockManagerMaster 54 - BlockManagerMaster stopped
2019-09-12 09:06:14,019  INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint 54 - OutputCommitCoordinator stopped!
2019-09-12 09:06:14,019  INFO org.apache.spark.SparkContext 54 - Successfully stopped SparkContext
2019-09-12 09:06:14,019  INFO org.apache.spark.util.ShutdownHookManager 54 - Shutdown hook called
2019-09-12 09:06:14,019  INFO org.apache.spark.util.ShutdownHookManager 54 - Deleting directory C:\Users\OUY\AppData\Local\Temp\spark-c6129e9c-f0e6-4219-864b-a311f3ea1bd3
2019-09-12 09:08:32,147  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-12 09:08:32,741  WARN org.apache.hadoop.util.NativeCodeLoader 62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-12 09:08:32,882 ERROR org.apache.hadoop.util.Shell 396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2430)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
	at com.datamodel.SparkUtil.CreateSession(SparkUtil.java:22)
	at com.datamodel.SparkUtil.readDataFromCsv(SparkUtil.java:27)
	at com.datamodel.SparkUtil.main(SparkUtil.java:40)
2019-09-12 09:08:33,007  INFO org.apache.spark.SparkContext 54 - Submitted application: splitCHWord
2019-09-12 09:08:33,038  INFO org.apache.spark.SecurityManager 54 - Changing view acls to: OUY
2019-09-12 09:08:33,038  INFO org.apache.spark.SecurityManager 54 - Changing modify acls to: OUY
2019-09-12 09:08:33,038  INFO org.apache.spark.SecurityManager 54 - Changing view acls groups to: 
2019-09-12 09:08:33,038  INFO org.apache.spark.SecurityManager 54 - Changing modify acls groups to: 
2019-09-12 09:08:33,038  INFO org.apache.spark.SecurityManager 54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(OUY); groups with view permissions: Set(); users  with modify permissions: Set(OUY); groups with modify permissions: Set()
2019-09-12 09:08:33,725  INFO org.apache.spark.util.Utils 54 - Successfully started service 'sparkDriver' on port 2701.
2019-09-12 09:08:33,772  INFO org.apache.spark.SparkEnv 54 - Registering MapOutputTracker
2019-09-12 09:08:33,803  INFO org.apache.spark.SparkEnv 54 - Registering BlockManagerMaster
2019-09-12 09:08:33,803  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-09-12 09:08:33,803  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - BlockManagerMasterEndpoint up
2019-09-12 09:08:33,803  INFO org.apache.spark.storage.DiskBlockManager 54 - Created local directory at C:\Users\OUY\AppData\Local\Temp\blockmgr-c0a71b88-26c6-4e41-b7fc-b39ef453ed20
2019-09-12 09:08:33,835  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore started with capacity 899.7 MB
2019-09-12 09:08:33,897  INFO org.apache.spark.SparkEnv 54 - Registering OutputCommitCoordinator
2019-09-12 09:08:33,975  INFO org.spark_project.jetty.util.log 192 - Logging initialized @3909ms
2019-09-12 09:08:34,038  INFO org.spark_project.jetty.server.Server 345 - jetty-9.3.z-SNAPSHOT
2019-09-12 09:08:34,053  INFO org.spark_project.jetty.server.Server 403 - Started @3993ms
2019-09-12 09:08:34,069  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2019-09-12 09:08:34,069  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
2019-09-12 09:08:34,084  INFO org.spark_project.jetty.server.AbstractConnector 270 - Started ServerConnector@1643d68f{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 09:08:34,084  INFO org.apache.spark.util.Utils 54 - Successfully started service 'SparkUI' on port 4042.
2019-09-12 09:08:34,116  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@410ae9a3{/jobs,null,AVAILABLE,@Spark}
2019-09-12 09:08:34,116  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3c0fae6c{/jobs/json,null,AVAILABLE,@Spark}
2019-09-12 09:08:34,116  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@52b56a3e{/jobs/job,null,AVAILABLE,@Spark}
2019-09-12 09:08:34,131  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@36b0fcd5{/jobs/job/json,null,AVAILABLE,@Spark}
2019-09-12 09:08:34,131  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@475835b1{/stages,null,AVAILABLE,@Spark}
2019-09-12 09:08:34,131  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5241cf67{/stages/json,null,AVAILABLE,@Spark}
2019-09-12 09:08:34,131  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@77192705{/stages/stage,null,AVAILABLE,@Spark}
2019-09-12 09:08:34,131  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@625e134e{/stages/stage/json,null,AVAILABLE,@Spark}
2019-09-12 09:08:34,131  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@89c10b7{/stages/pool,null,AVAILABLE,@Spark}
2019-09-12 09:08:34,131  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4fe89c24{/stages/pool/json,null,AVAILABLE,@Spark}
2019-09-12 09:08:34,131  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3d08f3f5{/storage,null,AVAILABLE,@Spark}
2019-09-12 09:08:34,131  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1a1da881{/storage/json,null,AVAILABLE,@Spark}
2019-09-12 09:08:34,131  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7fd4acee{/storage/rdd,null,AVAILABLE,@Spark}
2019-09-12 09:08:34,131  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6175619b{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-09-12 09:08:34,131  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@756cf158{/environment,null,AVAILABLE,@Spark}
2019-09-12 09:08:34,131  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@751d3241{/environment/json,null,AVAILABLE,@Spark}
2019-09-12 09:08:34,131  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@64337702{/executors,null,AVAILABLE,@Spark}
2019-09-12 09:08:34,147  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@30ea8c23{/executors/json,null,AVAILABLE,@Spark}
2019-09-12 09:08:34,147  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4e76dac{/executors/threadDump,null,AVAILABLE,@Spark}
2019-09-12 09:08:34,147  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5f2f577{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-09-12 09:08:34,147  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5d465e4b{/static,null,AVAILABLE,@Spark}
2019-09-12 09:08:34,147  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@70dd7e15{/,null,AVAILABLE,@Spark}
2019-09-12 09:08:34,147  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@35beb15e{/api,null,AVAILABLE,@Spark}
2019-09-12 09:08:34,147  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@9635fa{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-09-12 09:08:34,163  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@63c5efee{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-09-12 09:08:34,163  INFO org.apache.spark.ui.SparkUI 54 - Bound SparkUI to 0.0.0.0, and started at http://10.6.1.14:4042
2019-09-12 09:08:34,334  INFO org.apache.spark.executor.Executor 54 - Starting executor ID driver on host localhost
2019-09-12 09:08:34,381  INFO org.apache.spark.util.Utils 54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 2714.
2019-09-12 09:08:34,381  INFO org.apache.spark.network.netty.NettyBlockTransferService 54 - Server created on 10.6.1.14:2714
2019-09-12 09:08:34,381  INFO org.apache.spark.storage.BlockManager 54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-09-12 09:08:34,381  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registering BlockManager BlockManagerId(driver, 10.6.1.14, 2714, None)
2019-09-12 09:08:34,397  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Registering block manager 10.6.1.14:2714 with 899.7 MB RAM, BlockManagerId(driver, 10.6.1.14, 2714, None)
2019-09-12 09:08:34,397  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registered BlockManager BlockManagerId(driver, 10.6.1.14, 2714, None)
2019-09-12 09:08:34,397  INFO org.apache.spark.storage.BlockManager 54 - Initialized BlockManager: BlockManagerId(driver, 10.6.1.14, 2714, None)
2019-09-12 09:08:34,697  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@79c3f01f{/metrics/json,null,AVAILABLE,@Spark}
2019-09-12 09:08:34,791  INFO org.apache.spark.sql.internal.SharedState 54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/MyProject/splitCHword/spark-warehouse/').
2019-09-12 09:08:34,792  INFO org.apache.spark.sql.internal.SharedState 54 - Warehouse path is 'file:/D:/MyProject/splitCHword/spark-warehouse/'.
2019-09-12 09:08:34,805  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@15515c51{/SQL,null,AVAILABLE,@Spark}
2019-09-12 09:08:34,807  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@64a896b0{/SQL/json,null,AVAILABLE,@Spark}
2019-09-12 09:08:34,809  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@16943e88{/SQL/execution,null,AVAILABLE,@Spark}
2019-09-12 09:08:34,811  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@73d6d0c{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-09-12 09:08:34,818  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3956b302{/static/sql,null,AVAILABLE,@Spark}
2019-09-12 09:08:36,359  INFO org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef 54 - Registered StateStoreCoordinator endpoint
2019-09-12 09:08:36,687  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0 stored as values in memory (estimated size 219.8 KB, free 899.5 MB)
2019-09-12 09:08:36,812  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 09:08:36,812  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_0_piece0 in memory on 10.6.1.14:2714 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 09:08:36,812  INFO org.apache.spark.SparkContext 54 - Created broadcast 0 from load at SparkUtil.java:31
2019-09-12 09:08:37,172  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 09:08:37,172  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 09:08:37,187  INFO org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 413 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 326
2019-09-12 09:08:37,297  INFO org.apache.spark.SparkContext 54 - Starting job: load at SparkUtil.java:31
2019-09-12 09:08:37,328  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 0 (load at SparkUtil.java:31) with 1 output partitions
2019-09-12 09:08:37,328  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 0 (load at SparkUtil.java:31)
2019-09-12 09:08:37,328  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 09:08:37,328  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 09:08:37,344  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:31), which has no missing parents
2019-09-12 09:08:37,375  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1 stored as values in memory (estimated size 5.5 KB, free 899.5 MB)
2019-09-12 09:08:37,390  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 899.5 MB)
2019-09-12 09:08:37,390  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_1_piece0 in memory on 10.6.1.14:2714 (size: 3.4 KB, free: 899.7 MB)
2019-09-12 09:08:37,390  INFO org.apache.spark.SparkContext 54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-09-12 09:08:37,453  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:31) (first 15 tasks are for partitions Vector(0))
2019-09-12 09:08:37,453  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 0.0 with 1 tasks
2019-09-12 09:08:37,531  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes)
2019-09-12 09:08:37,562  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 0.0 (TID 0)
2019-09-12 09:08:37,640  INFO org.apache.spark.rdd.BinaryFileRDD 54 - Input split: Paths:/D:/MyProject/splitCHword/target/classes/test.csv:0+326
2019-09-12 09:08:37,843  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 0.0 (TID 0). 853 bytes result sent to driver
2019-09-12 09:08:37,859  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 0.0 (TID 0) in 343 ms on localhost (executor driver) (1/1)
2019-09-12 09:08:37,859  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-09-12 09:08:37,859  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 0 (load at SparkUtil.java:31) finished in 0.375 s
2019-09-12 09:08:37,875  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 0 finished: load at SparkUtil.java:31, took 0.569095 s
2019-09-12 09:08:38,359  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_1_piece0 on 10.6.1.14:2714 in memory (size: 3.4 KB, free: 899.7 MB)
2019-09-12 09:08:39,963  INFO org.apache.spark.SparkContext 54 - Invoking stop() from shutdown hook
2019-09-12 09:08:39,963  INFO org.spark_project.jetty.server.AbstractConnector 310 - Stopped Spark@1643d68f{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 09:08:39,963  INFO org.apache.spark.ui.SparkUI 54 - Stopped Spark web UI at http://10.6.1.14:4042
2019-09-12 09:08:39,979  INFO org.apache.spark.MapOutputTrackerMasterEndpoint 54 - MapOutputTrackerMasterEndpoint stopped!
2019-09-12 09:08:39,994  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore cleared
2019-09-12 09:08:39,994  INFO org.apache.spark.storage.BlockManager 54 - BlockManager stopped
2019-09-12 09:08:39,994  INFO org.apache.spark.storage.BlockManagerMaster 54 - BlockManagerMaster stopped
2019-09-12 09:08:40,010  INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint 54 - OutputCommitCoordinator stopped!
2019-09-12 09:08:40,010  INFO org.apache.spark.SparkContext 54 - Successfully stopped SparkContext
2019-09-12 09:08:40,010  INFO org.apache.spark.util.ShutdownHookManager 54 - Shutdown hook called
2019-09-12 09:08:40,010  INFO org.apache.spark.util.ShutdownHookManager 54 - Deleting directory C:\Users\OUY\AppData\Local\Temp\spark-13559ca5-a835-4c55-9405-8cee2ced37d5
2019-09-12 09:09:38,937  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-12 09:09:39,499  WARN org.apache.hadoop.util.NativeCodeLoader 62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-12 09:09:39,593 ERROR org.apache.hadoop.util.Shell 396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2430)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
	at com.datamodel.SparkUtil.CreateSession(SparkUtil.java:22)
	at com.datamodel.SparkUtil.readDataFromCsv(SparkUtil.java:27)
	at com.datamodel.SparkUtil.main(SparkUtil.java:40)
2019-09-12 09:09:39,811  INFO org.apache.spark.SparkContext 54 - Submitted application: splitCHWord
2019-09-12 09:09:39,908  INFO org.apache.spark.SecurityManager 54 - Changing view acls to: OUY
2019-09-12 09:09:39,909  INFO org.apache.spark.SecurityManager 54 - Changing modify acls to: OUY
2019-09-12 09:09:39,910  INFO org.apache.spark.SecurityManager 54 - Changing view acls groups to: 
2019-09-12 09:09:39,912  INFO org.apache.spark.SecurityManager 54 - Changing modify acls groups to: 
2019-09-12 09:09:39,913  INFO org.apache.spark.SecurityManager 54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(OUY); groups with view permissions: Set(); users  with modify permissions: Set(OUY); groups with modify permissions: Set()
2019-09-12 09:09:40,809  INFO org.apache.spark.util.Utils 54 - Successfully started service 'sparkDriver' on port 2749.
2019-09-12 09:09:40,840  INFO org.apache.spark.SparkEnv 54 - Registering MapOutputTracker
2019-09-12 09:09:40,871  INFO org.apache.spark.SparkEnv 54 - Registering BlockManagerMaster
2019-09-12 09:09:40,871  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-09-12 09:09:40,871  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - BlockManagerMasterEndpoint up
2019-09-12 09:09:40,887  INFO org.apache.spark.storage.DiskBlockManager 54 - Created local directory at C:\Users\OUY\AppData\Local\Temp\blockmgr-4cd092e9-110b-4a1b-9fad-c7f945b6c3e0
2019-09-12 09:09:40,918  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore started with capacity 899.7 MB
2019-09-12 09:09:41,012  INFO org.apache.spark.SparkEnv 54 - Registering OutputCommitCoordinator
2019-09-12 09:09:41,137  INFO org.spark_project.jetty.util.log 192 - Logging initialized @4701ms
2019-09-12 09:09:41,222  INFO org.spark_project.jetty.server.Server 345 - jetty-9.3.z-SNAPSHOT
2019-09-12 09:09:41,244  INFO org.spark_project.jetty.server.Server 403 - Started @4804ms
2019-09-12 09:09:41,269  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2019-09-12 09:09:41,270  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
2019-09-12 09:09:41,282  INFO org.spark_project.jetty.server.AbstractConnector 270 - Started ServerConnector@24039acd{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 09:09:41,283  INFO org.apache.spark.util.Utils 54 - Successfully started service 'SparkUI' on port 4042.
2019-09-12 09:09:41,331  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3531f3ca{/jobs,null,AVAILABLE,@Spark}
2019-09-12 09:09:41,331  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@316a598d{/jobs/json,null,AVAILABLE,@Spark}
2019-09-12 09:09:41,333  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6ba30587{/jobs/job,null,AVAILABLE,@Spark}
2019-09-12 09:09:41,335  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2eadc9f6{/jobs/job/json,null,AVAILABLE,@Spark}
2019-09-12 09:09:41,336  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@61af1510{/stages,null,AVAILABLE,@Spark}
2019-09-12 09:09:41,337  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@778d82e9{/stages/json,null,AVAILABLE,@Spark}
2019-09-12 09:09:41,338  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@59901c4d{/stages/stage,null,AVAILABLE,@Spark}
2019-09-12 09:09:41,341  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@71ad3d8a{/stages/stage/json,null,AVAILABLE,@Spark}
2019-09-12 09:09:41,342  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@700f518a{/stages/pool,null,AVAILABLE,@Spark}
2019-09-12 09:09:41,343  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@13da7ab0{/stages/pool/json,null,AVAILABLE,@Spark}
2019-09-12 09:09:41,344  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@260ff5b7{/storage,null,AVAILABLE,@Spark}
2019-09-12 09:09:41,344  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@77eb5790{/storage/json,null,AVAILABLE,@Spark}
2019-09-12 09:09:41,344  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@319c3a25{/storage/rdd,null,AVAILABLE,@Spark}
2019-09-12 09:09:41,344  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@ef1695a{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-09-12 09:09:41,344  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@81b5db0{/environment,null,AVAILABLE,@Spark}
2019-09-12 09:09:41,344  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7139bd31{/environment/json,null,AVAILABLE,@Spark}
2019-09-12 09:09:41,344  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4b3fe06e{/executors,null,AVAILABLE,@Spark}
2019-09-12 09:09:41,344  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3e17a0a1{/executors/json,null,AVAILABLE,@Spark}
2019-09-12 09:09:41,344  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4d8286c4{/executors/threadDump,null,AVAILABLE,@Spark}
2019-09-12 09:09:41,360  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@161f6623{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-09-12 09:09:41,360  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6778aea6{/static,null,AVAILABLE,@Spark}
2019-09-12 09:09:41,360  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3419e23b{/,null,AVAILABLE,@Spark}
2019-09-12 09:09:41,360  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1d75e7af{/api,null,AVAILABLE,@Spark}
2019-09-12 09:09:41,360  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@79d9214d{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-09-12 09:09:41,375  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1dd7796b{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-09-12 09:09:41,375  INFO org.apache.spark.ui.SparkUI 54 - Bound SparkUI to 0.0.0.0, and started at http://10.6.1.14:4042
2019-09-12 09:09:41,610  INFO org.apache.spark.executor.Executor 54 - Starting executor ID driver on host localhost
2019-09-12 09:09:41,672  INFO org.apache.spark.util.Utils 54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 2762.
2019-09-12 09:09:41,672  INFO org.apache.spark.network.netty.NettyBlockTransferService 54 - Server created on 10.6.1.14:2762
2019-09-12 09:09:41,672  INFO org.apache.spark.storage.BlockManager 54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-09-12 09:09:41,672  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registering BlockManager BlockManagerId(driver, 10.6.1.14, 2762, None)
2019-09-12 09:09:41,688  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Registering block manager 10.6.1.14:2762 with 899.7 MB RAM, BlockManagerId(driver, 10.6.1.14, 2762, None)
2019-09-12 09:09:41,688  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registered BlockManager BlockManagerId(driver, 10.6.1.14, 2762, None)
2019-09-12 09:09:41,703  INFO org.apache.spark.storage.BlockManager 54 - Initialized BlockManager: BlockManagerId(driver, 10.6.1.14, 2762, None)
2019-09-12 09:09:41,924  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@12bbfc54{/metrics/json,null,AVAILABLE,@Spark}
2019-09-12 09:09:42,049  INFO org.apache.spark.sql.internal.SharedState 54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/MyProject/splitCHword/spark-warehouse/').
2019-09-12 09:09:42,049  INFO org.apache.spark.sql.internal.SharedState 54 - Warehouse path is 'file:/D:/MyProject/splitCHword/spark-warehouse/'.
2019-09-12 09:09:42,065  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7301eebe{/SQL,null,AVAILABLE,@Spark}
2019-09-12 09:09:42,065  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4d6ccc97{/SQL/json,null,AVAILABLE,@Spark}
2019-09-12 09:09:42,065  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@149b0577{/SQL/execution,null,AVAILABLE,@Spark}
2019-09-12 09:09:42,065  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4a901445{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-09-12 09:09:42,081  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@68a78f3c{/static/sql,null,AVAILABLE,@Spark}
2019-09-12 09:09:43,377  INFO org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef 54 - Registered StateStoreCoordinator endpoint
2019-09-12 09:09:43,768  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0 stored as values in memory (estimated size 219.8 KB, free 899.5 MB)
2019-09-12 09:09:43,877  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 09:09:43,877  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_0_piece0 in memory on 10.6.1.14:2762 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 09:09:43,891  INFO org.apache.spark.SparkContext 54 - Created broadcast 0 from load at SparkUtil.java:31
2019-09-12 09:09:44,319  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 09:09:44,319  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 09:09:44,334  INFO org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 413 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 326
2019-09-12 09:09:44,412  INFO org.apache.spark.SparkContext 54 - Starting job: load at SparkUtil.java:31
2019-09-12 09:09:44,444  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 0 (load at SparkUtil.java:31) with 1 output partitions
2019-09-12 09:09:44,444  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 0 (load at SparkUtil.java:31)
2019-09-12 09:09:44,444  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 09:09:44,444  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 09:09:44,459  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:31), which has no missing parents
2019-09-12 09:09:44,490  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1 stored as values in memory (estimated size 5.5 KB, free 899.5 MB)
2019-09-12 09:09:44,506  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 899.5 MB)
2019-09-12 09:09:44,506  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_1_piece0 in memory on 10.6.1.14:2762 (size: 3.4 KB, free: 899.7 MB)
2019-09-12 09:09:44,506  INFO org.apache.spark.SparkContext 54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-09-12 09:09:44,553  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:31) (first 15 tasks are for partitions Vector(0))
2019-09-12 09:09:44,553  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 0.0 with 1 tasks
2019-09-12 09:09:44,627  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes)
2019-09-12 09:09:44,719  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 0.0 (TID 0)
2019-09-12 09:09:44,905  INFO org.apache.spark.rdd.BinaryFileRDD 54 - Input split: Paths:/D:/MyProject/splitCHword/target/classes/test.csv:0+326
2019-09-12 09:09:45,252  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 0.0 (TID 0). 853 bytes result sent to driver
2019-09-12 09:09:45,268  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 0.0 (TID 0) in 652 ms on localhost (executor driver) (1/1)
2019-09-12 09:09:45,268  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-09-12 09:09:45,268  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 0 (load at SparkUtil.java:31) finished in 0.699 s
2019-09-12 09:09:45,283  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 0 finished: load at SparkUtil.java:31, took 0.862820 s
2019-09-12 09:09:46,375  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_1_piece0 on 10.6.1.14:2762 in memory (size: 3.4 KB, free: 899.7 MB)
2019-09-12 09:09:48,078  INFO org.apache.spark.SparkContext 54 - Invoking stop() from shutdown hook
2019-09-12 09:09:48,093  INFO org.spark_project.jetty.server.AbstractConnector 310 - Stopped Spark@24039acd{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 09:09:48,109  INFO org.apache.spark.ui.SparkUI 54 - Stopped Spark web UI at http://10.6.1.14:4042
2019-09-12 09:09:48,125  INFO org.apache.spark.MapOutputTrackerMasterEndpoint 54 - MapOutputTrackerMasterEndpoint stopped!
2019-09-12 09:09:48,343  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore cleared
2019-09-12 09:09:48,343  INFO org.apache.spark.storage.BlockManager 54 - BlockManager stopped
2019-09-12 09:09:48,343  INFO org.apache.spark.storage.BlockManagerMaster 54 - BlockManagerMaster stopped
2019-09-12 09:09:48,343  INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint 54 - OutputCommitCoordinator stopped!
2019-09-12 09:09:48,359  INFO org.apache.spark.SparkContext 54 - Successfully stopped SparkContext
2019-09-12 09:09:48,359  INFO org.apache.spark.util.ShutdownHookManager 54 - Shutdown hook called
2019-09-12 09:09:48,359  INFO org.apache.spark.util.ShutdownHookManager 54 - Deleting directory C:\Users\OUY\AppData\Local\Temp\spark-826418d1-a539-404d-831b-de062a2d96a5
2019-09-12 09:11:34,292  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-12 09:11:34,995  WARN org.apache.hadoop.util.NativeCodeLoader 62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-12 09:11:35,120 ERROR org.apache.hadoop.util.Shell 396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2430)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
	at com.datamodel.SparkUtil.CreateSession(SparkUtil.java:19)
	at com.datamodel.SparkUtil.readDataFromCsv(SparkUtil.java:24)
	at com.datamodel.SparkUtil.main(SparkUtil.java:37)
2019-09-12 09:11:35,323  INFO org.apache.spark.SparkContext 54 - Submitted application: splitCHWord
2019-09-12 09:11:35,370  INFO org.apache.spark.SecurityManager 54 - Changing view acls to: OUY
2019-09-12 09:11:35,370  INFO org.apache.spark.SecurityManager 54 - Changing modify acls to: OUY
2019-09-12 09:11:35,370  INFO org.apache.spark.SecurityManager 54 - Changing view acls groups to: 
2019-09-12 09:11:35,370  INFO org.apache.spark.SecurityManager 54 - Changing modify acls groups to: 
2019-09-12 09:11:35,370  INFO org.apache.spark.SecurityManager 54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(OUY); groups with view permissions: Set(); users  with modify permissions: Set(OUY); groups with modify permissions: Set()
2019-09-12 09:11:36,214  INFO org.apache.spark.util.Utils 54 - Successfully started service 'sparkDriver' on port 2805.
2019-09-12 09:11:36,245  INFO org.apache.spark.SparkEnv 54 - Registering MapOutputTracker
2019-09-12 09:11:36,276  INFO org.apache.spark.SparkEnv 54 - Registering BlockManagerMaster
2019-09-12 09:11:36,276  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-09-12 09:11:36,276  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - BlockManagerMasterEndpoint up
2019-09-12 09:11:36,292  INFO org.apache.spark.storage.DiskBlockManager 54 - Created local directory at C:\Users\OUY\AppData\Local\Temp\blockmgr-113ed1a2-8dc3-483c-ab7d-58810171b896
2019-09-12 09:11:36,323  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore started with capacity 899.7 MB
2019-09-12 09:11:36,433  INFO org.apache.spark.SparkEnv 54 - Registering OutputCommitCoordinator
2019-09-12 09:11:36,589  INFO org.spark_project.jetty.util.log 192 - Logging initialized @4792ms
2019-09-12 09:11:36,761  INFO org.spark_project.jetty.server.Server 345 - jetty-9.3.z-SNAPSHOT
2019-09-12 09:11:36,792  INFO org.spark_project.jetty.server.Server 403 - Started @5007ms
2019-09-12 09:11:36,839  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2019-09-12 09:11:36,839  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
2019-09-12 09:11:36,870  INFO org.spark_project.jetty.server.AbstractConnector 270 - Started ServerConnector@7adea437{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 09:11:36,870  INFO org.apache.spark.util.Utils 54 - Successfully started service 'SparkUI' on port 4042.
2019-09-12 09:11:36,917  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1aa6e3c0{/jobs,null,AVAILABLE,@Spark}
2019-09-12 09:11:36,917  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@35cd68d4{/jobs/json,null,AVAILABLE,@Spark}
2019-09-12 09:11:36,917  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@216914{/jobs/job,null,AVAILABLE,@Spark}
2019-09-12 09:11:36,917  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5d5160e6{/jobs/job/json,null,AVAILABLE,@Spark}
2019-09-12 09:11:36,917  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2903c6ff{/stages,null,AVAILABLE,@Spark}
2019-09-12 09:11:36,917  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@37af1f93{/stages/json,null,AVAILABLE,@Spark}
2019-09-12 09:11:36,917  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@408e96d9{/stages/stage,null,AVAILABLE,@Spark}
2019-09-12 09:11:36,932  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@10cd6753{/stages/stage/json,null,AVAILABLE,@Spark}
2019-09-12 09:11:36,932  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@47af099e{/stages/pool,null,AVAILABLE,@Spark}
2019-09-12 09:11:36,932  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@b835727{/stages/pool/json,null,AVAILABLE,@Spark}
2019-09-12 09:11:36,932  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2c8662ac{/storage,null,AVAILABLE,@Spark}
2019-09-12 09:11:36,964  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3724b43e{/storage/json,null,AVAILABLE,@Spark}
2019-09-12 09:11:36,964  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@68e7c8c3{/storage/rdd,null,AVAILABLE,@Spark}
2019-09-12 09:11:36,964  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@238bfd6c{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-09-12 09:11:36,964  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@58860997{/environment,null,AVAILABLE,@Spark}
2019-09-12 09:11:36,964  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7487b142{/environment/json,null,AVAILABLE,@Spark}
2019-09-12 09:11:36,964  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@199bc830{/executors,null,AVAILABLE,@Spark}
2019-09-12 09:11:36,964  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@27b45ea{/executors/json,null,AVAILABLE,@Spark}
2019-09-12 09:11:36,964  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@790a251b{/executors/threadDump,null,AVAILABLE,@Spark}
2019-09-12 09:11:36,964  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@150ede8b{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-09-12 09:11:36,995  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3e15bb06{/static,null,AVAILABLE,@Spark}
2019-09-12 09:11:36,995  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@119c745c{/,null,AVAILABLE,@Spark}
2019-09-12 09:11:36,995  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3b1ed14b{/api,null,AVAILABLE,@Spark}
2019-09-12 09:11:37,011  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1debc91c{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-09-12 09:11:37,011  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@181e72d3{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-09-12 09:11:37,011  INFO org.apache.spark.ui.SparkUI 54 - Bound SparkUI to 0.0.0.0, and started at http://10.6.1.14:4042
2019-09-12 09:11:37,339  INFO org.apache.spark.executor.Executor 54 - Starting executor ID driver on host localhost
2019-09-12 09:11:37,448  INFO org.apache.spark.util.Utils 54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 2818.
2019-09-12 09:11:37,448  INFO org.apache.spark.network.netty.NettyBlockTransferService 54 - Server created on 10.6.1.14:2818
2019-09-12 09:11:37,448  INFO org.apache.spark.storage.BlockManager 54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-09-12 09:11:37,448  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registering BlockManager BlockManagerId(driver, 10.6.1.14, 2818, None)
2019-09-12 09:11:37,448  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Registering block manager 10.6.1.14:2818 with 899.7 MB RAM, BlockManagerId(driver, 10.6.1.14, 2818, None)
2019-09-12 09:11:37,464  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registered BlockManager BlockManagerId(driver, 10.6.1.14, 2818, None)
2019-09-12 09:11:37,464  INFO org.apache.spark.storage.BlockManager 54 - Initialized BlockManager: BlockManagerId(driver, 10.6.1.14, 2818, None)
2019-09-12 09:11:37,745  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3ac3f6f{/metrics/json,null,AVAILABLE,@Spark}
2019-09-12 09:11:37,870  INFO org.apache.spark.sql.internal.SharedState 54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/MyProject/splitCHword/spark-warehouse/').
2019-09-12 09:11:37,870  INFO org.apache.spark.sql.internal.SharedState 54 - Warehouse path is 'file:/D:/MyProject/splitCHword/spark-warehouse/'.
2019-09-12 09:11:37,870  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@41a374be{/SQL,null,AVAILABLE,@Spark}
2019-09-12 09:11:37,870  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@8cc8cdb{/SQL/json,null,AVAILABLE,@Spark}
2019-09-12 09:11:37,870  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@76b47204{/SQL/execution,null,AVAILABLE,@Spark}
2019-09-12 09:11:37,885  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6a12c7a8{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-09-12 09:11:37,885  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@67f77f6e{/static/sql,null,AVAILABLE,@Spark}
2019-09-12 09:11:39,041  INFO org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef 54 - Registered StateStoreCoordinator endpoint
2019-09-12 09:11:39,463  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0 stored as values in memory (estimated size 219.8 KB, free 899.5 MB)
2019-09-12 09:11:39,651  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 09:11:39,651  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_0_piece0 in memory on 10.6.1.14:2818 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 09:11:39,666  INFO org.apache.spark.SparkContext 54 - Created broadcast 0 from load at SparkUtil.java:28
2019-09-12 09:11:40,182  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 09:11:40,182  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 09:11:40,197  INFO org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 413 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 326
2019-09-12 09:11:40,307  INFO org.apache.spark.SparkContext 54 - Starting job: load at SparkUtil.java:28
2019-09-12 09:11:40,338  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 0 (load at SparkUtil.java:28) with 1 output partitions
2019-09-12 09:11:40,338  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 0 (load at SparkUtil.java:28)
2019-09-12 09:11:40,338  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 09:11:40,338  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 09:11:40,338  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:28), which has no missing parents
2019-09-12 09:11:40,385  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1 stored as values in memory (estimated size 5.5 KB, free 899.5 MB)
2019-09-12 09:11:40,385  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 899.5 MB)
2019-09-12 09:11:40,385  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_1_piece0 in memory on 10.6.1.14:2818 (size: 3.4 KB, free: 899.7 MB)
2019-09-12 09:11:40,400  INFO org.apache.spark.SparkContext 54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-09-12 09:11:40,447  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:28) (first 15 tasks are for partitions Vector(0))
2019-09-12 09:11:40,447  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 0.0 with 1 tasks
2019-09-12 09:11:40,525  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes)
2019-09-12 09:11:40,541  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 0.0 (TID 0)
2019-09-12 09:11:40,650  INFO org.apache.spark.rdd.BinaryFileRDD 54 - Input split: Paths:/D:/MyProject/splitCHword/target/classes/test.csv:0+326
2019-09-12 09:11:40,885  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 0.0 (TID 0). 853 bytes result sent to driver
2019-09-12 09:11:40,885  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 0.0 (TID 0) in 375 ms on localhost (executor driver) (1/1)
2019-09-12 09:11:40,900  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-09-12 09:11:40,900  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 0 (load at SparkUtil.java:28) finished in 0.421 s
2019-09-12 09:11:40,916  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 0 finished: load at SparkUtil.java:28, took 0.607947 s
2019-09-12 09:11:41,697  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_1_piece0 on 10.6.1.14:2818 in memory (size: 3.4 KB, free: 899.7 MB)
2019-09-12 09:11:43,400  INFO org.apache.spark.SparkContext 54 - Invoking stop() from shutdown hook
2019-09-12 09:11:43,400  INFO org.spark_project.jetty.server.AbstractConnector 310 - Stopped Spark@7adea437{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 09:11:43,415  INFO org.apache.spark.ui.SparkUI 54 - Stopped Spark web UI at http://10.6.1.14:4042
2019-09-12 09:11:43,431  INFO org.apache.spark.MapOutputTrackerMasterEndpoint 54 - MapOutputTrackerMasterEndpoint stopped!
2019-09-12 09:11:43,650  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore cleared
2019-09-12 09:11:43,650  INFO org.apache.spark.storage.BlockManager 54 - BlockManager stopped
2019-09-12 09:11:43,665  INFO org.apache.spark.storage.BlockManagerMaster 54 - BlockManagerMaster stopped
2019-09-12 09:11:43,681  INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint 54 - OutputCommitCoordinator stopped!
2019-09-12 09:11:43,696  INFO org.apache.spark.SparkContext 54 - Successfully stopped SparkContext
2019-09-12 09:11:43,696  INFO org.apache.spark.util.ShutdownHookManager 54 - Shutdown hook called
2019-09-12 09:11:43,712  INFO org.apache.spark.util.ShutdownHookManager 54 - Deleting directory C:\Users\OUY\AppData\Local\Temp\spark-1b1ef28e-4dc3-4868-bc8b-3b89f6a68269
2019-09-12 09:14:09,693  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-12 09:14:10,271  WARN org.apache.hadoop.util.NativeCodeLoader 62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-12 09:14:10,365 ERROR org.apache.hadoop.util.Shell 396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2430)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
	at com.datamodel.SparkUtil.CreateSession(SparkUtil.java:19)
	at com.datamodel.SparkUtil.readDataFromCsv(SparkUtil.java:24)
	at com.datamodel.SparkUtil.main(SparkUtil.java:37)
2019-09-12 09:14:10,553  INFO org.apache.spark.SparkContext 54 - Submitted application: splitCHWord
2019-09-12 09:14:10,599  INFO org.apache.spark.SecurityManager 54 - Changing view acls to: OUY
2019-09-12 09:14:10,599  INFO org.apache.spark.SecurityManager 54 - Changing modify acls to: OUY
2019-09-12 09:14:10,615  INFO org.apache.spark.SecurityManager 54 - Changing view acls groups to: 
2019-09-12 09:14:10,615  INFO org.apache.spark.SecurityManager 54 - Changing modify acls groups to: 
2019-09-12 09:14:10,615  INFO org.apache.spark.SecurityManager 54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(OUY); groups with view permissions: Set(); users  with modify permissions: Set(OUY); groups with modify permissions: Set()
2019-09-12 09:14:11,474  INFO org.apache.spark.util.Utils 54 - Successfully started service 'sparkDriver' on port 2969.
2019-09-12 09:14:11,505  INFO org.apache.spark.SparkEnv 54 - Registering MapOutputTracker
2019-09-12 09:14:11,537  INFO org.apache.spark.SparkEnv 54 - Registering BlockManagerMaster
2019-09-12 09:14:11,537  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-09-12 09:14:11,537  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - BlockManagerMasterEndpoint up
2019-09-12 09:14:11,552  INFO org.apache.spark.storage.DiskBlockManager 54 - Created local directory at C:\Users\OUY\AppData\Local\Temp\blockmgr-3f02b5f0-7c27-456e-9e94-36f53dc97a67
2019-09-12 09:14:11,584  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore started with capacity 899.7 MB
2019-09-12 09:14:11,662  INFO org.apache.spark.SparkEnv 54 - Registering OutputCommitCoordinator
2019-09-12 09:14:11,802  INFO org.spark_project.jetty.util.log 192 - Logging initialized @4592ms
2019-09-12 09:14:11,880  INFO org.spark_project.jetty.server.Server 345 - jetty-9.3.z-SNAPSHOT
2019-09-12 09:14:11,896  INFO org.spark_project.jetty.server.Server 403 - Started @4689ms
2019-09-12 09:14:11,912  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2019-09-12 09:14:11,912  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
2019-09-12 09:14:11,927  INFO org.spark_project.jetty.server.AbstractConnector 270 - Started ServerConnector@7f8abce8{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 09:14:11,927  INFO org.apache.spark.util.Utils 54 - Successfully started service 'SparkUI' on port 4042.
2019-09-12 09:14:11,974  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5fe7f967{/jobs,null,AVAILABLE,@Spark}
2019-09-12 09:14:11,990  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5633dafd{/jobs/json,null,AVAILABLE,@Spark}
2019-09-12 09:14:11,990  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2eadc9f6{/jobs/job,null,AVAILABLE,@Spark}
2019-09-12 09:14:11,990  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@778d82e9{/jobs/job/json,null,AVAILABLE,@Spark}
2019-09-12 09:14:11,990  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@59901c4d{/stages,null,AVAILABLE,@Spark}
2019-09-12 09:14:11,990  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@d8d9199{/stages/json,null,AVAILABLE,@Spark}
2019-09-12 09:14:11,990  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@602ae7b6{/stages/stage,null,AVAILABLE,@Spark}
2019-09-12 09:14:11,990  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@13da7ab0{/stages/stage/json,null,AVAILABLE,@Spark}
2019-09-12 09:14:11,990  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@260ff5b7{/stages/pool,null,AVAILABLE,@Spark}
2019-09-12 09:14:11,990  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@77eb5790{/stages/pool/json,null,AVAILABLE,@Spark}
2019-09-12 09:14:11,990  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@319c3a25{/storage,null,AVAILABLE,@Spark}
2019-09-12 09:14:11,990  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@ef1695a{/storage/json,null,AVAILABLE,@Spark}
2019-09-12 09:14:11,990  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@81b5db0{/storage/rdd,null,AVAILABLE,@Spark}
2019-09-12 09:14:11,990  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7139bd31{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-09-12 09:14:12,005  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4b3fe06e{/environment,null,AVAILABLE,@Spark}
2019-09-12 09:14:12,005  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3e17a0a1{/environment/json,null,AVAILABLE,@Spark}
2019-09-12 09:14:12,005  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4d8286c4{/executors,null,AVAILABLE,@Spark}
2019-09-12 09:14:12,005  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@161f6623{/executors/json,null,AVAILABLE,@Spark}
2019-09-12 09:14:12,005  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6778aea6{/executors/threadDump,null,AVAILABLE,@Spark}
2019-09-12 09:14:12,005  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@69228e85{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-09-12 09:14:12,021  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5853495b{/static,null,AVAILABLE,@Spark}
2019-09-12 09:14:12,021  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@34b27915{/,null,AVAILABLE,@Spark}
2019-09-12 09:14:12,021  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1b9776f5{/api,null,AVAILABLE,@Spark}
2019-09-12 09:14:12,021  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@57402ba1{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-09-12 09:14:12,021  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@702b06fb{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-09-12 09:14:12,037  INFO org.apache.spark.ui.SparkUI 54 - Bound SparkUI to 0.0.0.0, and started at http://10.6.1.14:4042
2019-09-12 09:14:12,255  INFO org.apache.spark.executor.Executor 54 - Starting executor ID driver on host localhost
2019-09-12 09:14:12,333  INFO org.apache.spark.util.Utils 54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 2982.
2019-09-12 09:14:12,333  INFO org.apache.spark.network.netty.NettyBlockTransferService 54 - Server created on 10.6.1.14:2982
2019-09-12 09:14:12,333  INFO org.apache.spark.storage.BlockManager 54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-09-12 09:14:12,333  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registering BlockManager BlockManagerId(driver, 10.6.1.14, 2982, None)
2019-09-12 09:14:12,349  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Registering block manager 10.6.1.14:2982 with 899.7 MB RAM, BlockManagerId(driver, 10.6.1.14, 2982, None)
2019-09-12 09:14:12,365  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registered BlockManager BlockManagerId(driver, 10.6.1.14, 2982, None)
2019-09-12 09:14:12,365  INFO org.apache.spark.storage.BlockManager 54 - Initialized BlockManager: BlockManagerId(driver, 10.6.1.14, 2982, None)
2019-09-12 09:14:12,630  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5e922647{/metrics/json,null,AVAILABLE,@Spark}
2019-09-12 09:14:12,740  INFO org.apache.spark.sql.internal.SharedState 54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/MyProject/splitCHword/spark-warehouse/').
2019-09-12 09:14:12,755  INFO org.apache.spark.sql.internal.SharedState 54 - Warehouse path is 'file:/D:/MyProject/splitCHword/spark-warehouse/'.
2019-09-12 09:14:12,755  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@161aa04a{/SQL,null,AVAILABLE,@Spark}
2019-09-12 09:14:12,755  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6848a051{/SQL/json,null,AVAILABLE,@Spark}
2019-09-12 09:14:12,755  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2b0b7e5a{/SQL/execution,null,AVAILABLE,@Spark}
2019-09-12 09:14:12,771  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3119cf6f{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-09-12 09:14:12,771  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@79518e00{/static/sql,null,AVAILABLE,@Spark}
2019-09-12 09:14:14,177  INFO org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef 54 - Registered StateStoreCoordinator endpoint
2019-09-12 09:14:14,630  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0 stored as values in memory (estimated size 219.8 KB, free 899.5 MB)
2019-09-12 09:14:14,802  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 09:14:14,802  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_0_piece0 in memory on 10.6.1.14:2982 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 09:14:14,817  INFO org.apache.spark.SparkContext 54 - Created broadcast 0 from load at SparkUtil.java:28
2019-09-12 09:14:15,301  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 09:14:15,301  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 09:14:15,317  INFO org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 413 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 326
2019-09-12 09:14:15,411  INFO org.apache.spark.SparkContext 54 - Starting job: load at SparkUtil.java:28
2019-09-12 09:14:15,426  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 0 (load at SparkUtil.java:28) with 1 output partitions
2019-09-12 09:14:15,426  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 0 (load at SparkUtil.java:28)
2019-09-12 09:14:15,426  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 09:14:15,442  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 09:14:15,442  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:28), which has no missing parents
2019-09-12 09:14:15,473  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1 stored as values in memory (estimated size 5.5 KB, free 899.5 MB)
2019-09-12 09:14:15,489  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 899.5 MB)
2019-09-12 09:14:15,489  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_1_piece0 in memory on 10.6.1.14:2982 (size: 3.4 KB, free: 899.7 MB)
2019-09-12 09:14:15,489  INFO org.apache.spark.SparkContext 54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-09-12 09:14:15,520  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:28) (first 15 tasks are for partitions Vector(0))
2019-09-12 09:14:15,520  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 0.0 with 1 tasks
2019-09-12 09:14:15,583  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes)
2019-09-12 09:14:15,598  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 0.0 (TID 0)
2019-09-12 09:14:15,692  INFO org.apache.spark.rdd.BinaryFileRDD 54 - Input split: Paths:/D:/MyProject/splitCHword/target/classes/test.csv:0+326
2019-09-12 09:14:15,895  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 0.0 (TID 0). 853 bytes result sent to driver
2019-09-12 09:14:15,911  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 0.0 (TID 0) in 344 ms on localhost (executor driver) (1/1)
2019-09-12 09:14:15,911  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-09-12 09:14:15,911  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 0 (load at SparkUtil.java:28) finished in 0.375 s
2019-09-12 09:14:15,926  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 0 finished: load at SparkUtil.java:28, took 0.522487 s
2019-09-12 09:14:16,692  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_1_piece0 on 10.6.1.14:2982 in memory (size: 3.4 KB, free: 899.7 MB)
2019-09-12 09:14:18,519  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_0_piece0 on 10.6.1.14:2982 in memory (size: 20.7 KB, free: 899.7 MB)
2019-09-12 09:14:18,691  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Pruning directories with: 
2019-09-12 09:14:18,691  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Post-Scan Filters: 
2019-09-12 09:14:18,707  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Output Data Schema: struct<feature: string>
2019-09-12 09:14:18,707  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Pushed Filters: 
2019-09-12 09:14:19,316  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 295.060655 ms
2019-09-12 09:14:19,316  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2 stored as values in memory (estimated size 220.5 KB, free 899.5 MB)
2019-09-12 09:14:19,332  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.6 KB, free 899.5 MB)
2019-09-12 09:14:19,347  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_2_piece0 in memory on 10.6.1.14:2982 (size: 20.6 KB, free: 899.7 MB)
2019-09-12 09:14:19,347  INFO org.apache.spark.SparkContext 54 - Created broadcast 2 from show at SparkUtil.java:40
2019-09-12 09:14:19,363  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2019-09-12 09:14:19,425  INFO org.apache.spark.SparkContext 54 - Starting job: show at SparkUtil.java:40
2019-09-12 09:14:19,425  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 1 (show at SparkUtil.java:40) with 1 output partitions
2019-09-12 09:14:19,425  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 1 (show at SparkUtil.java:40)
2019-09-12 09:14:19,425  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 09:14:19,425  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 09:14:19,425  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:40), which has no missing parents
2019-09-12 09:14:19,457  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3 stored as values in memory (estimated size 14.2 KB, free 899.5 MB)
2019-09-12 09:14:19,457  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.9 KB, free 899.4 MB)
2019-09-12 09:14:19,457  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_3_piece0 in memory on 10.6.1.14:2982 (size: 6.9 KB, free: 899.7 MB)
2019-09-12 09:14:19,457  INFO org.apache.spark.SparkContext 54 - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2019-09-12 09:14:19,457  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:40) (first 15 tasks are for partitions Vector(0))
2019-09-12 09:14:19,457  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 1.0 with 1 tasks
2019-09-12 09:14:19,472  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5293 bytes)
2019-09-12 09:14:19,472  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 1.0 (TID 1)
2019-09-12 09:14:19,488  INFO org.apache.spark.sql.execution.datasources.FileScanRDD 54 - Reading File path: file:///D:/MyProject/splitCHword/target/classes/test.csv, range: 0-326, partition values: [empty row]
2019-09-12 09:14:19,504  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 13.562016 ms
2019-09-12 09:14:19,769  INFO org.ansj.util.MyStaticValue 106 - init version.number to env value is : 2.11.8
2019-09-12 09:14:19,769  INFO org.ansj.util.MyStaticValue 106 - init copyright.string to env value is : Copyright 2002-2016, LAMP/EPFL
2019-09-12 09:14:19,769  INFO org.ansj.util.MyStaticValue 106 - init osgi.version.number to env value is : 2.11.8.v20160304-115712-1706a37eb8
2019-09-12 09:14:19,769  INFO org.ansj.util.MyStaticValue 106 - init maven.version.number to env value is : 2.11.8
2019-09-12 09:14:19,769  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/ambiguity.dic
2019-09-12 09:14:19,769 ERROR org.ansj.library.AmbiguityLibrary 117 - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:D:\MyProject\splitCHword\library\ambiguity.dic not found or can not to read, path: library/ambiguity.dic
2019-09-12 09:14:19,769  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/default.dic
2019-09-12 09:14:19,769 ERROR org.ansj.library.DicLibrary 221 - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:D:\MyProject\splitCHword\library\default.dic not found or can not to read, path: library/default.dic
2019-09-12 09:14:20,816  INFO org.ansj.library.DATDictionary 90 - init core library ok use time : 937
2019-09-12 09:14:21,300  INFO org.ansj.library.NgramLibrary 17 - init ngram ok use time :469
2019-09-12 09:14:21,331  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 1.0 (TID 1). 1343 bytes result sent to driver
2019-09-12 09:14:21,331  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 1.0 (TID 1) in 1874 ms on localhost (executor driver) (1/1)
2019-09-12 09:14:21,331  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-09-12 09:14:21,331  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 1 (show at SparkUtil.java:40) finished in 1.874 s
2019-09-12 09:14:21,331  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 1 finished: show at SparkUtil.java:40, took 1.914771 s
2019-09-12 09:14:21,363  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 9.407991 ms
2019-09-12 09:14:21,378  INFO org.apache.spark.SparkContext 54 - Invoking stop() from shutdown hook
2019-09-12 09:14:21,394  INFO org.spark_project.jetty.server.AbstractConnector 310 - Stopped Spark@7f8abce8{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 09:14:21,394  INFO org.apache.spark.ui.SparkUI 54 - Stopped Spark web UI at http://10.6.1.14:4042
2019-09-12 09:14:21,409  INFO org.apache.spark.MapOutputTrackerMasterEndpoint 54 - MapOutputTrackerMasterEndpoint stopped!
2019-09-12 09:14:21,441  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore cleared
2019-09-12 09:14:21,441  INFO org.apache.spark.storage.BlockManager 54 - BlockManager stopped
2019-09-12 09:14:21,441  INFO org.apache.spark.storage.BlockManagerMaster 54 - BlockManagerMaster stopped
2019-09-12 09:14:21,456  INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint 54 - OutputCommitCoordinator stopped!
2019-09-12 09:14:21,456  INFO org.apache.spark.SparkContext 54 - Successfully stopped SparkContext
2019-09-12 09:14:21,456  INFO org.apache.spark.util.ShutdownHookManager 54 - Shutdown hook called
2019-09-12 09:14:21,456  INFO org.apache.spark.util.ShutdownHookManager 54 - Deleting directory C:\Users\OUY\AppData\Local\Temp\spark-86a47e6d-4677-4c12-8f2e-c5787de8b494
2019-09-12 09:15:09,426  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-12 09:15:09,988  WARN org.apache.hadoop.util.NativeCodeLoader 62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-12 09:15:10,082 ERROR org.apache.hadoop.util.Shell 396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2430)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
	at com.datamodel.SparkUtil.CreateSession(SparkUtil.java:19)
	at com.datamodel.SparkUtil.readDataFromCsv(SparkUtil.java:24)
	at com.datamodel.SparkUtil.main(SparkUtil.java:37)
2019-09-12 09:15:10,270  INFO org.apache.spark.SparkContext 54 - Submitted application: splitCHWord
2019-09-12 09:15:10,332  INFO org.apache.spark.SecurityManager 54 - Changing view acls to: OUY
2019-09-12 09:15:10,332  INFO org.apache.spark.SecurityManager 54 - Changing modify acls to: OUY
2019-09-12 09:15:10,332  INFO org.apache.spark.SecurityManager 54 - Changing view acls groups to: 
2019-09-12 09:15:10,332  INFO org.apache.spark.SecurityManager 54 - Changing modify acls groups to: 
2019-09-12 09:15:10,332  INFO org.apache.spark.SecurityManager 54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(OUY); groups with view permissions: Set(); users  with modify permissions: Set(OUY); groups with modify permissions: Set()
2019-09-12 09:15:11,191  INFO org.apache.spark.util.Utils 54 - Successfully started service 'sparkDriver' on port 3013.
2019-09-12 09:15:11,223  INFO org.apache.spark.SparkEnv 54 - Registering MapOutputTracker
2019-09-12 09:15:11,238  INFO org.apache.spark.SparkEnv 54 - Registering BlockManagerMaster
2019-09-12 09:15:11,254  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-09-12 09:15:11,254  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - BlockManagerMasterEndpoint up
2019-09-12 09:15:11,269  INFO org.apache.spark.storage.DiskBlockManager 54 - Created local directory at C:\Users\OUY\AppData\Local\Temp\blockmgr-9e566e72-afea-462b-967a-18eba013fa92
2019-09-12 09:15:11,301  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore started with capacity 899.7 MB
2019-09-12 09:15:11,394  INFO org.apache.spark.SparkEnv 54 - Registering OutputCommitCoordinator
2019-09-12 09:15:11,504  INFO org.spark_project.jetty.util.log 192 - Logging initialized @4617ms
2019-09-12 09:15:11,582  INFO org.spark_project.jetty.server.Server 345 - jetty-9.3.z-SNAPSHOT
2019-09-12 09:15:11,597  INFO org.spark_project.jetty.server.Server 403 - Started @4712ms
2019-09-12 09:15:11,629  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2019-09-12 09:15:11,629  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
2019-09-12 09:15:11,644  INFO org.spark_project.jetty.server.AbstractConnector 270 - Started ServerConnector@5162ae33{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 09:15:11,644  INFO org.apache.spark.util.Utils 54 - Successfully started service 'SparkUI' on port 4042.
2019-09-12 09:15:11,691  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5fe7f967{/jobs,null,AVAILABLE,@Spark}
2019-09-12 09:15:11,691  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5633dafd{/jobs/json,null,AVAILABLE,@Spark}
2019-09-12 09:15:11,691  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2eadc9f6{/jobs/job,null,AVAILABLE,@Spark}
2019-09-12 09:15:11,691  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@778d82e9{/jobs/job/json,null,AVAILABLE,@Spark}
2019-09-12 09:15:11,691  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@59901c4d{/stages,null,AVAILABLE,@Spark}
2019-09-12 09:15:11,691  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@d8d9199{/stages/json,null,AVAILABLE,@Spark}
2019-09-12 09:15:11,691  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@602ae7b6{/stages/stage,null,AVAILABLE,@Spark}
2019-09-12 09:15:11,691  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@13da7ab0{/stages/stage/json,null,AVAILABLE,@Spark}
2019-09-12 09:15:11,707  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@260ff5b7{/stages/pool,null,AVAILABLE,@Spark}
2019-09-12 09:15:11,707  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@77eb5790{/stages/pool/json,null,AVAILABLE,@Spark}
2019-09-12 09:15:11,707  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@319c3a25{/storage,null,AVAILABLE,@Spark}
2019-09-12 09:15:11,707  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@ef1695a{/storage/json,null,AVAILABLE,@Spark}
2019-09-12 09:15:11,707  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@81b5db0{/storage/rdd,null,AVAILABLE,@Spark}
2019-09-12 09:15:11,707  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7139bd31{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-09-12 09:15:11,707  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4b3fe06e{/environment,null,AVAILABLE,@Spark}
2019-09-12 09:15:11,707  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3e17a0a1{/environment/json,null,AVAILABLE,@Spark}
2019-09-12 09:15:11,707  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4d8286c4{/executors,null,AVAILABLE,@Spark}
2019-09-12 09:15:11,722  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@161f6623{/executors/json,null,AVAILABLE,@Spark}
2019-09-12 09:15:11,722  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6778aea6{/executors/threadDump,null,AVAILABLE,@Spark}
2019-09-12 09:15:11,722  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@69228e85{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-09-12 09:15:11,722  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5853495b{/static,null,AVAILABLE,@Spark}
2019-09-12 09:15:11,722  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@34b27915{/,null,AVAILABLE,@Spark}
2019-09-12 09:15:11,738  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1b9776f5{/api,null,AVAILABLE,@Spark}
2019-09-12 09:15:11,738  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@57402ba1{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-09-12 09:15:11,738  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@702b06fb{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-09-12 09:15:11,738  INFO org.apache.spark.ui.SparkUI 54 - Bound SparkUI to 0.0.0.0, and started at http://10.6.1.14:4042
2019-09-12 09:15:11,941  INFO org.apache.spark.executor.Executor 54 - Starting executor ID driver on host localhost
2019-09-12 09:15:11,972  INFO org.apache.spark.util.Utils 54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 3026.
2019-09-12 09:15:11,972  INFO org.apache.spark.network.netty.NettyBlockTransferService 54 - Server created on 10.6.1.14:3026
2019-09-12 09:15:11,988  INFO org.apache.spark.storage.BlockManager 54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-09-12 09:15:11,988  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registering BlockManager BlockManagerId(driver, 10.6.1.14, 3026, None)
2019-09-12 09:15:11,988  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Registering block manager 10.6.1.14:3026 with 899.7 MB RAM, BlockManagerId(driver, 10.6.1.14, 3026, None)
2019-09-12 09:15:11,988  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registered BlockManager BlockManagerId(driver, 10.6.1.14, 3026, None)
2019-09-12 09:15:12,004  INFO org.apache.spark.storage.BlockManager 54 - Initialized BlockManager: BlockManagerId(driver, 10.6.1.14, 3026, None)
2019-09-12 09:15:12,207  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5e922647{/metrics/json,null,AVAILABLE,@Spark}
2019-09-12 09:15:12,316  INFO org.apache.spark.sql.internal.SharedState 54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/MyProject/splitCHword/spark-warehouse/').
2019-09-12 09:15:12,316  INFO org.apache.spark.sql.internal.SharedState 54 - Warehouse path is 'file:/D:/MyProject/splitCHword/spark-warehouse/'.
2019-09-12 09:15:12,332  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@161aa04a{/SQL,null,AVAILABLE,@Spark}
2019-09-12 09:15:12,332  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6848a051{/SQL/json,null,AVAILABLE,@Spark}
2019-09-12 09:15:12,332  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2b0b7e5a{/SQL/execution,null,AVAILABLE,@Spark}
2019-09-12 09:15:12,332  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3119cf6f{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-09-12 09:15:12,347  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@79518e00{/static/sql,null,AVAILABLE,@Spark}
2019-09-12 09:15:13,941  INFO org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef 54 - Registered StateStoreCoordinator endpoint
2019-09-12 09:15:14,331  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0 stored as values in memory (estimated size 219.8 KB, free 899.5 MB)
2019-09-12 09:15:14,456  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 09:15:14,456  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_0_piece0 in memory on 10.6.1.14:3026 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 09:15:14,472  INFO org.apache.spark.SparkContext 54 - Created broadcast 0 from load at SparkUtil.java:28
2019-09-12 09:15:14,894  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 09:15:14,909  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 09:15:14,925  INFO org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 413 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 326
2019-09-12 09:15:15,003  INFO org.apache.spark.SparkContext 54 - Starting job: load at SparkUtil.java:28
2019-09-12 09:15:15,034  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 0 (load at SparkUtil.java:28) with 1 output partitions
2019-09-12 09:15:15,034  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 0 (load at SparkUtil.java:28)
2019-09-12 09:15:15,034  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 09:15:15,034  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 09:15:15,034  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:28), which has no missing parents
2019-09-12 09:15:15,081  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1 stored as values in memory (estimated size 5.5 KB, free 899.5 MB)
2019-09-12 09:15:15,081  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 899.5 MB)
2019-09-12 09:15:15,097  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_1_piece0 in memory on 10.6.1.14:3026 (size: 3.4 KB, free: 899.7 MB)
2019-09-12 09:15:15,097  INFO org.apache.spark.SparkContext 54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-09-12 09:15:15,128  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:28) (first 15 tasks are for partitions Vector(0))
2019-09-12 09:15:15,128  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 0.0 with 1 tasks
2019-09-12 09:15:15,190  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes)
2019-09-12 09:15:15,206  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 0.0 (TID 0)
2019-09-12 09:15:15,300  INFO org.apache.spark.rdd.BinaryFileRDD 54 - Input split: Paths:/D:/MyProject/splitCHword/target/classes/test.csv:0+326
2019-09-12 09:15:15,503  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 0.0 (TID 0). 853 bytes result sent to driver
2019-09-12 09:15:15,518  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 0.0 (TID 0) in 328 ms on localhost (executor driver) (1/1)
2019-09-12 09:15:15,518  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-09-12 09:15:15,518  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 0 (load at SparkUtil.java:28) finished in 0.375 s
2019-09-12 09:15:15,534  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 0 finished: load at SparkUtil.java:28, took 0.528041 s
2019-09-12 09:15:16,284  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_1_piece0 on 10.6.1.14:3026 in memory (size: 3.4 KB, free: 899.7 MB)
2019-09-12 09:15:17,924  INFO org.apache.spark.SparkContext 54 - Invoking stop() from shutdown hook
2019-09-12 09:15:17,940  INFO org.spark_project.jetty.server.AbstractConnector 310 - Stopped Spark@5162ae33{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 09:15:17,940  INFO org.apache.spark.ui.SparkUI 54 - Stopped Spark web UI at http://10.6.1.14:4042
2019-09-12 09:15:17,971  INFO org.apache.spark.MapOutputTrackerMasterEndpoint 54 - MapOutputTrackerMasterEndpoint stopped!
2019-09-12 09:15:18,205  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore cleared
2019-09-12 09:15:18,221  INFO org.apache.spark.storage.BlockManager 54 - BlockManager stopped
2019-09-12 09:15:18,221  INFO org.apache.spark.storage.BlockManagerMaster 54 - BlockManagerMaster stopped
2019-09-12 09:15:18,221  INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint 54 - OutputCommitCoordinator stopped!
2019-09-12 09:15:18,236  INFO org.apache.spark.SparkContext 54 - Successfully stopped SparkContext
2019-09-12 09:15:18,236  INFO org.apache.spark.util.ShutdownHookManager 54 - Shutdown hook called
2019-09-12 09:15:18,236  INFO org.apache.spark.util.ShutdownHookManager 54 - Deleting directory C:\Users\OUY\AppData\Local\Temp\spark-6de191b7-52df-483f-9638-b9fe52dc2828
2019-09-12 09:16:32,248  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-12 09:16:32,842  WARN org.apache.hadoop.util.NativeCodeLoader 62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-12 09:16:32,935 ERROR org.apache.hadoop.util.Shell 396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2430)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
	at com.datamodel.SparkUtil.CreateSession(SparkUtil.java:19)
	at com.datamodel.SparkUtil.readDataFromCsv(SparkUtil.java:24)
	at com.datamodel.SparkUtil.main(SparkUtil.java:37)
2019-09-12 09:16:33,138  INFO org.apache.spark.SparkContext 54 - Submitted application: splitCHWord
2019-09-12 09:16:33,185  INFO org.apache.spark.SecurityManager 54 - Changing view acls to: OUY
2019-09-12 09:16:33,185  INFO org.apache.spark.SecurityManager 54 - Changing modify acls to: OUY
2019-09-12 09:16:33,185  INFO org.apache.spark.SecurityManager 54 - Changing view acls groups to: 
2019-09-12 09:16:33,201  INFO org.apache.spark.SecurityManager 54 - Changing modify acls groups to: 
2019-09-12 09:16:33,201  INFO org.apache.spark.SecurityManager 54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(OUY); groups with view permissions: Set(); users  with modify permissions: Set(OUY); groups with modify permissions: Set()
2019-09-12 09:16:34,154  INFO org.apache.spark.util.Utils 54 - Successfully started service 'sparkDriver' on port 3056.
2019-09-12 09:16:34,201  INFO org.apache.spark.SparkEnv 54 - Registering MapOutputTracker
2019-09-12 09:16:34,248  INFO org.apache.spark.SparkEnv 54 - Registering BlockManagerMaster
2019-09-12 09:16:34,248  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-09-12 09:16:34,248  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - BlockManagerMasterEndpoint up
2019-09-12 09:16:34,263  INFO org.apache.spark.storage.DiskBlockManager 54 - Created local directory at C:\Users\OUY\AppData\Local\Temp\blockmgr-2205191c-6223-4da1-b591-75465e8d9551
2019-09-12 09:16:34,310  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore started with capacity 899.7 MB
2019-09-12 09:16:34,419  INFO org.apache.spark.SparkEnv 54 - Registering OutputCommitCoordinator
2019-09-12 09:16:34,529  INFO org.spark_project.jetty.util.log 192 - Logging initialized @4887ms
2019-09-12 09:16:34,591  INFO org.spark_project.jetty.server.Server 345 - jetty-9.3.z-SNAPSHOT
2019-09-12 09:16:34,622  INFO org.spark_project.jetty.server.Server 403 - Started @4981ms
2019-09-12 09:16:34,638  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2019-09-12 09:16:34,638  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
2019-09-12 09:16:34,654  INFO org.spark_project.jetty.server.AbstractConnector 270 - Started ServerConnector@630be695{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 09:16:34,654  INFO org.apache.spark.util.Utils 54 - Successfully started service 'SparkUI' on port 4042.
2019-09-12 09:16:34,701  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5fe7f967{/jobs,null,AVAILABLE,@Spark}
2019-09-12 09:16:34,701  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5633dafd{/jobs/json,null,AVAILABLE,@Spark}
2019-09-12 09:16:34,701  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2eadc9f6{/jobs/job,null,AVAILABLE,@Spark}
2019-09-12 09:16:34,701  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@778d82e9{/jobs/job/json,null,AVAILABLE,@Spark}
2019-09-12 09:16:34,701  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@59901c4d{/stages,null,AVAILABLE,@Spark}
2019-09-12 09:16:34,701  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@d8d9199{/stages/json,null,AVAILABLE,@Spark}
2019-09-12 09:16:34,716  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@602ae7b6{/stages/stage,null,AVAILABLE,@Spark}
2019-09-12 09:16:34,716  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@13da7ab0{/stages/stage/json,null,AVAILABLE,@Spark}
2019-09-12 09:16:34,716  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@260ff5b7{/stages/pool,null,AVAILABLE,@Spark}
2019-09-12 09:16:34,716  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@77eb5790{/stages/pool/json,null,AVAILABLE,@Spark}
2019-09-12 09:16:34,716  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@319c3a25{/storage,null,AVAILABLE,@Spark}
2019-09-12 09:16:34,716  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@ef1695a{/storage/json,null,AVAILABLE,@Spark}
2019-09-12 09:16:34,716  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@81b5db0{/storage/rdd,null,AVAILABLE,@Spark}
2019-09-12 09:16:34,716  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7139bd31{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-09-12 09:16:34,716  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4b3fe06e{/environment,null,AVAILABLE,@Spark}
2019-09-12 09:16:34,716  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3e17a0a1{/environment/json,null,AVAILABLE,@Spark}
2019-09-12 09:16:34,716  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4d8286c4{/executors,null,AVAILABLE,@Spark}
2019-09-12 09:16:34,732  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@161f6623{/executors/json,null,AVAILABLE,@Spark}
2019-09-12 09:16:34,732  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6778aea6{/executors/threadDump,null,AVAILABLE,@Spark}
2019-09-12 09:16:34,732  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@69228e85{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-09-12 09:16:34,747  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5853495b{/static,null,AVAILABLE,@Spark}
2019-09-12 09:16:34,747  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@34b27915{/,null,AVAILABLE,@Spark}
2019-09-12 09:16:34,747  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1b9776f5{/api,null,AVAILABLE,@Spark}
2019-09-12 09:16:34,747  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@57402ba1{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-09-12 09:16:34,747  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@702b06fb{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-09-12 09:16:34,747  INFO org.apache.spark.ui.SparkUI 54 - Bound SparkUI to 0.0.0.0, and started at http://10.6.1.14:4042
2019-09-12 09:16:34,966  INFO org.apache.spark.executor.Executor 54 - Starting executor ID driver on host localhost
2019-09-12 09:16:35,013  INFO org.apache.spark.util.Utils 54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 3069.
2019-09-12 09:16:35,013  INFO org.apache.spark.network.netty.NettyBlockTransferService 54 - Server created on 10.6.1.14:3069
2019-09-12 09:16:35,013  INFO org.apache.spark.storage.BlockManager 54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-09-12 09:16:35,013  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registering BlockManager BlockManagerId(driver, 10.6.1.14, 3069, None)
2019-09-12 09:16:35,029  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Registering block manager 10.6.1.14:3069 with 899.7 MB RAM, BlockManagerId(driver, 10.6.1.14, 3069, None)
2019-09-12 09:16:35,029  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registered BlockManager BlockManagerId(driver, 10.6.1.14, 3069, None)
2019-09-12 09:16:35,029  INFO org.apache.spark.storage.BlockManager 54 - Initialized BlockManager: BlockManagerId(driver, 10.6.1.14, 3069, None)
2019-09-12 09:16:35,279  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5e922647{/metrics/json,null,AVAILABLE,@Spark}
2019-09-12 09:16:35,403  INFO org.apache.spark.sql.internal.SharedState 54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/MyProject/splitCHword/spark-warehouse/').
2019-09-12 09:16:35,403  INFO org.apache.spark.sql.internal.SharedState 54 - Warehouse path is 'file:/D:/MyProject/splitCHword/spark-warehouse/'.
2019-09-12 09:16:35,419  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@161aa04a{/SQL,null,AVAILABLE,@Spark}
2019-09-12 09:16:35,419  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6848a051{/SQL/json,null,AVAILABLE,@Spark}
2019-09-12 09:16:35,419  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2b0b7e5a{/SQL/execution,null,AVAILABLE,@Spark}
2019-09-12 09:16:35,419  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3119cf6f{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-09-12 09:16:35,419  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@79518e00{/static/sql,null,AVAILABLE,@Spark}
2019-09-12 09:16:36,778  INFO org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef 54 - Registered StateStoreCoordinator endpoint
2019-09-12 09:16:37,434  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0 stored as values in memory (estimated size 219.8 KB, free 899.5 MB)
2019-09-12 09:16:37,575  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 09:16:37,575  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_0_piece0 in memory on 10.6.1.14:3069 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 09:16:37,590  INFO org.apache.spark.SparkContext 54 - Created broadcast 0 from load at SparkUtil.java:28
2019-09-12 09:16:38,106  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 09:16:38,106  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 09:16:38,137  INFO org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 413 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 326
2019-09-12 09:16:38,200  INFO org.apache.spark.SparkContext 54 - Starting job: load at SparkUtil.java:28
2019-09-12 09:16:38,231  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 0 (load at SparkUtil.java:28) with 1 output partitions
2019-09-12 09:16:38,231  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 0 (load at SparkUtil.java:28)
2019-09-12 09:16:38,231  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 09:16:38,231  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 09:16:38,247  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:28), which has no missing parents
2019-09-12 09:16:38,309  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1 stored as values in memory (estimated size 5.5 KB, free 899.5 MB)
2019-09-12 09:16:38,309  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 899.5 MB)
2019-09-12 09:16:38,309  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_1_piece0 in memory on 10.6.1.14:3069 (size: 3.4 KB, free: 899.7 MB)
2019-09-12 09:16:38,309  INFO org.apache.spark.SparkContext 54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-09-12 09:16:38,356  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:28) (first 15 tasks are for partitions Vector(0))
2019-09-12 09:16:38,356  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 0.0 with 1 tasks
2019-09-12 09:16:38,434  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes)
2019-09-12 09:16:38,450  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 0.0 (TID 0)
2019-09-12 09:16:38,559  INFO org.apache.spark.rdd.BinaryFileRDD 54 - Input split: Paths:/D:/MyProject/splitCHword/target/classes/test.csv:0+326
2019-09-12 09:16:38,825  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 0.0 (TID 0). 853 bytes result sent to driver
2019-09-12 09:16:38,840  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 0.0 (TID 0) in 437 ms on localhost (executor driver) (1/1)
2019-09-12 09:16:38,840  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-09-12 09:16:38,856  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 0 (load at SparkUtil.java:28) finished in 0.469 s
2019-09-12 09:16:38,856  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 0 finished: load at SparkUtil.java:28, took 0.661987 s
2019-09-12 09:16:39,590  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_1_piece0 on 10.6.1.14:3069 in memory (size: 3.4 KB, free: 899.7 MB)
2019-09-12 09:16:41,574  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_0_piece0 on 10.6.1.14:3069 in memory (size: 20.7 KB, free: 899.7 MB)
2019-09-12 09:16:41,699  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Pruning directories with: 
2019-09-12 09:16:41,715  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Post-Scan Filters: 
2019-09-12 09:16:41,715  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Output Data Schema: struct<label: string, feature: string>
2019-09-12 09:16:41,730  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Pushed Filters: 
2019-09-12 09:16:42,214  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 259.386659 ms
2019-09-12 09:16:42,214  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2 stored as values in memory (estimated size 220.5 KB, free 899.5 MB)
2019-09-12 09:16:42,230  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.6 KB, free 899.5 MB)
2019-09-12 09:16:42,246  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_2_piece0 in memory on 10.6.1.14:3069 (size: 20.6 KB, free: 899.7 MB)
2019-09-12 09:16:42,246  INFO org.apache.spark.SparkContext 54 - Created broadcast 2 from show at SparkUtil.java:41
2019-09-12 09:16:42,261  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2019-09-12 09:16:42,308  INFO org.apache.spark.SparkContext 54 - Starting job: show at SparkUtil.java:41
2019-09-12 09:16:42,308  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 1 (show at SparkUtil.java:41) with 1 output partitions
2019-09-12 09:16:42,308  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 1 (show at SparkUtil.java:41)
2019-09-12 09:16:42,308  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 09:16:42,308  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 09:16:42,308  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:41), which has no missing parents
2019-09-12 09:16:42,339  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3 stored as values in memory (estimated size 9.4 KB, free 899.5 MB)
2019-09-12 09:16:42,355  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.3 KB, free 899.5 MB)
2019-09-12 09:16:42,355  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_3_piece0 in memory on 10.6.1.14:3069 (size: 5.3 KB, free: 899.7 MB)
2019-09-12 09:16:42,355  INFO org.apache.spark.SparkContext 54 - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2019-09-12 09:16:42,355  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:41) (first 15 tasks are for partitions Vector(0))
2019-09-12 09:16:42,355  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 1.0 with 1 tasks
2019-09-12 09:16:42,355  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5293 bytes)
2019-09-12 09:16:42,355  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 1.0 (TID 1)
2019-09-12 09:16:42,386  INFO org.apache.spark.sql.execution.datasources.FileScanRDD 54 - Reading File path: file:///D:/MyProject/splitCHword/target/classes/test.csv, range: 0-326, partition values: [empty row]
2019-09-12 09:16:42,417  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 18.124784 ms
2019-09-12 09:16:42,449  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 1.0 (TID 1). 1435 bytes result sent to driver
2019-09-12 09:16:42,449  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 1.0 (TID 1) in 94 ms on localhost (executor driver) (1/1)
2019-09-12 09:16:42,449  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 1 (show at SparkUtil.java:41) finished in 0.094 s
2019-09-12 09:16:42,464  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 1 finished: show at SparkUtil.java:41, took 0.147515 s
2019-09-12 09:16:42,464  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-09-12 09:16:42,511  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 24.525206 ms
2019-09-12 09:16:42,542  INFO org.apache.spark.SparkContext 54 - Invoking stop() from shutdown hook
2019-09-12 09:16:42,542  INFO org.spark_project.jetty.server.AbstractConnector 310 - Stopped Spark@630be695{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 09:16:42,558  INFO org.apache.spark.ui.SparkUI 54 - Stopped Spark web UI at http://10.6.1.14:4042
2019-09-12 09:16:42,574  INFO org.apache.spark.MapOutputTrackerMasterEndpoint 54 - MapOutputTrackerMasterEndpoint stopped!
2019-09-12 09:16:42,605  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore cleared
2019-09-12 09:16:42,605  INFO org.apache.spark.storage.BlockManager 54 - BlockManager stopped
2019-09-12 09:16:42,605  INFO org.apache.spark.storage.BlockManagerMaster 54 - BlockManagerMaster stopped
2019-09-12 09:16:42,605  INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint 54 - OutputCommitCoordinator stopped!
2019-09-12 09:16:42,621  INFO org.apache.spark.SparkContext 54 - Successfully stopped SparkContext
2019-09-12 09:16:42,621  INFO org.apache.spark.util.ShutdownHookManager 54 - Shutdown hook called
2019-09-12 09:16:42,621  INFO org.apache.spark.util.ShutdownHookManager 54 - Deleting directory C:\Users\OUY\AppData\Local\Temp\spark-e5cbdb22-ab54-4055-84ea-dd2cf36c522a
2019-09-12 09:17:11,679  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-12 09:17:12,382  WARN org.apache.hadoop.util.NativeCodeLoader 62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-12 09:17:12,538 ERROR org.apache.hadoop.util.Shell 396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2430)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
	at com.datamodel.SparkUtil.CreateSession(SparkUtil.java:19)
	at com.datamodel.SparkUtil.readDataFromCsv(SparkUtil.java:24)
	at com.datamodel.SparkUtil.main(SparkUtil.java:37)
2019-09-12 09:17:12,851  INFO org.apache.spark.SparkContext 54 - Submitted application: splitCHWord
2019-09-12 09:17:12,913  INFO org.apache.spark.SecurityManager 54 - Changing view acls to: OUY
2019-09-12 09:17:12,913  INFO org.apache.spark.SecurityManager 54 - Changing modify acls to: OUY
2019-09-12 09:17:12,913  INFO org.apache.spark.SecurityManager 54 - Changing view acls groups to: 
2019-09-12 09:17:12,913  INFO org.apache.spark.SecurityManager 54 - Changing modify acls groups to: 
2019-09-12 09:17:12,913  INFO org.apache.spark.SecurityManager 54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(OUY); groups with view permissions: Set(); users  with modify permissions: Set(OUY); groups with modify permissions: Set()
2019-09-12 09:17:13,882  INFO org.apache.spark.util.Utils 54 - Successfully started service 'sparkDriver' on port 3105.
2019-09-12 09:17:13,913  INFO org.apache.spark.SparkEnv 54 - Registering MapOutputTracker
2019-09-12 09:17:13,944  INFO org.apache.spark.SparkEnv 54 - Registering BlockManagerMaster
2019-09-12 09:17:13,944  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-09-12 09:17:13,944  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - BlockManagerMasterEndpoint up
2019-09-12 09:17:13,960  INFO org.apache.spark.storage.DiskBlockManager 54 - Created local directory at C:\Users\OUY\AppData\Local\Temp\blockmgr-6c4fbfcb-23a4-448a-9db0-e07863109033
2019-09-12 09:17:14,007  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore started with capacity 899.7 MB
2019-09-12 09:17:14,069  INFO org.apache.spark.SparkEnv 54 - Registering OutputCommitCoordinator
2019-09-12 09:17:14,194  INFO org.spark_project.jetty.util.log 192 - Logging initialized @6210ms
2019-09-12 09:17:14,272  INFO org.spark_project.jetty.server.Server 345 - jetty-9.3.z-SNAPSHOT
2019-09-12 09:17:14,288  INFO org.spark_project.jetty.server.Server 403 - Started @6307ms
2019-09-12 09:17:14,319  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2019-09-12 09:17:14,319  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
2019-09-12 09:17:14,319  INFO org.spark_project.jetty.server.AbstractConnector 270 - Started ServerConnector@6f2d1868{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 09:17:14,319  INFO org.apache.spark.util.Utils 54 - Successfully started service 'SparkUI' on port 4042.
2019-09-12 09:17:14,397  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6c42f2a1{/jobs,null,AVAILABLE,@Spark}
2019-09-12 09:17:14,397  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@42c28305{/jobs/json,null,AVAILABLE,@Spark}
2019-09-12 09:17:14,397  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7d199c68{/jobs/job,null,AVAILABLE,@Spark}
2019-09-12 09:17:14,413  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@216914{/jobs/job/json,null,AVAILABLE,@Spark}
2019-09-12 09:17:14,413  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@35764bef{/stages,null,AVAILABLE,@Spark}
2019-09-12 09:17:14,429  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5d5160e6{/stages/json,null,AVAILABLE,@Spark}
2019-09-12 09:17:14,444  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2903c6ff{/stages/stage,null,AVAILABLE,@Spark}
2019-09-12 09:17:14,460  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@168cd36b{/stages/stage/json,null,AVAILABLE,@Spark}
2019-09-12 09:17:14,460  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3901f6af{/stages/pool,null,AVAILABLE,@Spark}
2019-09-12 09:17:14,475  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@10cd6753{/stages/pool/json,null,AVAILABLE,@Spark}
2019-09-12 09:17:14,475  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@47af099e{/storage,null,AVAILABLE,@Spark}
2019-09-12 09:17:14,475  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@b835727{/storage/json,null,AVAILABLE,@Spark}
2019-09-12 09:17:14,475  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2c8662ac{/storage/rdd,null,AVAILABLE,@Spark}
2019-09-12 09:17:14,475  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3724b43e{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-09-12 09:17:14,475  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@68e7c8c3{/environment,null,AVAILABLE,@Spark}
2019-09-12 09:17:14,475  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@238bfd6c{/environment/json,null,AVAILABLE,@Spark}
2019-09-12 09:17:14,491  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@58860997{/executors,null,AVAILABLE,@Spark}
2019-09-12 09:17:14,491  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7487b142{/executors/json,null,AVAILABLE,@Spark}
2019-09-12 09:17:14,491  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@199bc830{/executors/threadDump,null,AVAILABLE,@Spark}
2019-09-12 09:17:14,507  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@27b45ea{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-09-12 09:17:14,522  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@790a251b{/static,null,AVAILABLE,@Spark}
2019-09-12 09:17:14,538  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@15b82644{/,null,AVAILABLE,@Spark}
2019-09-12 09:17:14,538  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@574cd322{/api,null,AVAILABLE,@Spark}
2019-09-12 09:17:14,538  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1bc776b7{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-09-12 09:17:14,538  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@a0f53fc{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-09-12 09:17:14,538  INFO org.apache.spark.ui.SparkUI 54 - Bound SparkUI to 0.0.0.0, and started at http://10.6.1.14:4042
2019-09-12 09:17:14,819  INFO org.apache.spark.executor.Executor 54 - Starting executor ID driver on host localhost
2019-09-12 09:17:14,929  INFO org.apache.spark.util.Utils 54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 3118.
2019-09-12 09:17:14,944  INFO org.apache.spark.network.netty.NettyBlockTransferService 54 - Server created on 10.6.1.14:3118
2019-09-12 09:17:14,944  INFO org.apache.spark.storage.BlockManager 54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-09-12 09:17:14,944  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registering BlockManager BlockManagerId(driver, 10.6.1.14, 3118, None)
2019-09-12 09:17:14,944  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Registering block manager 10.6.1.14:3118 with 899.7 MB RAM, BlockManagerId(driver, 10.6.1.14, 3118, None)
2019-09-12 09:17:14,960  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registered BlockManager BlockManagerId(driver, 10.6.1.14, 3118, None)
2019-09-12 09:17:14,960  INFO org.apache.spark.storage.BlockManager 54 - Initialized BlockManager: BlockManagerId(driver, 10.6.1.14, 3118, None)
2019-09-12 09:17:15,382  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@67cefd84{/metrics/json,null,AVAILABLE,@Spark}
2019-09-12 09:17:15,600  INFO org.apache.spark.sql.internal.SharedState 54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/MyProject/splitCHword/spark-warehouse/').
2019-09-12 09:17:15,600  INFO org.apache.spark.sql.internal.SharedState 54 - Warehouse path is 'file:/D:/MyProject/splitCHword/spark-warehouse/'.
2019-09-12 09:17:15,616  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@11f9535b{/SQL,null,AVAILABLE,@Spark}
2019-09-12 09:17:15,616  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1e236278{/SQL/json,null,AVAILABLE,@Spark}
2019-09-12 09:17:15,616  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4d6ccc97{/SQL/execution,null,AVAILABLE,@Spark}
2019-09-12 09:17:15,616  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@161aa04a{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-09-12 09:17:15,616  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2b0b7e5a{/static/sql,null,AVAILABLE,@Spark}
2019-09-12 09:17:17,990  INFO org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef 54 - Registered StateStoreCoordinator endpoint
2019-09-12 09:17:18,506  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0 stored as values in memory (estimated size 219.8 KB, free 899.5 MB)
2019-09-12 09:17:18,615  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 09:17:18,631  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_0_piece0 in memory on 10.6.1.14:3118 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 09:17:18,631  INFO org.apache.spark.SparkContext 54 - Created broadcast 0 from load at SparkUtil.java:28
2019-09-12 09:17:19,615  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 09:17:19,631  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 09:17:19,646  INFO org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 413 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 326
2019-09-12 09:17:19,724  INFO org.apache.spark.SparkContext 54 - Starting job: load at SparkUtil.java:28
2019-09-12 09:17:19,771  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 0 (load at SparkUtil.java:28) with 1 output partitions
2019-09-12 09:17:19,771  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 0 (load at SparkUtil.java:28)
2019-09-12 09:17:19,771  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 09:17:19,771  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 09:17:19,787  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:28), which has no missing parents
2019-09-12 09:17:19,912  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1 stored as values in memory (estimated size 5.5 KB, free 899.5 MB)
2019-09-12 09:17:19,943  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 899.5 MB)
2019-09-12 09:17:19,959  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_1_piece0 in memory on 10.6.1.14:3118 (size: 3.4 KB, free: 899.7 MB)
2019-09-12 09:17:19,959  INFO org.apache.spark.SparkContext 54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-09-12 09:17:19,990  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:28) (first 15 tasks are for partitions Vector(0))
2019-09-12 09:17:19,990  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 0.0 with 1 tasks
2019-09-12 09:17:20,068  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes)
2019-09-12 09:17:20,084  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 0.0 (TID 0)
2019-09-12 09:17:20,224  INFO org.apache.spark.rdd.BinaryFileRDD 54 - Input split: Paths:/D:/MyProject/splitCHword/target/classes/test.csv:0+326
2019-09-12 09:17:20,521  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 0.0 (TID 0). 853 bytes result sent to driver
2019-09-12 09:17:20,521  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 0.0 (TID 0) in 484 ms on localhost (executor driver) (1/1)
2019-09-12 09:17:20,537  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-09-12 09:17:20,537  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 0 (load at SparkUtil.java:28) finished in 0.516 s
2019-09-12 09:17:20,552  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 0 finished: load at SparkUtil.java:28, took 0.816278 s
2019-09-12 09:17:21,739  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_1_piece0 on 10.6.1.14:3118 in memory (size: 3.4 KB, free: 899.7 MB)
2019-09-12 09:17:23,614  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_0_piece0 on 10.6.1.14:3118 in memory (size: 20.7 KB, free: 899.7 MB)
2019-09-12 09:17:23,770  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Pruning directories with: 
2019-09-12 09:17:23,786  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Post-Scan Filters: 
2019-09-12 09:17:23,786  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Output Data Schema: struct<label: string, feature: string>
2019-09-12 09:17:23,801  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Pushed Filters: 
2019-09-12 09:17:24,317  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 289.063007 ms
2019-09-12 09:17:24,317  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2 stored as values in memory (estimated size 220.5 KB, free 899.5 MB)
2019-09-12 09:17:24,333  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.6 KB, free 899.5 MB)
2019-09-12 09:17:24,348  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_2_piece0 in memory on 10.6.1.14:3118 (size: 20.6 KB, free: 899.7 MB)
2019-09-12 09:17:24,348  INFO org.apache.spark.SparkContext 54 - Created broadcast 2 from show at SparkUtil.java:41
2019-09-12 09:17:24,364  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2019-09-12 09:17:24,426  INFO org.apache.spark.SparkContext 54 - Starting job: show at SparkUtil.java:41
2019-09-12 09:17:24,426  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 1 (show at SparkUtil.java:41) with 1 output partitions
2019-09-12 09:17:24,426  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 1 (show at SparkUtil.java:41)
2019-09-12 09:17:24,426  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 09:17:24,426  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 09:17:24,426  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:41), which has no missing parents
2019-09-12 09:17:24,473  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3 stored as values in memory (estimated size 9.4 KB, free 899.5 MB)
2019-09-12 09:17:24,489  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.3 KB, free 899.5 MB)
2019-09-12 09:17:24,489  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_3_piece0 in memory on 10.6.1.14:3118 (size: 5.3 KB, free: 899.7 MB)
2019-09-12 09:17:24,489  INFO org.apache.spark.SparkContext 54 - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2019-09-12 09:17:24,489  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:41) (first 15 tasks are for partitions Vector(0))
2019-09-12 09:17:24,489  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 1.0 with 1 tasks
2019-09-12 09:17:24,489  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5293 bytes)
2019-09-12 09:17:24,489  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 1.0 (TID 1)
2019-09-12 09:17:24,520  INFO org.apache.spark.sql.execution.datasources.FileScanRDD 54 - Reading File path: file:///D:/MyProject/splitCHword/target/classes/test.csv, range: 0-326, partition values: [empty row]
2019-09-12 09:17:24,567  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 24.843072 ms
2019-09-12 09:17:24,614  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 1.0 (TID 1). 1521 bytes result sent to driver
2019-09-12 09:17:24,614  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 1.0 (TID 1) in 125 ms on localhost (executor driver) (1/1)
2019-09-12 09:17:24,614  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 1 (show at SparkUtil.java:41) finished in 0.125 s
2019-09-12 09:17:24,614  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-09-12 09:17:24,629  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 1 finished: show at SparkUtil.java:41, took 0.196876 s
2019-09-12 09:17:24,661  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 20.836249 ms
2019-09-12 09:17:24,707  INFO org.apache.spark.SparkContext 54 - Invoking stop() from shutdown hook
2019-09-12 09:17:24,723  INFO org.spark_project.jetty.server.AbstractConnector 310 - Stopped Spark@6f2d1868{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 09:17:24,723  INFO org.apache.spark.ui.SparkUI 54 - Stopped Spark web UI at http://10.6.1.14:4042
2019-09-12 09:17:24,739  INFO org.apache.spark.MapOutputTrackerMasterEndpoint 54 - MapOutputTrackerMasterEndpoint stopped!
2019-09-12 09:17:24,786  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore cleared
2019-09-12 09:17:24,786  INFO org.apache.spark.storage.BlockManager 54 - BlockManager stopped
2019-09-12 09:17:24,786  INFO org.apache.spark.storage.BlockManagerMaster 54 - BlockManagerMaster stopped
2019-09-12 09:17:24,786  INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint 54 - OutputCommitCoordinator stopped!
2019-09-12 09:17:24,801  INFO org.apache.spark.SparkContext 54 - Successfully stopped SparkContext
2019-09-12 09:17:24,801  INFO org.apache.spark.util.ShutdownHookManager 54 - Shutdown hook called
2019-09-12 09:17:24,801  INFO org.apache.spark.util.ShutdownHookManager 54 - Deleting directory C:\Users\OUY\AppData\Local\Temp\spark-9a80c436-6d97-447a-a015-91f5ba434957
2019-09-12 09:26:06,922  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-12 09:26:07,625  WARN org.apache.hadoop.util.NativeCodeLoader 62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-12 09:26:07,750 ERROR org.apache.hadoop.util.Shell 396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2430)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
	at com.datamodel.SparkUtil.CreateSession(SparkUtil.java:19)
	at com.datamodel.SparkUtil.readDataFromCsv(SparkUtil.java:24)
	at com.datamodel.SparkUtil.main(SparkUtil.java:37)
2019-09-12 09:26:08,000  INFO org.apache.spark.SparkContext 54 - Submitted application: splitCHWord
2019-09-12 09:26:08,062  INFO org.apache.spark.SecurityManager 54 - Changing view acls to: OUY
2019-09-12 09:26:08,062  INFO org.apache.spark.SecurityManager 54 - Changing modify acls to: OUY
2019-09-12 09:26:08,062  INFO org.apache.spark.SecurityManager 54 - Changing view acls groups to: 
2019-09-12 09:26:08,062  INFO org.apache.spark.SecurityManager 54 - Changing modify acls groups to: 
2019-09-12 09:26:08,062  INFO org.apache.spark.SecurityManager 54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(OUY); groups with view permissions: Set(); users  with modify permissions: Set(OUY); groups with modify permissions: Set()
2019-09-12 09:26:09,130  INFO org.apache.spark.util.Utils 54 - Successfully started service 'sparkDriver' on port 3369.
2019-09-12 09:26:09,193  INFO org.apache.spark.SparkEnv 54 - Registering MapOutputTracker
2019-09-12 09:26:09,230  INFO org.apache.spark.SparkEnv 54 - Registering BlockManagerMaster
2019-09-12 09:26:09,236  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-09-12 09:26:09,237  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - BlockManagerMasterEndpoint up
2019-09-12 09:26:09,249  INFO org.apache.spark.storage.DiskBlockManager 54 - Created local directory at C:\Users\OUY\AppData\Local\Temp\blockmgr-4ded753a-ac8a-4cd1-9dc1-943e3d376f2c
2019-09-12 09:26:09,294  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore started with capacity 899.7 MB
2019-09-12 09:26:09,439  INFO org.apache.spark.SparkEnv 54 - Registering OutputCommitCoordinator
2019-09-12 09:26:09,557  INFO org.spark_project.jetty.util.log 192 - Logging initialized @5503ms
2019-09-12 09:26:09,659  INFO org.spark_project.jetty.server.Server 345 - jetty-9.3.z-SNAPSHOT
2019-09-12 09:26:09,681  INFO org.spark_project.jetty.server.Server 403 - Started @5624ms
2019-09-12 09:26:09,708  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2019-09-12 09:26:09,710  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
2019-09-12 09:26:09,723  INFO org.spark_project.jetty.server.AbstractConnector 270 - Started ServerConnector@729b55b3{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 09:26:09,723  INFO org.apache.spark.util.Utils 54 - Successfully started service 'SparkUI' on port 4042.
2019-09-12 09:26:09,770  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1aa6e3c0{/jobs,null,AVAILABLE,@Spark}
2019-09-12 09:26:09,771  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@35cd68d4{/jobs/json,null,AVAILABLE,@Spark}
2019-09-12 09:26:09,772  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@216914{/jobs/job,null,AVAILABLE,@Spark}
2019-09-12 09:26:09,774  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5d5160e6{/jobs/job/json,null,AVAILABLE,@Spark}
2019-09-12 09:26:09,776  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2903c6ff{/stages,null,AVAILABLE,@Spark}
2019-09-12 09:26:09,777  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@37af1f93{/stages/json,null,AVAILABLE,@Spark}
2019-09-12 09:26:09,778  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@408e96d9{/stages/stage,null,AVAILABLE,@Spark}
2019-09-12 09:26:09,781  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@10cd6753{/stages/stage/json,null,AVAILABLE,@Spark}
2019-09-12 09:26:09,783  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@47af099e{/stages/pool,null,AVAILABLE,@Spark}
2019-09-12 09:26:09,785  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@b835727{/stages/pool/json,null,AVAILABLE,@Spark}
2019-09-12 09:26:09,787  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2c8662ac{/storage,null,AVAILABLE,@Spark}
2019-09-12 09:26:09,788  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3724b43e{/storage/json,null,AVAILABLE,@Spark}
2019-09-12 09:26:09,789  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@68e7c8c3{/storage/rdd,null,AVAILABLE,@Spark}
2019-09-12 09:26:09,790  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@238bfd6c{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-09-12 09:26:09,791  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@58860997{/environment,null,AVAILABLE,@Spark}
2019-09-12 09:26:09,793  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7487b142{/environment/json,null,AVAILABLE,@Spark}
2019-09-12 09:26:09,794  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@199bc830{/executors,null,AVAILABLE,@Spark}
2019-09-12 09:26:09,802  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@27b45ea{/executors/json,null,AVAILABLE,@Spark}
2019-09-12 09:26:09,803  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@790a251b{/executors/threadDump,null,AVAILABLE,@Spark}
2019-09-12 09:26:09,812  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@150ede8b{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-09-12 09:26:09,828  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3e15bb06{/static,null,AVAILABLE,@Spark}
2019-09-12 09:26:09,829  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@119c745c{/,null,AVAILABLE,@Spark}
2019-09-12 09:26:09,832  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3b1ed14b{/api,null,AVAILABLE,@Spark}
2019-09-12 09:26:09,833  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1debc91c{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-09-12 09:26:09,834  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@181e72d3{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-09-12 09:26:09,839  INFO org.apache.spark.ui.SparkUI 54 - Bound SparkUI to 0.0.0.0, and started at http://10.6.1.14:4042
2019-09-12 09:26:10,069  INFO org.apache.spark.executor.Executor 54 - Starting executor ID driver on host localhost
2019-09-12 09:26:10,116  INFO org.apache.spark.util.Utils 54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 3382.
2019-09-12 09:26:10,116  INFO org.apache.spark.network.netty.NettyBlockTransferService 54 - Server created on 10.6.1.14:3382
2019-09-12 09:26:10,132  INFO org.apache.spark.storage.BlockManager 54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-09-12 09:26:10,132  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registering BlockManager BlockManagerId(driver, 10.6.1.14, 3382, None)
2019-09-12 09:26:10,132  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Registering block manager 10.6.1.14:3382 with 899.7 MB RAM, BlockManagerId(driver, 10.6.1.14, 3382, None)
2019-09-12 09:26:10,147  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registered BlockManager BlockManagerId(driver, 10.6.1.14, 3382, None)
2019-09-12 09:26:10,147  INFO org.apache.spark.storage.BlockManager 54 - Initialized BlockManager: BlockManagerId(driver, 10.6.1.14, 3382, None)
2019-09-12 09:26:10,540  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@48904d5a{/metrics/json,null,AVAILABLE,@Spark}
2019-09-12 09:26:10,723  INFO org.apache.spark.sql.internal.SharedState 54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/MyProject/splitCHword/spark-warehouse/').
2019-09-12 09:26:10,724  INFO org.apache.spark.sql.internal.SharedState 54 - Warehouse path is 'file:/D:/MyProject/splitCHword/spark-warehouse/'.
2019-09-12 09:26:10,740  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6a12c7a8{/SQL,null,AVAILABLE,@Spark}
2019-09-12 09:26:10,742  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@436bd4df{/SQL/json,null,AVAILABLE,@Spark}
2019-09-12 09:26:10,743  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@67f77f6e{/SQL/execution,null,AVAILABLE,@Spark}
2019-09-12 09:26:10,748  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7be1ce6a{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-09-12 09:26:10,762  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2ddb3ae8{/static/sql,null,AVAILABLE,@Spark}
2019-09-12 09:26:12,177  INFO org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef 54 - Registered StateStoreCoordinator endpoint
2019-09-12 09:26:12,762  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0 stored as values in memory (estimated size 219.8 KB, free 899.5 MB)
2019-09-12 09:26:12,947  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 09:26:12,963  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_0_piece0 in memory on 10.6.1.14:3382 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 09:26:12,999  INFO org.apache.spark.SparkContext 54 - Created broadcast 0 from load at SparkUtil.java:28
2019-09-12 09:26:13,659  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 09:26:13,662  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 09:26:13,693  INFO org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 413 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 326
2019-09-12 09:26:13,821  INFO org.apache.spark.SparkContext 54 - Starting job: load at SparkUtil.java:28
2019-09-12 09:26:13,862  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 0 (load at SparkUtil.java:28) with 1 output partitions
2019-09-12 09:26:13,863  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 0 (load at SparkUtil.java:28)
2019-09-12 09:26:13,864  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 09:26:13,869  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 09:26:13,885  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:28), which has no missing parents
2019-09-12 09:26:13,945  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1 stored as values in memory (estimated size 5.5 KB, free 899.5 MB)
2019-09-12 09:26:13,959  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 899.5 MB)
2019-09-12 09:26:13,962  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_1_piece0 in memory on 10.6.1.14:3382 (size: 3.4 KB, free: 899.7 MB)
2019-09-12 09:26:13,963  INFO org.apache.spark.SparkContext 54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-09-12 09:26:14,000  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:28) (first 15 tasks are for partitions Vector(0))
2019-09-12 09:26:14,002  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 0.0 with 1 tasks
2019-09-12 09:26:14,111  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes)
2019-09-12 09:26:14,136  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 0.0 (TID 0)
2019-09-12 09:26:14,242  INFO org.apache.spark.rdd.BinaryFileRDD 54 - Input split: Paths:/D:/MyProject/splitCHword/target/classes/test.csv:0+326
2019-09-12 09:26:14,600  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 0.0 (TID 0). 853 bytes result sent to driver
2019-09-12 09:26:14,614  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 0.0 (TID 0) in 539 ms on localhost (executor driver) (1/1)
2019-09-12 09:26:14,619  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-09-12 09:26:14,627  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 0 (load at SparkUtil.java:28) finished in 0.587 s
2019-09-12 09:26:14,641  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 0 finished: load at SparkUtil.java:28, took 0.818676 s
2019-09-12 09:26:15,750  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_1_piece0 on 10.6.1.14:3382 in memory (size: 3.4 KB, free: 899.7 MB)
2019-09-12 09:26:17,940  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_0_piece0 on 10.6.1.14:3382 in memory (size: 20.7 KB, free: 899.7 MB)
2019-09-12 09:26:18,239  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Pruning directories with: 
2019-09-12 09:26:18,245  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Post-Scan Filters: 
2019-09-12 09:26:18,254  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Output Data Schema: struct<feature: string>
2019-09-12 09:26:18,273  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Pushed Filters: 
2019-09-12 09:26:19,016  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 405.426001 ms
2019-09-12 09:26:19,036  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2 stored as values in memory (estimated size 220.5 KB, free 899.5 MB)
2019-09-12 09:26:19,070  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 09:26:19,076  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_2_piece0 in memory on 10.6.1.14:3382 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 09:26:19,077  INFO org.apache.spark.SparkContext 54 - Created broadcast 2 from show at SparkUtil.java:40
2019-09-12 09:26:19,098  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2019-09-12 09:26:19,161  INFO org.apache.spark.SparkContext 54 - Starting job: show at SparkUtil.java:40
2019-09-12 09:26:19,161  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 1 (show at SparkUtil.java:40) with 1 output partitions
2019-09-12 09:26:19,161  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 1 (show at SparkUtil.java:40)
2019-09-12 09:26:19,161  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 09:26:19,161  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 09:26:19,161  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:40), which has no missing parents
2019-09-12 09:26:19,192  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3 stored as values in memory (estimated size 14.2 KB, free 899.5 MB)
2019-09-12 09:26:19,192  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.9 KB, free 899.4 MB)
2019-09-12 09:26:19,192  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_3_piece0 in memory on 10.6.1.14:3382 (size: 6.9 KB, free: 899.7 MB)
2019-09-12 09:26:19,192  INFO org.apache.spark.SparkContext 54 - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2019-09-12 09:26:19,192  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:40) (first 15 tasks are for partitions Vector(0))
2019-09-12 09:26:19,192  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 1.0 with 1 tasks
2019-09-12 09:26:19,208  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5293 bytes)
2019-09-12 09:26:19,208  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 1.0 (TID 1)
2019-09-12 09:26:19,232  INFO org.apache.spark.sql.execution.datasources.FileScanRDD 54 - Reading File path: file:///D:/MyProject/splitCHword/target/classes/test.csv, range: 0-326, partition values: [empty row]
2019-09-12 09:26:19,278  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 36.799115 ms
2019-09-12 09:26:19,608  INFO org.ansj.util.MyStaticValue 106 - init version.number to env value is : 2.11.8
2019-09-12 09:26:19,608  INFO org.ansj.util.MyStaticValue 106 - init copyright.string to env value is : Copyright 2002-2016, LAMP/EPFL
2019-09-12 09:26:19,608  INFO org.ansj.util.MyStaticValue 106 - init osgi.version.number to env value is : 2.11.8.v20160304-115712-1706a37eb8
2019-09-12 09:26:19,608  INFO org.ansj.util.MyStaticValue 106 - init maven.version.number to env value is : 2.11.8
2019-09-12 09:26:19,624  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/ambiguity.dic
2019-09-12 09:26:19,624 ERROR org.ansj.library.AmbiguityLibrary 117 - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:D:\MyProject\splitCHword\library\ambiguity.dic not found or can not to read, path: library/ambiguity.dic
2019-09-12 09:26:19,624  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/default.dic
2019-09-12 09:26:19,624 ERROR org.ansj.library.DicLibrary 221 - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:D:\MyProject\splitCHword\library\default.dic not found or can not to read, path: library/default.dic
2019-09-12 09:26:21,022  INFO org.ansj.library.DATDictionary 90 - init core library ok use time : 1247
2019-09-12 09:26:21,728  INFO org.ansj.library.NgramLibrary 17 - init ngram ok use time :641
2019-09-12 09:26:21,764  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 1.0 (TID 1). 1343 bytes result sent to driver
2019-09-12 09:26:21,771  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 1.0 (TID 1) in 2579 ms on localhost (executor driver) (1/1)
2019-09-12 09:26:21,772  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 1 (show at SparkUtil.java:40) finished in 2.580 s
2019-09-12 09:26:21,775  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 1 finished: show at SparkUtil.java:40, took 2.611537 s
2019-09-12 09:26:21,778  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-09-12 09:26:21,811  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 16.431346 ms
2019-09-12 09:26:21,855  INFO org.apache.spark.SparkContext 54 - Invoking stop() from shutdown hook
2019-09-12 09:26:21,865  INFO org.spark_project.jetty.server.AbstractConnector 310 - Stopped Spark@729b55b3{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 09:26:21,872  INFO org.apache.spark.ui.SparkUI 54 - Stopped Spark web UI at http://10.6.1.14:4042
2019-09-12 09:26:21,898  INFO org.apache.spark.MapOutputTrackerMasterEndpoint 54 - MapOutputTrackerMasterEndpoint stopped!
2019-09-12 09:26:21,957  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore cleared
2019-09-12 09:26:21,959  INFO org.apache.spark.storage.BlockManager 54 - BlockManager stopped
2019-09-12 09:26:21,965  INFO org.apache.spark.storage.BlockManagerMaster 54 - BlockManagerMaster stopped
2019-09-12 09:26:21,971  INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint 54 - OutputCommitCoordinator stopped!
2019-09-12 09:26:21,979  INFO org.apache.spark.SparkContext 54 - Successfully stopped SparkContext
2019-09-12 09:26:21,981  INFO org.apache.spark.util.ShutdownHookManager 54 - Shutdown hook called
2019-09-12 09:26:21,985  INFO org.apache.spark.util.ShutdownHookManager 54 - Deleting directory C:\Users\OUY\AppData\Local\Temp\spark-dc68eb1f-c7cf-406a-abe3-2486bf42e3a1
2019-09-12 09:38:11,452  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-12 09:38:12,093  WARN org.apache.hadoop.util.NativeCodeLoader 62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-12 09:38:12,196 ERROR org.apache.hadoop.util.Shell 396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2430)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
	at com.datamodel.SparkUtil.CreateSession(SparkUtil.java:19)
	at com.datamodel.SparkUtil.readDataFromCsv(SparkUtil.java:24)
	at com.datamodel.SparkUtil.main(SparkUtil.java:37)
2019-09-12 09:38:12,406  INFO org.apache.spark.SparkContext 54 - Submitted application: splitCHWord
2019-09-12 09:38:12,474  INFO org.apache.spark.SecurityManager 54 - Changing view acls to: OUY
2019-09-12 09:38:12,475  INFO org.apache.spark.SecurityManager 54 - Changing modify acls to: OUY
2019-09-12 09:38:12,477  INFO org.apache.spark.SecurityManager 54 - Changing view acls groups to: 
2019-09-12 09:38:12,479  INFO org.apache.spark.SecurityManager 54 - Changing modify acls groups to: 
2019-09-12 09:38:12,480  INFO org.apache.spark.SecurityManager 54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(OUY); groups with view permissions: Set(); users  with modify permissions: Set(OUY); groups with modify permissions: Set()
2019-09-12 09:38:13,435  INFO org.apache.spark.util.Utils 54 - Successfully started service 'sparkDriver' on port 3564.
2019-09-12 09:38:13,473  INFO org.apache.spark.SparkEnv 54 - Registering MapOutputTracker
2019-09-12 09:38:13,511  INFO org.apache.spark.SparkEnv 54 - Registering BlockManagerMaster
2019-09-12 09:38:13,518  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-09-12 09:38:13,519  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - BlockManagerMasterEndpoint up
2019-09-12 09:38:13,536  INFO org.apache.spark.storage.DiskBlockManager 54 - Created local directory at C:\Users\OUY\AppData\Local\Temp\blockmgr-92567376-39d7-45b3-a8ae-3e5110bde3e4
2019-09-12 09:38:13,579  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore started with capacity 899.7 MB
2019-09-12 09:38:13,692  INFO org.apache.spark.SparkEnv 54 - Registering OutputCommitCoordinator
2019-09-12 09:38:13,817  INFO org.spark_project.jetty.util.log 192 - Logging initialized @5033ms
2019-09-12 09:38:13,903  INFO org.spark_project.jetty.server.Server 345 - jetty-9.3.z-SNAPSHOT
2019-09-12 09:38:13,920  INFO org.spark_project.jetty.server.Server 403 - Started @5137ms
2019-09-12 09:38:13,944  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2019-09-12 09:38:13,948  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
2019-09-12 09:38:13,959  INFO org.spark_project.jetty.server.AbstractConnector 270 - Started ServerConnector@cbc8d0f{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 09:38:13,959  INFO org.apache.spark.util.Utils 54 - Successfully started service 'SparkUI' on port 4042.
2019-09-12 09:38:14,007  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@59e43e8c{/jobs,null,AVAILABLE,@Spark}
2019-09-12 09:38:14,008  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5d5160e6{/jobs/json,null,AVAILABLE,@Spark}
2019-09-12 09:38:14,011  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2903c6ff{/jobs/job,null,AVAILABLE,@Spark}
2019-09-12 09:38:14,013  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@408e96d9{/jobs/job/json,null,AVAILABLE,@Spark}
2019-09-12 09:38:14,014  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@168cd36b{/stages,null,AVAILABLE,@Spark}
2019-09-12 09:38:14,015  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3901f6af{/stages/json,null,AVAILABLE,@Spark}
2019-09-12 09:38:14,016  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@10cd6753{/stages/stage,null,AVAILABLE,@Spark}
2019-09-12 09:38:14,018  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2c8662ac{/stages/stage/json,null,AVAILABLE,@Spark}
2019-09-12 09:38:14,019  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3724b43e{/stages/pool,null,AVAILABLE,@Spark}
2019-09-12 09:38:14,020  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@68e7c8c3{/stages/pool/json,null,AVAILABLE,@Spark}
2019-09-12 09:38:14,022  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@238bfd6c{/storage,null,AVAILABLE,@Spark}
2019-09-12 09:38:14,023  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@58860997{/storage/json,null,AVAILABLE,@Spark}
2019-09-12 09:38:14,024  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7487b142{/storage/rdd,null,AVAILABLE,@Spark}
2019-09-12 09:38:14,025  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@199bc830{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-09-12 09:38:14,027  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@27b45ea{/environment,null,AVAILABLE,@Spark}
2019-09-12 09:38:14,028  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@790a251b{/environment/json,null,AVAILABLE,@Spark}
2019-09-12 09:38:14,029  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@150ede8b{/executors,null,AVAILABLE,@Spark}
2019-09-12 09:38:14,038  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3e15bb06{/executors/json,null,AVAILABLE,@Spark}
2019-09-12 09:38:14,041  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4e1ce44{/executors/threadDump,null,AVAILABLE,@Spark}
2019-09-12 09:38:14,043  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7a7cc52c{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-09-12 09:38:14,059  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@524a2ffb{/static,null,AVAILABLE,@Spark}
2019-09-12 09:38:14,060  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@29eda4f8{/,null,AVAILABLE,@Spark}
2019-09-12 09:38:14,063  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5e048149{/api,null,AVAILABLE,@Spark}
2019-09-12 09:38:14,064  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@56913163{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-09-12 09:38:14,065  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@a18649a{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-09-12 09:38:14,069  INFO org.apache.spark.ui.SparkUI 54 - Bound SparkUI to 0.0.0.0, and started at http://10.6.1.14:4042
2019-09-12 09:38:14,294  INFO org.apache.spark.executor.Executor 54 - Starting executor ID driver on host localhost
2019-09-12 09:38:14,342  INFO org.apache.spark.util.Utils 54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 3577.
2019-09-12 09:38:14,343  INFO org.apache.spark.network.netty.NettyBlockTransferService 54 - Server created on 10.6.1.14:3577
2019-09-12 09:38:14,346  INFO org.apache.spark.storage.BlockManager 54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-09-12 09:38:14,349  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registering BlockManager BlockManagerId(driver, 10.6.1.14, 3577, None)
2019-09-12 09:38:14,354  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Registering block manager 10.6.1.14:3577 with 899.7 MB RAM, BlockManagerId(driver, 10.6.1.14, 3577, None)
2019-09-12 09:38:14,362  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registered BlockManager BlockManagerId(driver, 10.6.1.14, 3577, None)
2019-09-12 09:38:14,363  INFO org.apache.spark.storage.BlockManager 54 - Initialized BlockManager: BlockManagerId(driver, 10.6.1.14, 3577, None)
2019-09-12 09:38:14,671  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@708f018e{/metrics/json,null,AVAILABLE,@Spark}
2019-09-12 09:38:14,788  INFO org.apache.spark.sql.internal.SharedState 54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/MyProject/splitCHword/spark-warehouse/').
2019-09-12 09:38:14,789  INFO org.apache.spark.sql.internal.SharedState 54 - Warehouse path is 'file:/D:/MyProject/splitCHword/spark-warehouse/'.
2019-09-12 09:38:14,799  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@766a49c7{/SQL,null,AVAILABLE,@Spark}
2019-09-12 09:38:14,800  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@75e27856{/SQL/json,null,AVAILABLE,@Spark}
2019-09-12 09:38:14,801  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1f3b992{/SQL/execution,null,AVAILABLE,@Spark}
2019-09-12 09:38:14,802  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6b63e6ad{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-09-12 09:38:14,806  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@bbd4791{/static/sql,null,AVAILABLE,@Spark}
2019-09-12 09:38:16,127  INFO org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef 54 - Registered StateStoreCoordinator endpoint
2019-09-12 09:38:16,573  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0 stored as values in memory (estimated size 219.8 KB, free 899.5 MB)
2019-09-12 09:38:16,702  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 09:38:16,709  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_0_piece0 in memory on 10.6.1.14:3577 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 09:38:16,717  INFO org.apache.spark.SparkContext 54 - Created broadcast 0 from load at SparkUtil.java:28
2019-09-12 09:38:17,202  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 09:38:17,204  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 09:38:17,223  INFO org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 413 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 326
2019-09-12 09:38:17,284  INFO org.apache.spark.SparkContext 54 - Starting job: load at SparkUtil.java:28
2019-09-12 09:38:17,310  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 0 (load at SparkUtil.java:28) with 1 output partitions
2019-09-12 09:38:17,310  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 0 (load at SparkUtil.java:28)
2019-09-12 09:38:17,311  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 09:38:17,313  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 09:38:17,323  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:28), which has no missing parents
2019-09-12 09:38:17,352  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1 stored as values in memory (estimated size 5.5 KB, free 899.5 MB)
2019-09-12 09:38:17,358  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 899.5 MB)
2019-09-12 09:38:17,363  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_1_piece0 in memory on 10.6.1.14:3577 (size: 3.4 KB, free: 899.7 MB)
2019-09-12 09:38:17,367  INFO org.apache.spark.SparkContext 54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-09-12 09:38:17,401  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:28) (first 15 tasks are for partitions Vector(0))
2019-09-12 09:38:17,403  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 0.0 with 1 tasks
2019-09-12 09:38:17,492  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes)
2019-09-12 09:38:17,512  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 0.0 (TID 0)
2019-09-12 09:38:17,615  INFO org.apache.spark.rdd.BinaryFileRDD 54 - Input split: Paths:/D:/MyProject/splitCHword/target/classes/test.csv:0+326
2019-09-12 09:38:17,870  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 0.0 (TID 0). 896 bytes result sent to driver
2019-09-12 09:38:17,882  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 0.0 (TID 0) in 416 ms on localhost (executor driver) (1/1)
2019-09-12 09:38:17,887  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-09-12 09:38:17,896  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 0 (load at SparkUtil.java:28) finished in 0.455 s
2019-09-12 09:38:17,909  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 0 finished: load at SparkUtil.java:28, took 0.624531 s
2019-09-12 09:38:20,121  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_1_piece0 on 10.6.1.14:3577 in memory (size: 3.4 KB, free: 899.7 MB)
2019-09-12 09:38:20,132  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_0_piece0 on 10.6.1.14:3577 in memory (size: 20.7 KB, free: 899.7 MB)
2019-09-12 09:38:20,844  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Pruning directories with: 
2019-09-12 09:38:20,848  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Post-Scan Filters: 
2019-09-12 09:38:20,853  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Output Data Schema: struct<feature: string>
2019-09-12 09:38:20,870  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Pushed Filters: 
2019-09-12 09:38:21,550  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 332.148197 ms
2019-09-12 09:38:21,559  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2 stored as values in memory (estimated size 220.5 KB, free 899.5 MB)
2019-09-12 09:38:21,577  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.6 KB, free 899.5 MB)
2019-09-12 09:38:21,580  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_2_piece0 in memory on 10.6.1.14:3577 (size: 20.6 KB, free: 899.7 MB)
2019-09-12 09:38:21,582  INFO org.apache.spark.SparkContext 54 - Created broadcast 2 from show at SparkUtil.java:41
2019-09-12 09:38:21,600  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2019-09-12 09:38:21,667  INFO org.apache.spark.SparkContext 54 - Starting job: show at SparkUtil.java:41
2019-09-12 09:38:21,668  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 1 (show at SparkUtil.java:41) with 1 output partitions
2019-09-12 09:38:21,668  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 1 (show at SparkUtil.java:41)
2019-09-12 09:38:21,668  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 09:38:21,669  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 09:38:21,669  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:41), which has no missing parents
2019-09-12 09:38:21,696  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3 stored as values in memory (estimated size 14.2 KB, free 899.5 MB)
2019-09-12 09:38:21,701  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.9 KB, free 899.4 MB)
2019-09-12 09:38:21,702  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_3_piece0 in memory on 10.6.1.14:3577 (size: 6.9 KB, free: 899.7 MB)
2019-09-12 09:38:21,703  INFO org.apache.spark.SparkContext 54 - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2019-09-12 09:38:21,703  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:41) (first 15 tasks are for partitions Vector(0))
2019-09-12 09:38:21,704  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 1.0 with 1 tasks
2019-09-12 09:38:21,714  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5293 bytes)
2019-09-12 09:38:21,714  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 1.0 (TID 1)
2019-09-12 09:38:21,736  INFO org.apache.spark.sql.execution.datasources.FileScanRDD 54 - Reading File path: file:///D:/MyProject/splitCHword/target/classes/test.csv, range: 0-326, partition values: [empty row]
2019-09-12 09:38:21,753  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 12.18303 ms
2019-09-12 09:38:22,026  INFO org.ansj.util.MyStaticValue 106 - init version.number to env value is : 2.11.8
2019-09-12 09:38:22,026  INFO org.ansj.util.MyStaticValue 106 - init copyright.string to env value is : Copyright 2002-2016, LAMP/EPFL
2019-09-12 09:38:22,026  INFO org.ansj.util.MyStaticValue 106 - init osgi.version.number to env value is : 2.11.8.v20160304-115712-1706a37eb8
2019-09-12 09:38:22,026  INFO org.ansj.util.MyStaticValue 106 - init maven.version.number to env value is : 2.11.8
2019-09-12 09:38:22,032  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/ambiguity.dic
2019-09-12 09:38:22,032 ERROR org.ansj.library.AmbiguityLibrary 117 - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:D:\MyProject\splitCHword\library\ambiguity.dic not found or can not to read, path: library/ambiguity.dic
2019-09-12 09:38:22,034  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/default.dic
2019-09-12 09:38:22,035 ERROR org.ansj.library.DicLibrary 221 - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:D:\MyProject\splitCHword\library\default.dic not found or can not to read, path: library/default.dic
2019-09-12 09:38:23,220  INFO org.ansj.library.DATDictionary 90 - init core library ok use time : 1057
2019-09-12 09:38:23,640  INFO org.ansj.library.NgramLibrary 17 - init ngram ok use time :408
2019-09-12 09:38:23,654  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 1.0 (TID 1). 1429 bytes result sent to driver
2019-09-12 09:38:23,657  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 1.0 (TID 1) in 1953 ms on localhost (executor driver) (1/1)
2019-09-12 09:38:23,658  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 1 (show at SparkUtil.java:41) finished in 1.954 s
2019-09-12 09:38:23,662  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 1 finished: show at SparkUtil.java:41, took 1.993926 s
2019-09-12 09:38:23,665  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-09-12 09:38:23,684  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 9.898231 ms
2019-09-12 09:38:23,708  INFO org.apache.spark.SparkContext 54 - Invoking stop() from shutdown hook
2019-09-12 09:38:23,717  INFO org.spark_project.jetty.server.AbstractConnector 310 - Stopped Spark@cbc8d0f{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 09:38:23,721  INFO org.apache.spark.ui.SparkUI 54 - Stopped Spark web UI at http://10.6.1.14:4042
2019-09-12 09:38:23,739  INFO org.apache.spark.MapOutputTrackerMasterEndpoint 54 - MapOutputTrackerMasterEndpoint stopped!
2019-09-12 09:38:23,774  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore cleared
2019-09-12 09:38:23,775  INFO org.apache.spark.storage.BlockManager 54 - BlockManager stopped
2019-09-12 09:38:23,777  INFO org.apache.spark.storage.BlockManagerMaster 54 - BlockManagerMaster stopped
2019-09-12 09:38:23,784  INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint 54 - OutputCommitCoordinator stopped!
2019-09-12 09:38:23,790  INFO org.apache.spark.SparkContext 54 - Successfully stopped SparkContext
2019-09-12 09:38:23,790  INFO org.apache.spark.util.ShutdownHookManager 54 - Shutdown hook called
2019-09-12 09:38:23,792  INFO org.apache.spark.util.ShutdownHookManager 54 - Deleting directory C:\Users\OUY\AppData\Local\Temp\spark-b53f509f-0c5b-41dc-942b-466000bcc997
2019-09-12 09:40:50,182  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-12 09:40:50,934  WARN org.apache.hadoop.util.NativeCodeLoader 62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-12 09:40:51,108 ERROR org.apache.hadoop.util.Shell 396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2430)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
	at com.datamodel.SparkUtil.CreateSession(SparkUtil.java:19)
	at com.datamodel.SparkUtil.readDataFromCsv(SparkUtil.java:24)
	at com.datamodel.SparkUtil.main(SparkUtil.java:37)
2019-09-12 09:40:51,277  INFO org.apache.spark.SparkContext 54 - Submitted application: splitCHWord
2019-09-12 09:40:51,308  INFO org.apache.spark.SecurityManager 54 - Changing view acls to: OUY
2019-09-12 09:40:51,308  INFO org.apache.spark.SecurityManager 54 - Changing modify acls to: OUY
2019-09-12 09:40:51,309  INFO org.apache.spark.SecurityManager 54 - Changing view acls groups to: 
2019-09-12 09:40:51,310  INFO org.apache.spark.SecurityManager 54 - Changing modify acls groups to: 
2019-09-12 09:40:51,311  INFO org.apache.spark.SecurityManager 54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(OUY); groups with view permissions: Set(); users  with modify permissions: Set(OUY); groups with modify permissions: Set()
2019-09-12 09:40:52,061  INFO org.apache.spark.util.Utils 54 - Successfully started service 'sparkDriver' on port 3612.
2019-09-12 09:40:52,122  INFO org.apache.spark.SparkEnv 54 - Registering MapOutputTracker
2019-09-12 09:40:52,143  INFO org.apache.spark.SparkEnv 54 - Registering BlockManagerMaster
2019-09-12 09:40:52,147  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-09-12 09:40:52,148  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - BlockManagerMasterEndpoint up
2019-09-12 09:40:52,159  INFO org.apache.spark.storage.DiskBlockManager 54 - Created local directory at C:\Users\OUY\AppData\Local\Temp\blockmgr-e428e401-44af-4e94-aa37-ce6efe890cfc
2019-09-12 09:40:52,184  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore started with capacity 899.7 MB
2019-09-12 09:40:52,249  INFO org.apache.spark.SparkEnv 54 - Registering OutputCommitCoordinator
2019-09-12 09:40:52,380  INFO org.spark_project.jetty.util.log 192 - Logging initialized @4333ms
2019-09-12 09:40:52,451  INFO org.spark_project.jetty.server.Server 345 - jetty-9.3.z-SNAPSHOT
2019-09-12 09:40:52,467  INFO org.spark_project.jetty.server.Server 403 - Started @4421ms
2019-09-12 09:40:52,482  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2019-09-12 09:40:52,483  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
2019-09-12 09:40:52,492  INFO org.spark_project.jetty.server.AbstractConnector 270 - Started ServerConnector@191265ad{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 09:40:52,492  INFO org.apache.spark.util.Utils 54 - Successfully started service 'SparkUI' on port 4042.
2019-09-12 09:40:52,523  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@319988b0{/jobs,null,AVAILABLE,@Spark}
2019-09-12 09:40:52,524  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4c168660{/jobs/json,null,AVAILABLE,@Spark}
2019-09-12 09:40:52,525  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@fd0e5b6{/jobs/job,null,AVAILABLE,@Spark}
2019-09-12 09:40:52,527  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4fad94a7{/jobs/job/json,null,AVAILABLE,@Spark}
2019-09-12 09:40:52,529  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6326d182{/stages,null,AVAILABLE,@Spark}
2019-09-12 09:40:52,529  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@716a7124{/stages/json,null,AVAILABLE,@Spark}
2019-09-12 09:40:52,530  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@226642a5{/stages/stage,null,AVAILABLE,@Spark}
2019-09-12 09:40:52,532  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@72bd06ca{/stages/stage/json,null,AVAILABLE,@Spark}
2019-09-12 09:40:52,533  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5dbe30be{/stages/pool,null,AVAILABLE,@Spark}
2019-09-12 09:40:52,534  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@55795845{/stages/pool/json,null,AVAILABLE,@Spark}
2019-09-12 09:40:52,536  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@119f1f2a{/storage,null,AVAILABLE,@Spark}
2019-09-12 09:40:52,537  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5b970f7{/storage/json,null,AVAILABLE,@Spark}
2019-09-12 09:40:52,538  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@165b8a71{/storage/rdd,null,AVAILABLE,@Spark}
2019-09-12 09:40:52,539  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2f058b8a{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-09-12 09:40:52,540  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3f2ef586{/environment,null,AVAILABLE,@Spark}
2019-09-12 09:40:52,542  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@76c7beb3{/environment/json,null,AVAILABLE,@Spark}
2019-09-12 09:40:52,543  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2cf92cc7{/executors,null,AVAILABLE,@Spark}
2019-09-12 09:40:52,545  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7b139eab{/executors/json,null,AVAILABLE,@Spark}
2019-09-12 09:40:52,546  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@611df6e3{/executors/threadDump,null,AVAILABLE,@Spark}
2019-09-12 09:40:52,547  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6273c5a4{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-09-12 09:40:52,556  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@53e211ee{/static,null,AVAILABLE,@Spark}
2019-09-12 09:40:52,557  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4a9f80d3{/,null,AVAILABLE,@Spark}
2019-09-12 09:40:52,559  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@41fe9859{/api,null,AVAILABLE,@Spark}
2019-09-12 09:40:52,559  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@53ab0286{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-09-12 09:40:52,560  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2d10e0b1{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-09-12 09:40:52,563  INFO org.apache.spark.ui.SparkUI 54 - Bound SparkUI to 0.0.0.0, and started at http://10.6.1.14:4042
2019-09-12 09:40:52,735  INFO org.apache.spark.executor.Executor 54 - Starting executor ID driver on host localhost
2019-09-12 09:40:52,788  INFO org.apache.spark.util.Utils 54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 3625.
2019-09-12 09:40:52,789  INFO org.apache.spark.network.netty.NettyBlockTransferService 54 - Server created on 10.6.1.14:3625
2019-09-12 09:40:52,791  INFO org.apache.spark.storage.BlockManager 54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-09-12 09:40:52,801  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registering BlockManager BlockManagerId(driver, 10.6.1.14, 3625, None)
2019-09-12 09:40:52,805  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Registering block manager 10.6.1.14:3625 with 899.7 MB RAM, BlockManagerId(driver, 10.6.1.14, 3625, None)
2019-09-12 09:40:52,813  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registered BlockManager BlockManagerId(driver, 10.6.1.14, 3625, None)
2019-09-12 09:40:52,814  INFO org.apache.spark.storage.BlockManager 54 - Initialized BlockManager: BlockManagerId(driver, 10.6.1.14, 3625, None)
2019-09-12 09:40:53,236  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6c2f1700{/metrics/json,null,AVAILABLE,@Spark}
2019-09-12 09:40:53,331  INFO org.apache.spark.sql.internal.SharedState 54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/MyProject/splitCHword/spark-warehouse/').
2019-09-12 09:40:53,331  INFO org.apache.spark.sql.internal.SharedState 54 - Warehouse path is 'file:/D:/MyProject/splitCHword/spark-warehouse/'.
2019-09-12 09:40:53,339  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@36a7abe1{/SQL,null,AVAILABLE,@Spark}
2019-09-12 09:40:53,340  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@e044b4a{/SQL/json,null,AVAILABLE,@Spark}
2019-09-12 09:40:53,342  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4604b900{/SQL/execution,null,AVAILABLE,@Spark}
2019-09-12 09:40:53,342  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@e36bb2a{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-09-12 09:40:53,345  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1500e009{/static/sql,null,AVAILABLE,@Spark}
2019-09-12 09:40:54,398  INFO org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef 54 - Registered StateStoreCoordinator endpoint
2019-09-12 09:40:54,730  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0 stored as values in memory (estimated size 219.8 KB, free 899.5 MB)
2019-09-12 09:40:54,867  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 09:40:54,872  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_0_piece0 in memory on 10.6.1.14:3625 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 09:40:54,877  INFO org.apache.spark.SparkContext 54 - Created broadcast 0 from load at SparkUtil.java:28
2019-09-12 09:40:55,241  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 09:40:55,243  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 09:40:55,259  INFO org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 413 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 326
2019-09-12 09:40:55,325  INFO org.apache.spark.SparkContext 54 - Starting job: load at SparkUtil.java:28
2019-09-12 09:40:55,344  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 0 (load at SparkUtil.java:28) with 1 output partitions
2019-09-12 09:40:55,344  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 0 (load at SparkUtil.java:28)
2019-09-12 09:40:55,345  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 09:40:55,346  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 09:40:55,352  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:28), which has no missing parents
2019-09-12 09:40:55,371  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1 stored as values in memory (estimated size 5.5 KB, free 899.5 MB)
2019-09-12 09:40:55,378  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 899.5 MB)
2019-09-12 09:40:55,380  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_1_piece0 in memory on 10.6.1.14:3625 (size: 3.4 KB, free: 899.7 MB)
2019-09-12 09:40:55,382  INFO org.apache.spark.SparkContext 54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-09-12 09:40:55,410  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:28) (first 15 tasks are for partitions Vector(0))
2019-09-12 09:40:55,412  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 0.0 with 1 tasks
2019-09-12 09:40:55,472  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes)
2019-09-12 09:40:55,484  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 0.0 (TID 0)
2019-09-12 09:40:55,581  INFO org.apache.spark.rdd.BinaryFileRDD 54 - Input split: Paths:/D:/MyProject/splitCHword/target/classes/test.csv:0+326
2019-09-12 09:40:55,763  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 0.0 (TID 0). 853 bytes result sent to driver
2019-09-12 09:40:55,771  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 0.0 (TID 0) in 317 ms on localhost (executor driver) (1/1)
2019-09-12 09:40:55,773  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-09-12 09:40:55,778  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 0 (load at SparkUtil.java:28) finished in 0.346 s
2019-09-12 09:40:55,785  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 0 finished: load at SparkUtil.java:28, took 0.459464 s
2019-09-12 09:40:56,248  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_1_piece0 on 10.6.1.14:3625 in memory (size: 3.4 KB, free: 899.7 MB)
2019-09-12 09:40:58,113  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_0_piece0 on 10.6.1.14:3625 in memory (size: 20.7 KB, free: 899.7 MB)
2019-09-12 09:40:58,187  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Pruning directories with: 
2019-09-12 09:40:58,190  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Post-Scan Filters: 
2019-09-12 09:40:58,194  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Output Data Schema: struct<feature: string>
2019-09-12 09:40:58,205  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Pushed Filters: 
2019-09-12 09:40:58,736  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 274.322539 ms
2019-09-12 09:40:58,751  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2 stored as values in memory (estimated size 220.5 KB, free 899.5 MB)
2019-09-12 09:40:58,771  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.6 KB, free 899.5 MB)
2019-09-12 09:40:58,774  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_2_piece0 in memory on 10.6.1.14:3625 (size: 20.6 KB, free: 899.7 MB)
2019-09-12 09:40:58,776  INFO org.apache.spark.SparkContext 54 - Created broadcast 2 from show at SparkUtil.java:41
2019-09-12 09:40:58,795  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2019-09-12 09:40:58,850  INFO org.apache.spark.SparkContext 54 - Starting job: show at SparkUtil.java:41
2019-09-12 09:40:58,852  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 1 (show at SparkUtil.java:41) with 1 output partitions
2019-09-12 09:40:58,852  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 1 (show at SparkUtil.java:41)
2019-09-12 09:40:58,852  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 09:40:58,852  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 09:40:58,855  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:41), which has no missing parents
2019-09-12 09:40:58,888  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3 stored as values in memory (estimated size 14.7 KB, free 899.5 MB)
2019-09-12 09:40:58,894  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.0 KB, free 899.4 MB)
2019-09-12 09:40:58,895  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_3_piece0 in memory on 10.6.1.14:3625 (size: 7.0 KB, free: 899.7 MB)
2019-09-12 09:40:58,896  INFO org.apache.spark.SparkContext 54 - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2019-09-12 09:40:58,896  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:41) (first 15 tasks are for partitions Vector(0))
2019-09-12 09:40:58,897  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 1.0 with 1 tasks
2019-09-12 09:40:58,904  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5293 bytes)
2019-09-12 09:40:58,904  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 1.0 (TID 1)
2019-09-12 09:40:58,924  INFO org.apache.spark.sql.execution.datasources.FileScanRDD 54 - Reading File path: file:///D:/MyProject/splitCHword/target/classes/test.csv, range: 0-326, partition values: [empty row]
2019-09-12 09:40:58,938  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 10.663244 ms
2019-09-12 09:40:59,194  INFO org.ansj.util.MyStaticValue 106 - init version.number to env value is : 2.11.8
2019-09-12 09:40:59,194  INFO org.ansj.util.MyStaticValue 106 - init copyright.string to env value is : Copyright 2002-2016, LAMP/EPFL
2019-09-12 09:40:59,194  INFO org.ansj.util.MyStaticValue 106 - init osgi.version.number to env value is : 2.11.8.v20160304-115712-1706a37eb8
2019-09-12 09:40:59,194  INFO org.ansj.util.MyStaticValue 106 - init maven.version.number to env value is : 2.11.8
2019-09-12 09:40:59,198  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/ambiguity.dic
2019-09-12 09:40:59,198 ERROR org.ansj.library.AmbiguityLibrary 117 - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:D:\MyProject\splitCHword\library\ambiguity.dic not found or can not to read, path: library/ambiguity.dic
2019-09-12 09:40:59,200  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/default.dic
2019-09-12 09:40:59,200 ERROR org.ansj.library.DicLibrary 221 - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:D:\MyProject\splitCHword\library\default.dic not found or can not to read, path: library/default.dic
2019-09-12 09:41:00,260  INFO org.ansj.library.DATDictionary 90 - init core library ok use time : 952
2019-09-12 09:41:00,741  INFO org.ansj.library.NgramLibrary 17 - init ngram ok use time :477
2019-09-12 09:41:00,752  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 1.0 (TID 1). 1429 bytes result sent to driver
2019-09-12 09:41:00,753  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 1.0 (TID 1) in 1856 ms on localhost (executor driver) (1/1)
2019-09-12 09:41:00,754  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-09-12 09:41:00,755  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 1 (show at SparkUtil.java:41) finished in 1.857 s
2019-09-12 09:41:00,756  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 1 finished: show at SparkUtil.java:41, took 1.905280 s
2019-09-12 09:41:00,780  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 12.247029 ms
2019-09-12 09:41:00,806  INFO org.apache.spark.SparkContext 54 - Invoking stop() from shutdown hook
2019-09-12 09:41:00,815  INFO org.spark_project.jetty.server.AbstractConnector 310 - Stopped Spark@191265ad{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 09:41:00,820  INFO org.apache.spark.ui.SparkUI 54 - Stopped Spark web UI at http://10.6.1.14:4042
2019-09-12 09:41:00,835  INFO org.apache.spark.MapOutputTrackerMasterEndpoint 54 - MapOutputTrackerMasterEndpoint stopped!
2019-09-12 09:41:00,871  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore cleared
2019-09-12 09:41:00,871  INFO org.apache.spark.storage.BlockManager 54 - BlockManager stopped
2019-09-12 09:41:00,874  INFO org.apache.spark.storage.BlockManagerMaster 54 - BlockManagerMaster stopped
2019-09-12 09:41:00,877  INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint 54 - OutputCommitCoordinator stopped!
2019-09-12 09:41:00,883  INFO org.apache.spark.SparkContext 54 - Successfully stopped SparkContext
2019-09-12 09:41:00,884  INFO org.apache.spark.util.ShutdownHookManager 54 - Shutdown hook called
2019-09-12 09:41:00,885  INFO org.apache.spark.util.ShutdownHookManager 54 - Deleting directory C:\Users\OUY\AppData\Local\Temp\spark-fad74a12-9f0a-4c52-b54f-0aaf1baf50c4
2019-09-12 09:42:01,396  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-12 09:42:01,993  WARN org.apache.hadoop.util.NativeCodeLoader 62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-12 09:42:02,075 ERROR org.apache.hadoop.util.Shell 396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2430)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
	at com.datamodel.SparkUtil.CreateSession(SparkUtil.java:19)
	at com.datamodel.SparkUtil.readDataFromCsv(SparkUtil.java:24)
	at com.datamodel.SparkUtil.main(SparkUtil.java:37)
2019-09-12 09:42:02,203  INFO org.apache.spark.SparkContext 54 - Submitted application: splitCHWord
2019-09-12 09:42:02,241  INFO org.apache.spark.SecurityManager 54 - Changing view acls to: OUY
2019-09-12 09:42:02,243  INFO org.apache.spark.SecurityManager 54 - Changing modify acls to: OUY
2019-09-12 09:42:02,244  INFO org.apache.spark.SecurityManager 54 - Changing view acls groups to: 
2019-09-12 09:42:02,245  INFO org.apache.spark.SecurityManager 54 - Changing modify acls groups to: 
2019-09-12 09:42:02,246  INFO org.apache.spark.SecurityManager 54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(OUY); groups with view permissions: Set(); users  with modify permissions: Set(OUY); groups with modify permissions: Set()
2019-09-12 09:42:03,012  INFO org.apache.spark.util.Utils 54 - Successfully started service 'sparkDriver' on port 3652.
2019-09-12 09:42:03,065  INFO org.apache.spark.SparkEnv 54 - Registering MapOutputTracker
2019-09-12 09:42:03,088  INFO org.apache.spark.SparkEnv 54 - Registering BlockManagerMaster
2019-09-12 09:42:03,093  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-09-12 09:42:03,094  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - BlockManagerMasterEndpoint up
2019-09-12 09:42:03,107  INFO org.apache.spark.storage.DiskBlockManager 54 - Created local directory at C:\Users\OUY\AppData\Local\Temp\blockmgr-7f6895d3-1ee8-49be-9133-f21d236a1ac4
2019-09-12 09:42:03,131  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore started with capacity 899.7 MB
2019-09-12 09:42:03,230  INFO org.apache.spark.SparkEnv 54 - Registering OutputCommitCoordinator
2019-09-12 09:42:03,384  INFO org.spark_project.jetty.util.log 192 - Logging initialized @4201ms
2019-09-12 09:42:03,455  INFO org.spark_project.jetty.server.Server 345 - jetty-9.3.z-SNAPSHOT
2019-09-12 09:42:03,473  INFO org.spark_project.jetty.server.Server 403 - Started @4291ms
2019-09-12 09:42:03,489  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2019-09-12 09:42:03,490  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
2019-09-12 09:42:03,498  INFO org.spark_project.jetty.server.AbstractConnector 270 - Started ServerConnector@7fa2e67e{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 09:42:03,499  INFO org.apache.spark.util.Utils 54 - Successfully started service 'SparkUI' on port 4042.
2019-09-12 09:42:03,536  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@319988b0{/jobs,null,AVAILABLE,@Spark}
2019-09-12 09:42:03,537  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4c168660{/jobs/json,null,AVAILABLE,@Spark}
2019-09-12 09:42:03,539  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@fd0e5b6{/jobs/job,null,AVAILABLE,@Spark}
2019-09-12 09:42:03,541  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4fad94a7{/jobs/job/json,null,AVAILABLE,@Spark}
2019-09-12 09:42:03,542  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6326d182{/stages,null,AVAILABLE,@Spark}
2019-09-12 09:42:03,543  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@716a7124{/stages/json,null,AVAILABLE,@Spark}
2019-09-12 09:42:03,544  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@226642a5{/stages/stage,null,AVAILABLE,@Spark}
2019-09-12 09:42:03,545  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@72bd06ca{/stages/stage/json,null,AVAILABLE,@Spark}
2019-09-12 09:42:03,547  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5dbe30be{/stages/pool,null,AVAILABLE,@Spark}
2019-09-12 09:42:03,548  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@55795845{/stages/pool/json,null,AVAILABLE,@Spark}
2019-09-12 09:42:03,549  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@119f1f2a{/storage,null,AVAILABLE,@Spark}
2019-09-12 09:42:03,550  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5b970f7{/storage/json,null,AVAILABLE,@Spark}
2019-09-12 09:42:03,551  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@165b8a71{/storage/rdd,null,AVAILABLE,@Spark}
2019-09-12 09:42:03,552  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2f058b8a{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-09-12 09:42:03,554  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3f2ef586{/environment,null,AVAILABLE,@Spark}
2019-09-12 09:42:03,555  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@76c7beb3{/environment/json,null,AVAILABLE,@Spark}
2019-09-12 09:42:03,556  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2cf92cc7{/executors,null,AVAILABLE,@Spark}
2019-09-12 09:42:03,557  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7b139eab{/executors/json,null,AVAILABLE,@Spark}
2019-09-12 09:42:03,558  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@611df6e3{/executors/threadDump,null,AVAILABLE,@Spark}
2019-09-12 09:42:03,561  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6273c5a4{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-09-12 09:42:03,571  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@53e211ee{/static,null,AVAILABLE,@Spark}
2019-09-12 09:42:03,572  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4a9f80d3{/,null,AVAILABLE,@Spark}
2019-09-12 09:42:03,574  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@41fe9859{/api,null,AVAILABLE,@Spark}
2019-09-12 09:42:03,575  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@53ab0286{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-09-12 09:42:03,576  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2d10e0b1{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-09-12 09:42:03,580  INFO org.apache.spark.ui.SparkUI 54 - Bound SparkUI to 0.0.0.0, and started at http://10.6.1.14:4042
2019-09-12 09:42:03,749  INFO org.apache.spark.executor.Executor 54 - Starting executor ID driver on host localhost
2019-09-12 09:42:03,796  INFO org.apache.spark.util.Utils 54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 3665.
2019-09-12 09:42:03,797  INFO org.apache.spark.network.netty.NettyBlockTransferService 54 - Server created on 10.6.1.14:3665
2019-09-12 09:42:03,799  INFO org.apache.spark.storage.BlockManager 54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-09-12 09:42:03,808  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registering BlockManager BlockManagerId(driver, 10.6.1.14, 3665, None)
2019-09-12 09:42:03,813  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Registering block manager 10.6.1.14:3665 with 899.7 MB RAM, BlockManagerId(driver, 10.6.1.14, 3665, None)
2019-09-12 09:42:03,817  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registered BlockManager BlockManagerId(driver, 10.6.1.14, 3665, None)
2019-09-12 09:42:03,818  INFO org.apache.spark.storage.BlockManager 54 - Initialized BlockManager: BlockManagerId(driver, 10.6.1.14, 3665, None)
2019-09-12 09:42:04,064  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6c2f1700{/metrics/json,null,AVAILABLE,@Spark}
2019-09-12 09:42:04,167  INFO org.apache.spark.sql.internal.SharedState 54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/MyProject/splitCHword/spark-warehouse/').
2019-09-12 09:42:04,169  INFO org.apache.spark.sql.internal.SharedState 54 - Warehouse path is 'file:/D:/MyProject/splitCHword/spark-warehouse/'.
2019-09-12 09:42:04,182  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@36a7abe1{/SQL,null,AVAILABLE,@Spark}
2019-09-12 09:42:04,183  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@e044b4a{/SQL/json,null,AVAILABLE,@Spark}
2019-09-12 09:42:04,185  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4604b900{/SQL/execution,null,AVAILABLE,@Spark}
2019-09-12 09:42:04,186  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@e36bb2a{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-09-12 09:42:04,190  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1500e009{/static/sql,null,AVAILABLE,@Spark}
2019-09-12 09:42:05,297  INFO org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef 54 - Registered StateStoreCoordinator endpoint
2019-09-12 09:42:05,596  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0 stored as values in memory (estimated size 219.8 KB, free 899.5 MB)
2019-09-12 09:42:05,684  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 09:42:05,689  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_0_piece0 in memory on 10.6.1.14:3665 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 09:42:05,693  INFO org.apache.spark.SparkContext 54 - Created broadcast 0 from load at SparkUtil.java:28
2019-09-12 09:42:06,052  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 09:42:06,055  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 09:42:06,075  INFO org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 413 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 326
2019-09-12 09:42:06,146  INFO org.apache.spark.SparkContext 54 - Starting job: load at SparkUtil.java:28
2019-09-12 09:42:06,179  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 0 (load at SparkUtil.java:28) with 1 output partitions
2019-09-12 09:42:06,179  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 0 (load at SparkUtil.java:28)
2019-09-12 09:42:06,180  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 09:42:06,181  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 09:42:06,186  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:28), which has no missing parents
2019-09-12 09:42:06,207  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1 stored as values in memory (estimated size 5.5 KB, free 899.5 MB)
2019-09-12 09:42:06,215  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 899.5 MB)
2019-09-12 09:42:06,216  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_1_piece0 in memory on 10.6.1.14:3665 (size: 3.4 KB, free: 899.7 MB)
2019-09-12 09:42:06,218  INFO org.apache.spark.SparkContext 54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-09-12 09:42:06,248  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:28) (first 15 tasks are for partitions Vector(0))
2019-09-12 09:42:06,249  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 0.0 with 1 tasks
2019-09-12 09:42:06,325  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes)
2019-09-12 09:42:06,341  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 0.0 (TID 0)
2019-09-12 09:42:06,435  INFO org.apache.spark.rdd.BinaryFileRDD 54 - Input split: Paths:/D:/MyProject/splitCHword/target/classes/test.csv:0+326
2019-09-12 09:42:06,619  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 0.0 (TID 0). 853 bytes result sent to driver
2019-09-12 09:42:06,627  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 0.0 (TID 0) in 322 ms on localhost (executor driver) (1/1)
2019-09-12 09:42:06,629  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-09-12 09:42:06,634  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 0 (load at SparkUtil.java:28) finished in 0.350 s
2019-09-12 09:42:06,641  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 0 finished: load at SparkUtil.java:28, took 0.494281 s
2019-09-12 09:42:07,138  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_1_piece0 on 10.6.1.14:3665 in memory (size: 3.4 KB, free: 899.7 MB)
2019-09-12 09:42:07,143  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_0_piece0 on 10.6.1.14:3665 in memory (size: 20.7 KB, free: 899.7 MB)
2019-09-12 09:42:08,997  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Pruning directories with: 
2019-09-12 09:42:09,000  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Post-Scan Filters: 
2019-09-12 09:42:09,003  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Output Data Schema: struct<feature: string>
2019-09-12 09:42:09,013  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Pushed Filters: 
2019-09-12 09:42:09,559  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 293.13383 ms
2019-09-12 09:42:09,570  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2 stored as values in memory (estimated size 220.5 KB, free 899.5 MB)
2019-09-12 09:42:09,593  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 09:42:09,597  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_2_piece0 in memory on 10.6.1.14:3665 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 09:42:09,600  INFO org.apache.spark.SparkContext 54 - Created broadcast 2 from show at SparkUtil.java:41
2019-09-12 09:42:09,616  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2019-09-12 09:42:09,670  INFO org.apache.spark.SparkContext 54 - Starting job: show at SparkUtil.java:41
2019-09-12 09:42:09,672  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 1 (show at SparkUtil.java:41) with 1 output partitions
2019-09-12 09:42:09,672  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 1 (show at SparkUtil.java:41)
2019-09-12 09:42:09,672  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 09:42:09,673  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 09:42:09,673  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:41), which has no missing parents
2019-09-12 09:42:09,697  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3 stored as values in memory (estimated size 14.7 KB, free 899.5 MB)
2019-09-12 09:42:09,701  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.0 KB, free 899.4 MB)
2019-09-12 09:42:09,702  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_3_piece0 in memory on 10.6.1.14:3665 (size: 7.0 KB, free: 899.7 MB)
2019-09-12 09:42:09,703  INFO org.apache.spark.SparkContext 54 - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2019-09-12 09:42:09,704  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:41) (first 15 tasks are for partitions Vector(0))
2019-09-12 09:42:09,704  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 1.0 with 1 tasks
2019-09-12 09:42:09,711  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5293 bytes)
2019-09-12 09:42:09,711  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 1.0 (TID 1)
2019-09-12 09:42:09,729  INFO org.apache.spark.sql.execution.datasources.FileScanRDD 54 - Reading File path: file:///D:/MyProject/splitCHword/target/classes/test.csv, range: 0-326, partition values: [empty row]
2019-09-12 09:42:09,743  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 10.470391 ms
2019-09-12 09:42:10,005  INFO org.ansj.util.MyStaticValue 106 - init version.number to env value is : 2.11.8
2019-09-12 09:42:10,005  INFO org.ansj.util.MyStaticValue 106 - init copyright.string to env value is : Copyright 2002-2016, LAMP/EPFL
2019-09-12 09:42:10,006  INFO org.ansj.util.MyStaticValue 106 - init osgi.version.number to env value is : 2.11.8.v20160304-115712-1706a37eb8
2019-09-12 09:42:10,006  INFO org.ansj.util.MyStaticValue 106 - init maven.version.number to env value is : 2.11.8
2019-09-12 09:42:10,011  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/ambiguity.dic
2019-09-12 09:42:10,012 ERROR org.ansj.library.AmbiguityLibrary 117 - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:D:\MyProject\splitCHword\library\ambiguity.dic not found or can not to read, path: library/ambiguity.dic
2019-09-12 09:42:10,014  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/default.dic
2019-09-12 09:42:10,014 ERROR org.ansj.library.DicLibrary 221 - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:D:\MyProject\splitCHword\library\default.dic not found or can not to read, path: library/default.dic
2019-09-12 09:42:11,073  INFO org.ansj.library.DATDictionary 90 - init core library ok use time : 953
2019-09-12 09:42:11,734  INFO org.ansj.library.NgramLibrary 17 - init ngram ok use time :658
2019-09-12 09:42:11,745  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 1.0 (TID 1). 1429 bytes result sent to driver
2019-09-12 09:42:11,748  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 1.0 (TID 1) in 2043 ms on localhost (executor driver) (1/1)
2019-09-12 09:42:11,748  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-09-12 09:42:11,752  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 1 (show at SparkUtil.java:41) finished in 2.048 s
2019-09-12 09:42:11,756  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 1 finished: show at SparkUtil.java:41, took 2.084327 s
2019-09-12 09:42:11,783  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 15.498653 ms
2019-09-12 09:42:11,820  INFO org.apache.spark.SparkContext 54 - Invoking stop() from shutdown hook
2019-09-12 09:42:11,826  INFO org.spark_project.jetty.server.AbstractConnector 310 - Stopped Spark@7fa2e67e{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 09:42:11,830  INFO org.apache.spark.ui.SparkUI 54 - Stopped Spark web UI at http://10.6.1.14:4042
2019-09-12 09:42:11,847  INFO org.apache.spark.MapOutputTrackerMasterEndpoint 54 - MapOutputTrackerMasterEndpoint stopped!
2019-09-12 09:42:11,889  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore cleared
2019-09-12 09:42:11,890  INFO org.apache.spark.storage.BlockManager 54 - BlockManager stopped
2019-09-12 09:42:11,892  INFO org.apache.spark.storage.BlockManagerMaster 54 - BlockManagerMaster stopped
2019-09-12 09:42:11,895  INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint 54 - OutputCommitCoordinator stopped!
2019-09-12 09:42:11,901  INFO org.apache.spark.SparkContext 54 - Successfully stopped SparkContext
2019-09-12 09:42:11,901  INFO org.apache.spark.util.ShutdownHookManager 54 - Shutdown hook called
2019-09-12 09:42:11,902  INFO org.apache.spark.util.ShutdownHookManager 54 - Deleting directory C:\Users\OUY\AppData\Local\Temp\spark-d7b9cf1f-9ed7-49ea-b088-4300f7d2fa95
2019-09-12 09:43:53,008  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-12 09:43:53,751  WARN org.apache.hadoop.util.NativeCodeLoader 62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-12 09:43:53,835 ERROR org.apache.hadoop.util.Shell 396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2430)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
	at com.datamodel.SparkUtil.CreateSession(SparkUtil.java:19)
	at com.datamodel.SparkUtil.readDataFromCsv(SparkUtil.java:24)
	at com.datamodel.SparkUtil.main(SparkUtil.java:37)
2019-09-12 09:43:53,954  INFO org.apache.spark.SparkContext 54 - Submitted application: splitCHWord
2019-09-12 09:43:54,001  INFO org.apache.spark.SecurityManager 54 - Changing view acls to: OUY
2019-09-12 09:43:54,002  INFO org.apache.spark.SecurityManager 54 - Changing modify acls to: OUY
2019-09-12 09:43:54,003  INFO org.apache.spark.SecurityManager 54 - Changing view acls groups to: 
2019-09-12 09:43:54,004  INFO org.apache.spark.SecurityManager 54 - Changing modify acls groups to: 
2019-09-12 09:43:54,004  INFO org.apache.spark.SecurityManager 54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(OUY); groups with view permissions: Set(); users  with modify permissions: Set(OUY); groups with modify permissions: Set()
2019-09-12 09:43:54,829  INFO org.apache.spark.util.Utils 54 - Successfully started service 'sparkDriver' on port 3698.
2019-09-12 09:43:54,910  INFO org.apache.spark.SparkEnv 54 - Registering MapOutputTracker
2019-09-12 09:43:54,933  INFO org.apache.spark.SparkEnv 54 - Registering BlockManagerMaster
2019-09-12 09:43:54,938  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-09-12 09:43:54,939  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - BlockManagerMasterEndpoint up
2019-09-12 09:43:54,956  INFO org.apache.spark.storage.DiskBlockManager 54 - Created local directory at C:\Users\OUY\AppData\Local\Temp\blockmgr-5c527a7e-2a0b-4a27-a7b9-34bd41dec337
2019-09-12 09:43:55,000  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore started with capacity 899.7 MB
2019-09-12 09:43:55,188  INFO org.apache.spark.SparkEnv 54 - Registering OutputCommitCoordinator
2019-09-12 09:43:55,306  INFO org.spark_project.jetty.util.log 192 - Logging initialized @4752ms
2019-09-12 09:43:55,369  INFO org.spark_project.jetty.server.Server 345 - jetty-9.3.z-SNAPSHOT
2019-09-12 09:43:55,386  INFO org.spark_project.jetty.server.Server 403 - Started @4834ms
2019-09-12 09:43:55,403  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2019-09-12 09:43:55,404  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
2019-09-12 09:43:55,412  INFO org.spark_project.jetty.server.AbstractConnector 270 - Started ServerConnector@2e029d61{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 09:43:55,413  INFO org.apache.spark.util.Utils 54 - Successfully started service 'SparkUI' on port 4042.
2019-09-12 09:43:55,444  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@d5ae57e{/jobs,null,AVAILABLE,@Spark}
2019-09-12 09:43:55,445  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@52b56a3e{/jobs/json,null,AVAILABLE,@Spark}
2019-09-12 09:43:55,447  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4eed46ee{/jobs/job,null,AVAILABLE,@Spark}
2019-09-12 09:43:55,449  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@475835b1{/jobs/job/json,null,AVAILABLE,@Spark}
2019-09-12 09:43:55,450  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5241cf67{/stages,null,AVAILABLE,@Spark}
2019-09-12 09:43:55,451  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@77192705{/stages/json,null,AVAILABLE,@Spark}
2019-09-12 09:43:55,451  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7e809b79{/stages/stage,null,AVAILABLE,@Spark}
2019-09-12 09:43:55,453  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@89c10b7{/stages/stage/json,null,AVAILABLE,@Spark}
2019-09-12 09:43:55,454  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4fe89c24{/stages/pool,null,AVAILABLE,@Spark}
2019-09-12 09:43:55,455  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3d08f3f5{/stages/pool/json,null,AVAILABLE,@Spark}
2019-09-12 09:43:55,456  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1a1da881{/storage,null,AVAILABLE,@Spark}
2019-09-12 09:43:55,457  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7fd4acee{/storage/json,null,AVAILABLE,@Spark}
2019-09-12 09:43:55,458  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6175619b{/storage/rdd,null,AVAILABLE,@Spark}
2019-09-12 09:43:55,459  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@756cf158{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-09-12 09:43:55,463  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@751d3241{/environment,null,AVAILABLE,@Spark}
2019-09-12 09:43:55,466  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@64337702{/environment/json,null,AVAILABLE,@Spark}
2019-09-12 09:43:55,467  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@30ea8c23{/executors,null,AVAILABLE,@Spark}
2019-09-12 09:43:55,468  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4e76dac{/executors/json,null,AVAILABLE,@Spark}
2019-09-12 09:43:55,469  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5f2f577{/executors/threadDump,null,AVAILABLE,@Spark}
2019-09-12 09:43:55,471  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5d465e4b{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-09-12 09:43:55,479  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@41a90fa8{/static,null,AVAILABLE,@Spark}
2019-09-12 09:43:55,481  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@35beb15e{/,null,AVAILABLE,@Spark}
2019-09-12 09:43:55,483  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5ac86ba5{/api,null,AVAILABLE,@Spark}
2019-09-12 09:43:55,484  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@63c5efee{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-09-12 09:43:55,485  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1c98290c{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-09-12 09:43:55,488  INFO org.apache.spark.ui.SparkUI 54 - Bound SparkUI to 0.0.0.0, and started at http://10.6.1.14:4042
2019-09-12 09:43:55,649  INFO org.apache.spark.executor.Executor 54 - Starting executor ID driver on host localhost
2019-09-12 09:43:55,705  INFO org.apache.spark.util.Utils 54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 3711.
2019-09-12 09:43:55,707  INFO org.apache.spark.network.netty.NettyBlockTransferService 54 - Server created on 10.6.1.14:3711
2019-09-12 09:43:55,718  INFO org.apache.spark.storage.BlockManager 54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-09-12 09:43:55,720  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registering BlockManager BlockManagerId(driver, 10.6.1.14, 3711, None)
2019-09-12 09:43:55,724  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Registering block manager 10.6.1.14:3711 with 899.7 MB RAM, BlockManagerId(driver, 10.6.1.14, 3711, None)
2019-09-12 09:43:55,734  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registered BlockManager BlockManagerId(driver, 10.6.1.14, 3711, None)
2019-09-12 09:43:55,735  INFO org.apache.spark.storage.BlockManager 54 - Initialized BlockManager: BlockManagerId(driver, 10.6.1.14, 3711, None)
2019-09-12 09:43:56,020  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@350b3a17{/metrics/json,null,AVAILABLE,@Spark}
2019-09-12 09:43:56,189  INFO org.apache.spark.sql.internal.SharedState 54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/MyProject/splitCHword/spark-warehouse/').
2019-09-12 09:43:56,192  INFO org.apache.spark.sql.internal.SharedState 54 - Warehouse path is 'file:/D:/MyProject/splitCHword/spark-warehouse/'.
2019-09-12 09:43:56,233  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@64a896b0{/SQL,null,AVAILABLE,@Spark}
2019-09-12 09:43:56,236  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@11a82d0f{/SQL/json,null,AVAILABLE,@Spark}
2019-09-12 09:43:56,239  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@73d6d0c{/SQL/execution,null,AVAILABLE,@Spark}
2019-09-12 09:43:56,242  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3961a41a{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-09-12 09:43:56,250  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1fd386c3{/static/sql,null,AVAILABLE,@Spark}
2019-09-12 09:43:57,880  INFO org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef 54 - Registered StateStoreCoordinator endpoint
2019-09-12 09:43:58,226  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0 stored as values in memory (estimated size 219.8 KB, free 899.5 MB)
2019-09-12 09:43:58,323  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 09:43:58,328  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_0_piece0 in memory on 10.6.1.14:3711 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 09:43:58,335  INFO org.apache.spark.SparkContext 54 - Created broadcast 0 from load at SparkUtil.java:28
2019-09-12 09:43:58,682  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 09:43:58,684  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 09:43:58,700  INFO org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 413 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 326
2019-09-12 09:43:58,757  INFO org.apache.spark.SparkContext 54 - Starting job: load at SparkUtil.java:28
2019-09-12 09:43:58,778  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 0 (load at SparkUtil.java:28) with 1 output partitions
2019-09-12 09:43:58,779  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 0 (load at SparkUtil.java:28)
2019-09-12 09:43:58,779  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 09:43:58,781  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 09:43:58,786  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:28), which has no missing parents
2019-09-12 09:43:58,829  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1 stored as values in memory (estimated size 5.5 KB, free 899.5 MB)
2019-09-12 09:43:58,835  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 899.5 MB)
2019-09-12 09:43:58,837  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_1_piece0 in memory on 10.6.1.14:3711 (size: 3.4 KB, free: 899.7 MB)
2019-09-12 09:43:58,838  INFO org.apache.spark.SparkContext 54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-09-12 09:43:58,865  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:28) (first 15 tasks are for partitions Vector(0))
2019-09-12 09:43:58,867  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 0.0 with 1 tasks
2019-09-12 09:43:58,933  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes)
2019-09-12 09:43:58,946  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 0.0 (TID 0)
2019-09-12 09:43:59,041  INFO org.apache.spark.rdd.BinaryFileRDD 54 - Input split: Paths:/D:/MyProject/splitCHword/target/classes/test.csv:0+326
2019-09-12 09:43:59,224  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 0.0 (TID 0). 853 bytes result sent to driver
2019-09-12 09:43:59,232  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 0.0 (TID 0) in 317 ms on localhost (executor driver) (1/1)
2019-09-12 09:43:59,234  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-09-12 09:43:59,238  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 0 (load at SparkUtil.java:28) finished in 0.342 s
2019-09-12 09:43:59,245  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 0 finished: load at SparkUtil.java:28, took 0.488030 s
2019-09-12 09:43:59,867  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_1_piece0 on 10.6.1.14:3711 in memory (size: 3.4 KB, free: 899.7 MB)
2019-09-12 09:44:01,473  INFO org.apache.spark.SparkContext 54 - Invoking stop() from shutdown hook
2019-09-12 09:44:01,479  INFO org.spark_project.jetty.server.AbstractConnector 310 - Stopped Spark@2e029d61{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 09:44:01,482  INFO org.apache.spark.ui.SparkUI 54 - Stopped Spark web UI at http://10.6.1.14:4042
2019-09-12 09:44:01,506  INFO org.apache.spark.MapOutputTrackerMasterEndpoint 54 - MapOutputTrackerMasterEndpoint stopped!
2019-09-12 09:44:01,528  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore cleared
2019-09-12 09:44:01,528  INFO org.apache.spark.storage.BlockManager 54 - BlockManager stopped
2019-09-12 09:44:01,531  INFO org.apache.spark.storage.BlockManagerMaster 54 - BlockManagerMaster stopped
2019-09-12 09:44:01,534  INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint 54 - OutputCommitCoordinator stopped!
2019-09-12 09:44:01,542  INFO org.apache.spark.SparkContext 54 - Successfully stopped SparkContext
2019-09-12 09:44:01,544  INFO org.apache.spark.util.ShutdownHookManager 54 - Shutdown hook called
2019-09-12 09:44:01,547  INFO org.apache.spark.util.ShutdownHookManager 54 - Deleting directory C:\Users\OUY\AppData\Local\Temp\spark-482ae49e-a73c-4184-99f2-20b588f1ef00
2019-09-12 09:46:06,432  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-12 09:46:06,982  WARN org.apache.hadoop.util.NativeCodeLoader 62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-12 09:46:07,075 ERROR org.apache.hadoop.util.Shell 396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2430)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
	at com.datamodel.SparkUtil.CreateSession(SparkUtil.java:19)
	at com.datamodel.SparkUtil.readDataFromCsv(SparkUtil.java:24)
	at com.datamodel.SparkUtil.main(SparkUtil.java:37)
2019-09-12 09:46:07,278  INFO org.apache.spark.SparkContext 54 - Submitted application: splitCHWord
2019-09-12 09:46:07,325  INFO org.apache.spark.SecurityManager 54 - Changing view acls to: OUY
2019-09-12 09:46:07,325  INFO org.apache.spark.SecurityManager 54 - Changing modify acls to: OUY
2019-09-12 09:46:07,325  INFO org.apache.spark.SecurityManager 54 - Changing view acls groups to: 
2019-09-12 09:46:07,325  INFO org.apache.spark.SecurityManager 54 - Changing modify acls groups to: 
2019-09-12 09:46:07,325  INFO org.apache.spark.SecurityManager 54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(OUY); groups with view permissions: Set(); users  with modify permissions: Set(OUY); groups with modify permissions: Set()
2019-09-12 09:46:08,211  INFO org.apache.spark.util.Utils 54 - Successfully started service 'sparkDriver' on port 3807.
2019-09-12 09:46:08,242  INFO org.apache.spark.SparkEnv 54 - Registering MapOutputTracker
2019-09-12 09:46:08,273  INFO org.apache.spark.SparkEnv 54 - Registering BlockManagerMaster
2019-09-12 09:46:08,273  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-09-12 09:46:08,273  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - BlockManagerMasterEndpoint up
2019-09-12 09:46:08,289  INFO org.apache.spark.storage.DiskBlockManager 54 - Created local directory at C:\Users\OUY\AppData\Local\Temp\blockmgr-7124bb21-080f-4c5c-84f1-764db8fe5fa0
2019-09-12 09:46:08,320  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore started with capacity 899.7 MB
2019-09-12 09:46:08,517  INFO org.apache.spark.SparkEnv 54 - Registering OutputCommitCoordinator
2019-09-12 09:46:08,709  INFO org.spark_project.jetty.util.log 192 - Logging initialized @5863ms
2019-09-12 09:46:08,829  INFO org.spark_project.jetty.server.Server 345 - jetty-9.3.z-SNAPSHOT
2019-09-12 09:46:08,856  INFO org.spark_project.jetty.server.Server 403 - Started @6013ms
2019-09-12 09:46:08,893  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2019-09-12 09:46:08,895  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
2019-09-12 09:46:08,907  INFO org.spark_project.jetty.server.AbstractConnector 270 - Started ServerConnector@cbc8d0f{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 09:46:08,908  INFO org.apache.spark.util.Utils 54 - Successfully started service 'SparkUI' on port 4042.
2019-09-12 09:46:08,968  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@59e43e8c{/jobs,null,AVAILABLE,@Spark}
2019-09-12 09:46:08,969  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5d5160e6{/jobs/json,null,AVAILABLE,@Spark}
2019-09-12 09:46:08,970  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2903c6ff{/jobs/job,null,AVAILABLE,@Spark}
2019-09-12 09:46:08,972  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@408e96d9{/jobs/job/json,null,AVAILABLE,@Spark}
2019-09-12 09:46:08,977  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@168cd36b{/stages,null,AVAILABLE,@Spark}
2019-09-12 09:46:08,979  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3901f6af{/stages/json,null,AVAILABLE,@Spark}
2019-09-12 09:46:08,981  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@10cd6753{/stages/stage,null,AVAILABLE,@Spark}
2019-09-12 09:46:08,983  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2c8662ac{/stages/stage/json,null,AVAILABLE,@Spark}
2019-09-12 09:46:08,984  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3724b43e{/stages/pool,null,AVAILABLE,@Spark}
2019-09-12 09:46:08,985  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@68e7c8c3{/stages/pool/json,null,AVAILABLE,@Spark}
2019-09-12 09:46:08,986  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@238bfd6c{/storage,null,AVAILABLE,@Spark}
2019-09-12 09:46:08,987  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@58860997{/storage/json,null,AVAILABLE,@Spark}
2019-09-12 09:46:08,988  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7487b142{/storage/rdd,null,AVAILABLE,@Spark}
2019-09-12 09:46:08,989  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@199bc830{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-09-12 09:46:08,992  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@27b45ea{/environment,null,AVAILABLE,@Spark}
2019-09-12 09:46:08,996  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@790a251b{/environment/json,null,AVAILABLE,@Spark}
2019-09-12 09:46:08,998  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@150ede8b{/executors,null,AVAILABLE,@Spark}
2019-09-12 09:46:09,005  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3e15bb06{/executors/json,null,AVAILABLE,@Spark}
2019-09-12 09:46:09,006  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4e1ce44{/executors/threadDump,null,AVAILABLE,@Spark}
2019-09-12 09:46:09,010  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7a7cc52c{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-09-12 09:46:09,026  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@524a2ffb{/static,null,AVAILABLE,@Spark}
2019-09-12 09:46:09,027  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@29eda4f8{/,null,AVAILABLE,@Spark}
2019-09-12 09:46:09,030  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5e048149{/api,null,AVAILABLE,@Spark}
2019-09-12 09:46:09,031  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@56913163{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-09-12 09:46:09,033  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@a18649a{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-09-12 09:46:09,039  INFO org.apache.spark.ui.SparkUI 54 - Bound SparkUI to 0.0.0.0, and started at http://10.6.1.14:4042
2019-09-12 09:46:09,310  INFO org.apache.spark.executor.Executor 54 - Starting executor ID driver on host localhost
2019-09-12 09:46:09,362  INFO org.apache.spark.util.Utils 54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 3820.
2019-09-12 09:46:09,364  INFO org.apache.spark.network.netty.NettyBlockTransferService 54 - Server created on 10.6.1.14:3820
2019-09-12 09:46:09,366  INFO org.apache.spark.storage.BlockManager 54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-09-12 09:46:09,370  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registering BlockManager BlockManagerId(driver, 10.6.1.14, 3820, None)
2019-09-12 09:46:09,377  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Registering block manager 10.6.1.14:3820 with 899.7 MB RAM, BlockManagerId(driver, 10.6.1.14, 3820, None)
2019-09-12 09:46:09,386  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registered BlockManager BlockManagerId(driver, 10.6.1.14, 3820, None)
2019-09-12 09:46:09,387  INFO org.apache.spark.storage.BlockManager 54 - Initialized BlockManager: BlockManagerId(driver, 10.6.1.14, 3820, None)
2019-09-12 09:46:09,728  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@433c6abb{/metrics/json,null,AVAILABLE,@Spark}
2019-09-12 09:46:09,854  INFO org.apache.spark.sql.internal.SharedState 54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/MyProject/splitCHword/spark-warehouse/').
2019-09-12 09:46:09,855  INFO org.apache.spark.sql.internal.SharedState 54 - Warehouse path is 'file:/D:/MyProject/splitCHword/spark-warehouse/'.
2019-09-12 09:46:09,872  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@436bd4df{/SQL,null,AVAILABLE,@Spark}
2019-09-12 09:46:09,873  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@149b0577{/SQL/json,null,AVAILABLE,@Spark}
2019-09-12 09:46:09,880  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7be1ce6a{/SQL/execution,null,AVAILABLE,@Spark}
2019-09-12 09:46:09,882  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6dc9da2d{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-09-12 09:46:09,885  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3c91530d{/static/sql,null,AVAILABLE,@Spark}
2019-09-12 09:46:11,634  INFO org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef 54 - Registered StateStoreCoordinator endpoint
2019-09-12 09:46:12,098  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0 stored as values in memory (estimated size 219.8 KB, free 899.5 MB)
2019-09-12 09:46:12,216  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 09:46:12,231  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_0_piece0 in memory on 10.6.1.14:3820 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 09:46:12,247  INFO org.apache.spark.SparkContext 54 - Created broadcast 0 from load at SparkUtil.java:28
2019-09-12 09:46:12,865  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 09:46:12,867  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 09:46:12,885  INFO org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 413 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 326
2019-09-12 09:46:12,969  INFO org.apache.spark.SparkContext 54 - Starting job: load at SparkUtil.java:28
2019-09-12 09:46:13,000  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 0 (load at SparkUtil.java:28) with 1 output partitions
2019-09-12 09:46:13,000  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 0 (load at SparkUtil.java:28)
2019-09-12 09:46:13,001  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 09:46:13,003  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 09:46:13,013  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:28), which has no missing parents
2019-09-12 09:46:13,043  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1 stored as values in memory (estimated size 5.5 KB, free 899.5 MB)
2019-09-12 09:46:13,094  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 899.5 MB)
2019-09-12 09:46:13,103  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_1_piece0 in memory on 10.6.1.14:3820 (size: 3.4 KB, free: 899.7 MB)
2019-09-12 09:46:13,110  INFO org.apache.spark.SparkContext 54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-09-12 09:46:13,148  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:28) (first 15 tasks are for partitions Vector(0))
2019-09-12 09:46:13,150  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 0.0 with 1 tasks
2019-09-12 09:46:13,228  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes)
2019-09-12 09:46:13,244  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 0.0 (TID 0)
2019-09-12 09:46:13,397  INFO org.apache.spark.rdd.BinaryFileRDD 54 - Input split: Paths:/D:/MyProject/splitCHword/target/classes/test.csv:0+326
2019-09-12 09:46:13,661  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 0.0 (TID 0). 853 bytes result sent to driver
2019-09-12 09:46:13,674  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 0.0 (TID 0) in 466 ms on localhost (executor driver) (1/1)
2019-09-12 09:46:13,678  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-09-12 09:46:13,686  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 0 (load at SparkUtil.java:28) finished in 0.503 s
2019-09-12 09:46:13,696  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 0 finished: load at SparkUtil.java:28, took 0.725657 s
2019-09-12 09:46:15,185  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_1_piece0 on 10.6.1.14:3820 in memory (size: 3.4 KB, free: 899.7 MB)
2019-09-12 09:46:16,898  INFO org.apache.spark.SparkContext 54 - Invoking stop() from shutdown hook
2019-09-12 09:46:16,907  INFO org.spark_project.jetty.server.AbstractConnector 310 - Stopped Spark@cbc8d0f{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 09:46:16,912  INFO org.apache.spark.ui.SparkUI 54 - Stopped Spark web UI at http://10.6.1.14:4042
2019-09-12 09:46:16,935  INFO org.apache.spark.MapOutputTrackerMasterEndpoint 54 - MapOutputTrackerMasterEndpoint stopped!
2019-09-12 09:46:17,151  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore cleared
2019-09-12 09:46:17,151  INFO org.apache.spark.storage.BlockManager 54 - BlockManager stopped
2019-09-12 09:46:17,167  INFO org.apache.spark.storage.BlockManagerMaster 54 - BlockManagerMaster stopped
2019-09-12 09:46:17,167  INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint 54 - OutputCommitCoordinator stopped!
2019-09-12 09:46:17,167  INFO org.apache.spark.SparkContext 54 - Successfully stopped SparkContext
2019-09-12 09:46:17,182  INFO org.apache.spark.util.ShutdownHookManager 54 - Shutdown hook called
2019-09-12 09:46:17,182  INFO org.apache.spark.util.ShutdownHookManager 54 - Deleting directory C:\Users\OUY\AppData\Local\Temp\spark-da72c6ca-8801-4a2c-a12e-894999a4c62a
2019-09-12 09:47:40,973  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-12 09:47:41,830  WARN org.apache.hadoop.util.NativeCodeLoader 62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-12 09:47:41,955 ERROR org.apache.hadoop.util.Shell 396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2430)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
	at com.datamodel.SparkUtil.CreateSession(SparkUtil.java:19)
	at com.datamodel.SparkUtil.readDataFromCsv(SparkUtil.java:24)
	at com.datamodel.SparkUtil.main(SparkUtil.java:37)
2019-09-12 09:47:42,146  INFO org.apache.spark.SparkContext 54 - Submitted application: splitCHWord
2019-09-12 09:47:42,210  INFO org.apache.spark.SecurityManager 54 - Changing view acls to: OUY
2019-09-12 09:47:42,212  INFO org.apache.spark.SecurityManager 54 - Changing modify acls to: OUY
2019-09-12 09:47:42,213  INFO org.apache.spark.SecurityManager 54 - Changing view acls groups to: 
2019-09-12 09:47:42,214  INFO org.apache.spark.SecurityManager 54 - Changing modify acls groups to: 
2019-09-12 09:47:42,215  INFO org.apache.spark.SecurityManager 54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(OUY); groups with view permissions: Set(); users  with modify permissions: Set(OUY); groups with modify permissions: Set()
2019-09-12 09:47:43,123  INFO org.apache.spark.util.Utils 54 - Successfully started service 'sparkDriver' on port 3879.
2019-09-12 09:47:43,194  INFO org.apache.spark.SparkEnv 54 - Registering MapOutputTracker
2019-09-12 09:47:43,223  INFO org.apache.spark.SparkEnv 54 - Registering BlockManagerMaster
2019-09-12 09:47:43,227  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-09-12 09:47:43,228  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - BlockManagerMasterEndpoint up
2019-09-12 09:47:43,245  INFO org.apache.spark.storage.DiskBlockManager 54 - Created local directory at C:\Users\OUY\AppData\Local\Temp\blockmgr-f7cd6a9c-f610-4992-8279-b02d56e700cb
2019-09-12 09:47:43,282  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore started with capacity 899.7 MB
2019-09-12 09:47:43,421  INFO org.apache.spark.SparkEnv 54 - Registering OutputCommitCoordinator
2019-09-12 09:47:43,575  INFO org.spark_project.jetty.util.log 192 - Logging initialized @5119ms
2019-09-12 09:47:43,695  INFO org.spark_project.jetty.server.Server 345 - jetty-9.3.z-SNAPSHOT
2019-09-12 09:47:43,727  INFO org.spark_project.jetty.server.Server 403 - Started @5274ms
2019-09-12 09:47:43,771  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2019-09-12 09:47:43,777  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
2019-09-12 09:47:43,786  INFO org.spark_project.jetty.server.AbstractConnector 270 - Started ServerConnector@191265ad{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 09:47:43,786  INFO org.apache.spark.util.Utils 54 - Successfully started service 'SparkUI' on port 4042.
2019-09-12 09:47:43,825  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@319988b0{/jobs,null,AVAILABLE,@Spark}
2019-09-12 09:47:43,827  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4c168660{/jobs/json,null,AVAILABLE,@Spark}
2019-09-12 09:47:43,828  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@fd0e5b6{/jobs/job,null,AVAILABLE,@Spark}
2019-09-12 09:47:43,829  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4fad94a7{/jobs/job/json,null,AVAILABLE,@Spark}
2019-09-12 09:47:43,829  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6326d182{/stages,null,AVAILABLE,@Spark}
2019-09-12 09:47:43,830  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@716a7124{/stages/json,null,AVAILABLE,@Spark}
2019-09-12 09:47:43,832  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@226642a5{/stages/stage,null,AVAILABLE,@Spark}
2019-09-12 09:47:43,835  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@72bd06ca{/stages/stage/json,null,AVAILABLE,@Spark}
2019-09-12 09:47:43,836  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5dbe30be{/stages/pool,null,AVAILABLE,@Spark}
2019-09-12 09:47:43,839  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@55795845{/stages/pool/json,null,AVAILABLE,@Spark}
2019-09-12 09:47:43,840  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@119f1f2a{/storage,null,AVAILABLE,@Spark}
2019-09-12 09:47:43,841  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5b970f7{/storage/json,null,AVAILABLE,@Spark}
2019-09-12 09:47:43,842  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@165b8a71{/storage/rdd,null,AVAILABLE,@Spark}
2019-09-12 09:47:43,844  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2f058b8a{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-09-12 09:47:43,845  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3f2ef586{/environment,null,AVAILABLE,@Spark}
2019-09-12 09:47:43,846  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@76c7beb3{/environment/json,null,AVAILABLE,@Spark}
2019-09-12 09:47:43,847  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2cf92cc7{/executors,null,AVAILABLE,@Spark}
2019-09-12 09:47:43,848  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7b139eab{/executors/json,null,AVAILABLE,@Spark}
2019-09-12 09:47:43,849  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@611df6e3{/executors/threadDump,null,AVAILABLE,@Spark}
2019-09-12 09:47:43,851  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6273c5a4{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-09-12 09:47:43,867  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@53e211ee{/static,null,AVAILABLE,@Spark}
2019-09-12 09:47:43,868  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4a9f80d3{/,null,AVAILABLE,@Spark}
2019-09-12 09:47:43,873  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@41fe9859{/api,null,AVAILABLE,@Spark}
2019-09-12 09:47:43,875  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@53ab0286{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-09-12 09:47:43,876  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2d10e0b1{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-09-12 09:47:43,878  INFO org.apache.spark.ui.SparkUI 54 - Bound SparkUI to 0.0.0.0, and started at http://10.6.1.14:4042
2019-09-12 09:47:44,143  INFO org.apache.spark.executor.Executor 54 - Starting executor ID driver on host localhost
2019-09-12 09:47:44,230  INFO org.apache.spark.util.Utils 54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 3892.
2019-09-12 09:47:44,231  INFO org.apache.spark.network.netty.NettyBlockTransferService 54 - Server created on 10.6.1.14:3892
2019-09-12 09:47:44,235  INFO org.apache.spark.storage.BlockManager 54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-09-12 09:47:44,248  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registering BlockManager BlockManagerId(driver, 10.6.1.14, 3892, None)
2019-09-12 09:47:44,259  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Registering block manager 10.6.1.14:3892 with 899.7 MB RAM, BlockManagerId(driver, 10.6.1.14, 3892, None)
2019-09-12 09:47:44,262  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registered BlockManager BlockManagerId(driver, 10.6.1.14, 3892, None)
2019-09-12 09:47:44,275  INFO org.apache.spark.storage.BlockManager 54 - Initialized BlockManager: BlockManagerId(driver, 10.6.1.14, 3892, None)
2019-09-12 09:47:44,763  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6c2f1700{/metrics/json,null,AVAILABLE,@Spark}
2019-09-12 09:47:45,071  INFO org.apache.spark.sql.internal.SharedState 54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/MyProject/splitCHword/spark-warehouse/').
2019-09-12 09:47:45,072  INFO org.apache.spark.sql.internal.SharedState 54 - Warehouse path is 'file:/D:/MyProject/splitCHword/spark-warehouse/'.
2019-09-12 09:47:45,084  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1adb7478{/SQL,null,AVAILABLE,@Spark}
2019-09-12 09:47:45,085  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@16943e88{/SQL/json,null,AVAILABLE,@Spark}
2019-09-12 09:47:45,088  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5a4ed68f{/SQL/execution,null,AVAILABLE,@Spark}
2019-09-12 09:47:45,089  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@d2387c8{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-09-12 09:47:45,092  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@100f9bbe{/static/sql,null,AVAILABLE,@Spark}
2019-09-12 09:47:47,366  INFO org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef 54 - Registered StateStoreCoordinator endpoint
2019-09-12 09:47:47,751  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0 stored as values in memory (estimated size 219.8 KB, free 899.5 MB)
2019-09-12 09:47:47,882  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 09:47:47,892  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_0_piece0 in memory on 10.6.1.14:3892 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 09:47:47,899  INFO org.apache.spark.SparkContext 54 - Created broadcast 0 from load at SparkUtil.java:28
2019-09-12 09:47:48,413  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 09:47:48,415  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 09:47:48,434  INFO org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 413 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 326
2019-09-12 09:47:48,507  INFO org.apache.spark.SparkContext 54 - Starting job: load at SparkUtil.java:28
2019-09-12 09:47:48,535  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 0 (load at SparkUtil.java:28) with 1 output partitions
2019-09-12 09:47:48,536  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 0 (load at SparkUtil.java:28)
2019-09-12 09:47:48,537  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 09:47:48,539  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 09:47:48,547  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:28), which has no missing parents
2019-09-12 09:47:48,592  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1 stored as values in memory (estimated size 5.5 KB, free 899.5 MB)
2019-09-12 09:47:48,602  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 899.5 MB)
2019-09-12 09:47:48,604  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_1_piece0 in memory on 10.6.1.14:3892 (size: 3.4 KB, free: 899.7 MB)
2019-09-12 09:47:48,605  INFO org.apache.spark.SparkContext 54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-09-12 09:47:48,693  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:28) (first 15 tasks are for partitions Vector(0))
2019-09-12 09:47:48,699  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 0.0 with 1 tasks
2019-09-12 09:47:48,964  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes)
2019-09-12 09:47:48,982  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 0.0 (TID 0)
2019-09-12 09:47:49,096  INFO org.apache.spark.rdd.BinaryFileRDD 54 - Input split: Paths:/D:/MyProject/splitCHword/target/classes/test.csv:0+326
2019-09-12 09:47:49,454  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 0.0 (TID 0). 853 bytes result sent to driver
2019-09-12 09:47:49,465  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 0.0 (TID 0) in 553 ms on localhost (executor driver) (1/1)
2019-09-12 09:47:49,470  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-09-12 09:47:49,475  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 0 (load at SparkUtil.java:28) finished in 0.596 s
2019-09-12 09:47:49,484  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 0 finished: load at SparkUtil.java:28, took 0.975042 s
2019-09-12 09:47:50,079  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_1_piece0 on 10.6.1.14:3892 in memory (size: 3.4 KB, free: 899.7 MB)
2019-09-12 09:47:51,955  INFO org.apache.spark.SparkContext 54 - Invoking stop() from shutdown hook
2019-09-12 09:47:51,964  INFO org.spark_project.jetty.server.AbstractConnector 310 - Stopped Spark@191265ad{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 09:47:51,967  INFO org.apache.spark.ui.SparkUI 54 - Stopped Spark web UI at http://10.6.1.14:4042
2019-09-12 09:47:51,982  INFO org.apache.spark.MapOutputTrackerMasterEndpoint 54 - MapOutputTrackerMasterEndpoint stopped!
2019-09-12 09:47:52,028  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore cleared
2019-09-12 09:47:52,030  INFO org.apache.spark.storage.BlockManager 54 - BlockManager stopped
2019-09-12 09:47:52,034  INFO org.apache.spark.storage.BlockManagerMaster 54 - BlockManagerMaster stopped
2019-09-12 09:47:52,051  INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint 54 - OutputCommitCoordinator stopped!
2019-09-12 09:47:52,057  INFO org.apache.spark.SparkContext 54 - Successfully stopped SparkContext
2019-09-12 09:47:52,058  INFO org.apache.spark.util.ShutdownHookManager 54 - Shutdown hook called
2019-09-12 09:47:52,060  INFO org.apache.spark.util.ShutdownHookManager 54 - Deleting directory C:\Users\OUY\AppData\Local\Temp\spark-659980ed-15cc-49b9-9766-74119075e3a3
2019-09-12 10:06:55,664  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-12 10:06:56,418  WARN org.apache.hadoop.util.NativeCodeLoader 62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-12 10:06:56,590 ERROR org.apache.hadoop.util.Shell 396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2430)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
	at com.datamodel.SparkUtil.CreateSession(SparkUtil.java:19)
	at com.datamodel.SparkUtil.readDataFromCsv(SparkUtil.java:24)
	at com.datamodel.SparkUtil.main(SparkUtil.java:37)
2019-09-12 10:06:56,731  INFO org.apache.spark.SparkContext 54 - Submitted application: splitCHWord
2019-09-12 10:06:56,762  INFO org.apache.spark.SecurityManager 54 - Changing view acls to: OUY
2019-09-12 10:06:56,762  INFO org.apache.spark.SecurityManager 54 - Changing modify acls to: OUY
2019-09-12 10:06:56,762  INFO org.apache.spark.SecurityManager 54 - Changing view acls groups to: 
2019-09-12 10:06:56,762  INFO org.apache.spark.SecurityManager 54 - Changing modify acls groups to: 
2019-09-12 10:06:56,762  INFO org.apache.spark.SecurityManager 54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(OUY); groups with view permissions: Set(); users  with modify permissions: Set(OUY); groups with modify permissions: Set()
2019-09-12 10:06:57,746  INFO org.apache.spark.util.Utils 54 - Successfully started service 'sparkDriver' on port 4125.
2019-09-12 10:06:57,808  INFO org.apache.spark.SparkEnv 54 - Registering MapOutputTracker
2019-09-12 10:06:57,840  INFO org.apache.spark.SparkEnv 54 - Registering BlockManagerMaster
2019-09-12 10:06:57,840  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-09-12 10:06:57,840  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - BlockManagerMasterEndpoint up
2019-09-12 10:06:57,855  INFO org.apache.spark.storage.DiskBlockManager 54 - Created local directory at C:\Users\OUY\AppData\Local\Temp\blockmgr-741ca26a-3b97-4dea-8450-75825c3592e5
2019-09-12 10:06:57,887  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore started with capacity 899.7 MB
2019-09-12 10:06:57,949  INFO org.apache.spark.SparkEnv 54 - Registering OutputCommitCoordinator
2019-09-12 10:06:58,058  INFO org.spark_project.jetty.util.log 192 - Logging initialized @4790ms
2019-09-12 10:06:58,137  INFO org.spark_project.jetty.server.Server 345 - jetty-9.3.z-SNAPSHOT
2019-09-12 10:06:58,152  INFO org.spark_project.jetty.server.Server 403 - Started @4882ms
2019-09-12 10:06:58,168  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2019-09-12 10:06:58,168  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
2019-09-12 10:06:58,199  INFO org.spark_project.jetty.server.AbstractConnector 270 - Started ServerConnector@60843569{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 10:06:58,199  INFO org.apache.spark.util.Utils 54 - Successfully started service 'SparkUI' on port 4042.
2019-09-12 10:06:58,402  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2a39fe6a{/jobs,null,AVAILABLE,@Spark}
2019-09-12 10:06:58,402  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@649725e3{/jobs/json,null,AVAILABLE,@Spark}
2019-09-12 10:06:58,402  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4c168660{/jobs/job,null,AVAILABLE,@Spark}
2019-09-12 10:06:58,402  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4eed46ee{/jobs/job/json,null,AVAILABLE,@Spark}
2019-09-12 10:06:58,402  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4fad94a7{/stages,null,AVAILABLE,@Spark}
2019-09-12 10:06:58,402  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6326d182{/stages/json,null,AVAILABLE,@Spark}
2019-09-12 10:06:58,402  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@716a7124{/stages/stage,null,AVAILABLE,@Spark}
2019-09-12 10:06:58,418  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5cc126dc{/stages/stage/json,null,AVAILABLE,@Spark}
2019-09-12 10:06:58,418  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@72bd06ca{/stages/pool,null,AVAILABLE,@Spark}
2019-09-12 10:06:58,418  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5dbe30be{/stages/pool/json,null,AVAILABLE,@Spark}
2019-09-12 10:06:58,418  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@55795845{/storage,null,AVAILABLE,@Spark}
2019-09-12 10:06:58,418  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@119f1f2a{/storage/json,null,AVAILABLE,@Spark}
2019-09-12 10:06:58,418  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5b970f7{/storage/rdd,null,AVAILABLE,@Spark}
2019-09-12 10:06:58,418  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@165b8a71{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-09-12 10:06:58,418  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2f058b8a{/environment,null,AVAILABLE,@Spark}
2019-09-12 10:06:58,418  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3f2ef586{/environment/json,null,AVAILABLE,@Spark}
2019-09-12 10:06:58,418  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@76c7beb3{/executors,null,AVAILABLE,@Spark}
2019-09-12 10:06:58,418  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2cf92cc7{/executors/json,null,AVAILABLE,@Spark}
2019-09-12 10:06:58,418  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7b139eab{/executors/threadDump,null,AVAILABLE,@Spark}
2019-09-12 10:06:58,433  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@611df6e3{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-09-12 10:06:58,433  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6273c5a4{/static,null,AVAILABLE,@Spark}
2019-09-12 10:06:58,433  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3e2943ab{/,null,AVAILABLE,@Spark}
2019-09-12 10:06:58,449  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4a9f80d3{/api,null,AVAILABLE,@Spark}
2019-09-12 10:06:58,449  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@191ae03f{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-09-12 10:06:58,449  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@53ab0286{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-09-12 10:06:58,449  INFO org.apache.spark.ui.SparkUI 54 - Bound SparkUI to 0.0.0.0, and started at http://10.6.1.14:4042
2019-09-12 10:06:58,652  INFO org.apache.spark.executor.Executor 54 - Starting executor ID driver on host localhost
2019-09-12 10:06:58,699  INFO org.apache.spark.util.Utils 54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 4138.
2019-09-12 10:06:58,699  INFO org.apache.spark.network.netty.NettyBlockTransferService 54 - Server created on 10.6.1.14:4138
2019-09-12 10:06:58,699  INFO org.apache.spark.storage.BlockManager 54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-09-12 10:06:58,714  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registering BlockManager BlockManagerId(driver, 10.6.1.14, 4138, None)
2019-09-12 10:06:58,714  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Registering block manager 10.6.1.14:4138 with 899.7 MB RAM, BlockManagerId(driver, 10.6.1.14, 4138, None)
2019-09-12 10:06:58,714  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registered BlockManager BlockManagerId(driver, 10.6.1.14, 4138, None)
2019-09-12 10:06:58,730  INFO org.apache.spark.storage.BlockManager 54 - Initialized BlockManager: BlockManagerId(driver, 10.6.1.14, 4138, None)
2019-09-12 10:06:59,011  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3e1162e7{/metrics/json,null,AVAILABLE,@Spark}
2019-09-12 10:06:59,105  INFO org.apache.spark.sql.internal.SharedState 54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/MyProject/splitCHword/spark-warehouse/').
2019-09-12 10:06:59,121  INFO org.apache.spark.sql.internal.SharedState 54 - Warehouse path is 'file:/D:/MyProject/splitCHword/spark-warehouse/'.
2019-09-12 10:06:59,121  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@36a7abe1{/SQL,null,AVAILABLE,@Spark}
2019-09-12 10:06:59,121  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@e044b4a{/SQL/json,null,AVAILABLE,@Spark}
2019-09-12 10:06:59,121  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4604b900{/SQL/execution,null,AVAILABLE,@Spark}
2019-09-12 10:06:59,121  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@e36bb2a{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-09-12 10:06:59,136  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1500e009{/static/sql,null,AVAILABLE,@Spark}
2019-09-12 10:07:00,308  INFO org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef 54 - Registered StateStoreCoordinator endpoint
2019-09-12 10:07:00,589  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0 stored as values in memory (estimated size 219.8 KB, free 899.5 MB)
2019-09-12 10:07:00,698  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 10:07:00,698  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_0_piece0 in memory on 10.6.1.14:4138 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 10:07:00,714  INFO org.apache.spark.SparkContext 54 - Created broadcast 0 from load at SparkUtil.java:28
2019-09-12 10:07:01,042  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 10:07:01,042  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 10:07:01,058  INFO org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 413 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 326
2019-09-12 10:07:01,120  INFO org.apache.spark.SparkContext 54 - Starting job: load at SparkUtil.java:28
2019-09-12 10:07:01,151  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 0 (load at SparkUtil.java:28) with 1 output partitions
2019-09-12 10:07:01,151  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 0 (load at SparkUtil.java:28)
2019-09-12 10:07:01,151  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 10:07:01,151  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 10:07:01,151  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:28), which has no missing parents
2019-09-12 10:07:01,167  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1 stored as values in memory (estimated size 5.5 KB, free 899.5 MB)
2019-09-12 10:07:01,183  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 899.5 MB)
2019-09-12 10:07:01,183  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_1_piece0 in memory on 10.6.1.14:4138 (size: 3.4 KB, free: 899.7 MB)
2019-09-12 10:07:01,183  INFO org.apache.spark.SparkContext 54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-09-12 10:07:01,198  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:28) (first 15 tasks are for partitions Vector(0))
2019-09-12 10:07:01,198  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 0.0 with 1 tasks
2019-09-12 10:07:01,261  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes)
2019-09-12 10:07:01,276  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 0.0 (TID 0)
2019-09-12 10:07:01,354  INFO org.apache.spark.rdd.BinaryFileRDD 54 - Input split: Paths:/D:/MyProject/splitCHword/target/classes/test.csv:0+326
2019-09-12 10:07:01,511  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 0.0 (TID 0). 853 bytes result sent to driver
2019-09-12 10:07:01,526  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 0.0 (TID 0) in 281 ms on localhost (executor driver) (1/1)
2019-09-12 10:07:01,526  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-09-12 10:07:01,526  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 0 (load at SparkUtil.java:28) finished in 0.296 s
2019-09-12 10:07:01,542  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 0 finished: load at SparkUtil.java:28, took 0.410762 s
2019-09-12 10:07:01,948  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_1_piece0 on 10.6.1.14:4138 in memory (size: 3.4 KB, free: 899.7 MB)
2019-09-12 10:07:03,698  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_0_piece0 on 10.6.1.14:4138 in memory (size: 20.7 KB, free: 899.7 MB)
2019-09-12 10:07:03,901  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Pruning directories with: 
2019-09-12 10:07:03,916  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Post-Scan Filters: 
2019-09-12 10:07:03,916  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Output Data Schema: struct<feature: string>
2019-09-12 10:07:03,932  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Pushed Filters: 
2019-09-12 10:07:04,479  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 294.296923 ms
2019-09-12 10:07:04,557  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 38.117087 ms
2019-09-12 10:07:04,557  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2 stored as values in memory (estimated size 220.5 KB, free 899.5 MB)
2019-09-12 10:07:04,572  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 10:07:04,588  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_2_piece0 in memory on 10.6.1.14:4138 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 10:07:04,588  INFO org.apache.spark.SparkContext 54 - Created broadcast 2 from count at SparkUtil.java:41
2019-09-12 10:07:04,604  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2019-09-12 10:07:04,682  INFO org.apache.spark.SparkContext 54 - Starting job: count at SparkUtil.java:41
2019-09-12 10:07:04,713  INFO org.apache.spark.scheduler.DAGScheduler 54 - Registering RDD 6 (count at SparkUtil.java:41)
2019-09-12 10:07:04,713  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 1 (count at SparkUtil.java:41) with 1 output partitions
2019-09-12 10:07:04,713  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 2 (count at SparkUtil.java:41)
2019-09-12 10:07:04,713  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List(ShuffleMapStage 1)
2019-09-12 10:07:04,713  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List(ShuffleMapStage 1)
2019-09-12 10:07:04,713  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ShuffleMapStage 1 (MapPartitionsRDD[6] at count at SparkUtil.java:41), which has no missing parents
2019-09-12 10:07:04,744  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3 stored as values in memory (estimated size 16.5 KB, free 899.4 MB)
2019-09-12 10:07:04,744  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.0 KB, free 899.4 MB)
2019-09-12 10:07:04,744  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_3_piece0 in memory on 10.6.1.14:4138 (size: 8.0 KB, free: 899.7 MB)
2019-09-12 10:07:04,744  INFO org.apache.spark.SparkContext 54 - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2019-09-12 10:07:04,760  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[6] at count at SparkUtil.java:41) (first 15 tasks are for partitions Vector(0))
2019-09-12 10:07:04,760  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 1.0 with 1 tasks
2019-09-12 10:07:04,776  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5282 bytes)
2019-09-12 10:07:04,776  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 1.0 (TID 1)
2019-09-12 10:07:04,822  INFO org.apache.spark.sql.execution.datasources.FileScanRDD 54 - Reading File path: file:///D:/MyProject/splitCHword/target/classes/test.csv, range: 0-326, partition values: [empty row]
2019-09-12 10:07:04,838  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 14.218228 ms
2019-09-12 10:07:05,119  INFO org.ansj.util.MyStaticValue 106 - init version.number to env value is : 2.11.8
2019-09-12 10:07:05,119  INFO org.ansj.util.MyStaticValue 106 - init copyright.string to env value is : Copyright 2002-2016, LAMP/EPFL
2019-09-12 10:07:05,119  INFO org.ansj.util.MyStaticValue 106 - init osgi.version.number to env value is : 2.11.8.v20160304-115712-1706a37eb8
2019-09-12 10:07:05,119  INFO org.ansj.util.MyStaticValue 106 - init maven.version.number to env value is : 2.11.8
2019-09-12 10:07:05,119  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/ambiguity.dic
2019-09-12 10:07:05,119 ERROR org.ansj.library.AmbiguityLibrary 117 - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:D:\MyProject\splitCHword\library\ambiguity.dic not found or can not to read, path: library/ambiguity.dic
2019-09-12 10:07:05,119  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/default.dic
2019-09-12 10:07:05,119 ERROR org.ansj.library.DicLibrary 221 - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:D:\MyProject\splitCHword\library\default.dic not found or can not to read, path: library/default.dic
2019-09-12 10:07:05,510  INFO org.apache.spark.ContextCleaner 54 - Cleaned accumulator 48
2019-09-12 10:07:06,244  INFO org.ansj.library.DATDictionary 90 - init core library ok use time : 969
2019-09-12 10:07:06,900  INFO org.ansj.library.NgramLibrary 17 - init ngram ok use time :656
2019-09-12 10:07:07,041  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
2019-09-12 10:07:07,041  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 1.0 (TID 1) in 2281 ms on localhost (executor driver) (1/1)
2019-09-12 10:07:07,041  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-09-12 10:07:07,041  INFO org.apache.spark.scheduler.DAGScheduler 54 - ShuffleMapStage 1 (count at SparkUtil.java:41) finished in 2.281 s
2019-09-12 10:07:07,041  INFO org.apache.spark.scheduler.DAGScheduler 54 - looking for newly runnable stages
2019-09-12 10:07:07,041  INFO org.apache.spark.scheduler.DAGScheduler 54 - running: Set()
2019-09-12 10:07:07,041  INFO org.apache.spark.scheduler.DAGScheduler 54 - waiting: Set(ResultStage 2)
2019-09-12 10:07:07,041  INFO org.apache.spark.scheduler.DAGScheduler 54 - failed: Set()
2019-09-12 10:07:07,041  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 2 (MapPartitionsRDD[9] at count at SparkUtil.java:41), which has no missing parents
2019-09-12 10:07:07,056  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_4 stored as values in memory (estimated size 7.0 KB, free 899.4 MB)
2019-09-12 10:07:07,056  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.7 KB, free 899.4 MB)
2019-09-12 10:07:07,072  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_4_piece0 in memory on 10.6.1.14:4138 (size: 3.7 KB, free: 899.7 MB)
2019-09-12 10:07:07,072  INFO org.apache.spark.SparkContext 54 - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
2019-09-12 10:07:07,072  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at count at SparkUtil.java:41) (first 15 tasks are for partitions Vector(0))
2019-09-12 10:07:07,072  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 2.0 with 1 tasks
2019-09-12 10:07:07,087  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4726 bytes)
2019-09-12 10:07:07,087  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 2.0 (TID 2)
2019-09-12 10:07:07,103  INFO org.apache.spark.storage.ShuffleBlockFetcherIterator 54 - Getting 1 non-empty blocks out of 1 blocks
2019-09-12 10:07:07,103  INFO org.apache.spark.storage.ShuffleBlockFetcherIterator 54 - Started 0 remote fetches in 0 ms
2019-09-12 10:07:07,134  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 2.0 (TID 2). 1495 bytes result sent to driver
2019-09-12 10:07:07,150  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 2.0 (TID 2) in 47 ms on localhost (executor driver) (1/1)
2019-09-12 10:07:07,150  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 2 (count at SparkUtil.java:41) finished in 0.063 s
2019-09-12 10:07:07,150  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 1 finished: count at SparkUtil.java:41, took 2.458012 s
2019-09-12 10:07:07,150  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-09-12 10:07:07,166  INFO org.apache.spark.SparkContext 54 - Invoking stop() from shutdown hook
2019-09-12 10:07:07,181  INFO org.spark_project.jetty.server.AbstractConnector 310 - Stopped Spark@60843569{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 10:07:07,181  INFO org.apache.spark.ui.SparkUI 54 - Stopped Spark web UI at http://10.6.1.14:4042
2019-09-12 10:07:07,197  INFO org.apache.spark.MapOutputTrackerMasterEndpoint 54 - MapOutputTrackerMasterEndpoint stopped!
2019-09-12 10:07:07,259  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore cleared
2019-09-12 10:07:07,259  INFO org.apache.spark.storage.BlockManager 54 - BlockManager stopped
2019-09-12 10:07:07,259  INFO org.apache.spark.storage.BlockManagerMaster 54 - BlockManagerMaster stopped
2019-09-12 10:07:07,259  INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint 54 - OutputCommitCoordinator stopped!
2019-09-12 10:07:07,259  INFO org.apache.spark.SparkContext 54 - Successfully stopped SparkContext
2019-09-12 10:07:07,259  INFO org.apache.spark.util.ShutdownHookManager 54 - Shutdown hook called
2019-09-12 10:07:07,259  INFO org.apache.spark.util.ShutdownHookManager 54 - Deleting directory C:\Users\OUY\AppData\Local\Temp\spark-783b2708-17b4-4f0f-a840-7b23ace8ee9f
2019-09-12 10:07:19,378  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-12 10:07:20,034  WARN org.apache.hadoop.util.NativeCodeLoader 62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-12 10:07:20,213 ERROR org.apache.hadoop.util.Shell 396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2430)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
	at com.datamodel.SparkUtil.CreateSession(SparkUtil.java:19)
	at com.datamodel.SparkUtil.readDataFromCsv(SparkUtil.java:24)
	at com.datamodel.SparkUtil.main(SparkUtil.java:37)
2019-09-12 10:07:20,372  INFO org.apache.spark.SparkContext 54 - Submitted application: splitCHWord
2019-09-12 10:07:20,406  INFO org.apache.spark.SecurityManager 54 - Changing view acls to: OUY
2019-09-12 10:07:20,407  INFO org.apache.spark.SecurityManager 54 - Changing modify acls to: OUY
2019-09-12 10:07:20,408  INFO org.apache.spark.SecurityManager 54 - Changing view acls groups to: 
2019-09-12 10:07:20,409  INFO org.apache.spark.SecurityManager 54 - Changing modify acls groups to: 
2019-09-12 10:07:20,410  INFO org.apache.spark.SecurityManager 54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(OUY); groups with view permissions: Set(); users  with modify permissions: Set(OUY); groups with modify permissions: Set()
2019-09-12 10:07:21,149  INFO org.apache.spark.util.Utils 54 - Successfully started service 'sparkDriver' on port 4161.
2019-09-12 10:07:21,212  INFO org.apache.spark.SparkEnv 54 - Registering MapOutputTracker
2019-09-12 10:07:21,227  INFO org.apache.spark.SparkEnv 54 - Registering BlockManagerMaster
2019-09-12 10:07:21,227  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-09-12 10:07:21,227  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - BlockManagerMasterEndpoint up
2019-09-12 10:07:21,243  INFO org.apache.spark.storage.DiskBlockManager 54 - Created local directory at C:\Users\OUY\AppData\Local\Temp\blockmgr-98c9f066-79aa-4a8a-ab07-70b24af44021
2019-09-12 10:07:21,274  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore started with capacity 899.7 MB
2019-09-12 10:07:21,321  INFO org.apache.spark.SparkEnv 54 - Registering OutputCommitCoordinator
2019-09-12 10:07:21,430  INFO org.spark_project.jetty.util.log 192 - Logging initialized @4020ms
2019-09-12 10:07:21,493  INFO org.spark_project.jetty.server.Server 345 - jetty-9.3.z-SNAPSHOT
2019-09-12 10:07:21,509  INFO org.spark_project.jetty.server.Server 403 - Started @4097ms
2019-09-12 10:07:21,524  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2019-09-12 10:07:21,524  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
2019-09-12 10:07:21,540  INFO org.spark_project.jetty.server.AbstractConnector 270 - Started ServerConnector@2e029d61{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 10:07:21,540  INFO org.apache.spark.util.Utils 54 - Successfully started service 'SparkUI' on port 4042.
2019-09-12 10:07:21,571  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@d5ae57e{/jobs,null,AVAILABLE,@Spark}
2019-09-12 10:07:21,571  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@52b56a3e{/jobs/json,null,AVAILABLE,@Spark}
2019-09-12 10:07:21,571  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4eed46ee{/jobs/job,null,AVAILABLE,@Spark}
2019-09-12 10:07:21,571  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@475835b1{/jobs/job/json,null,AVAILABLE,@Spark}
2019-09-12 10:07:21,571  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5241cf67{/stages,null,AVAILABLE,@Spark}
2019-09-12 10:07:21,571  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@77192705{/stages/json,null,AVAILABLE,@Spark}
2019-09-12 10:07:21,571  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7e809b79{/stages/stage,null,AVAILABLE,@Spark}
2019-09-12 10:07:21,571  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@89c10b7{/stages/stage/json,null,AVAILABLE,@Spark}
2019-09-12 10:07:21,571  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4fe89c24{/stages/pool,null,AVAILABLE,@Spark}
2019-09-12 10:07:21,571  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3d08f3f5{/stages/pool/json,null,AVAILABLE,@Spark}
2019-09-12 10:07:21,587  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1a1da881{/storage,null,AVAILABLE,@Spark}
2019-09-12 10:07:21,587  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7fd4acee{/storage/json,null,AVAILABLE,@Spark}
2019-09-12 10:07:21,587  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6175619b{/storage/rdd,null,AVAILABLE,@Spark}
2019-09-12 10:07:21,587  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@756cf158{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-09-12 10:07:21,587  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@751d3241{/environment,null,AVAILABLE,@Spark}
2019-09-12 10:07:21,587  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@64337702{/environment/json,null,AVAILABLE,@Spark}
2019-09-12 10:07:21,587  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@30ea8c23{/executors,null,AVAILABLE,@Spark}
2019-09-12 10:07:21,587  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4e76dac{/executors/json,null,AVAILABLE,@Spark}
2019-09-12 10:07:21,587  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5f2f577{/executors/threadDump,null,AVAILABLE,@Spark}
2019-09-12 10:07:21,587  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5d465e4b{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-09-12 10:07:21,602  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@41a90fa8{/static,null,AVAILABLE,@Spark}
2019-09-12 10:07:21,602  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@35beb15e{/,null,AVAILABLE,@Spark}
2019-09-12 10:07:21,602  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5ac86ba5{/api,null,AVAILABLE,@Spark}
2019-09-12 10:07:21,602  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@63c5efee{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-09-12 10:07:21,602  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1c98290c{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-09-12 10:07:21,602  INFO org.apache.spark.ui.SparkUI 54 - Bound SparkUI to 0.0.0.0, and started at http://10.6.1.14:4042
2019-09-12 10:07:21,759  INFO org.apache.spark.executor.Executor 54 - Starting executor ID driver on host localhost
2019-09-12 10:07:21,805  INFO org.apache.spark.util.Utils 54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 4174.
2019-09-12 10:07:21,805  INFO org.apache.spark.network.netty.NettyBlockTransferService 54 - Server created on 10.6.1.14:4174
2019-09-12 10:07:21,805  INFO org.apache.spark.storage.BlockManager 54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-09-12 10:07:21,805  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registering BlockManager BlockManagerId(driver, 10.6.1.14, 4174, None)
2019-09-12 10:07:21,805  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Registering block manager 10.6.1.14:4174 with 899.7 MB RAM, BlockManagerId(driver, 10.6.1.14, 4174, None)
2019-09-12 10:07:21,821  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registered BlockManager BlockManagerId(driver, 10.6.1.14, 4174, None)
2019-09-12 10:07:21,821  INFO org.apache.spark.storage.BlockManager 54 - Initialized BlockManager: BlockManagerId(driver, 10.6.1.14, 4174, None)
2019-09-12 10:07:21,993  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@350b3a17{/metrics/json,null,AVAILABLE,@Spark}
2019-09-12 10:07:22,102  INFO org.apache.spark.sql.internal.SharedState 54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/MyProject/splitCHword/spark-warehouse/').
2019-09-12 10:07:22,102  INFO org.apache.spark.sql.internal.SharedState 54 - Warehouse path is 'file:/D:/MyProject/splitCHword/spark-warehouse/'.
2019-09-12 10:07:22,102  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@64a896b0{/SQL,null,AVAILABLE,@Spark}
2019-09-12 10:07:22,102  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@11a82d0f{/SQL/json,null,AVAILABLE,@Spark}
2019-09-12 10:07:22,118  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@73d6d0c{/SQL/execution,null,AVAILABLE,@Spark}
2019-09-12 10:07:22,118  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3961a41a{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-09-12 10:07:22,118  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1fd386c3{/static/sql,null,AVAILABLE,@Spark}
2019-09-12 10:07:23,243  INFO org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef 54 - Registered StateStoreCoordinator endpoint
2019-09-12 10:07:23,586  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0 stored as values in memory (estimated size 219.8 KB, free 899.5 MB)
2019-09-12 10:07:23,758  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 10:07:23,758  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_0_piece0 in memory on 10.6.1.14:4174 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 10:07:23,758  INFO org.apache.spark.SparkContext 54 - Created broadcast 0 from load at SparkUtil.java:28
2019-09-12 10:07:24,086  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 10:07:24,086  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 10:07:24,102  INFO org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 413 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 326
2019-09-12 10:07:24,149  INFO org.apache.spark.SparkContext 54 - Starting job: load at SparkUtil.java:28
2019-09-12 10:07:24,180  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 0 (load at SparkUtil.java:28) with 1 output partitions
2019-09-12 10:07:24,180  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 0 (load at SparkUtil.java:28)
2019-09-12 10:07:24,180  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 10:07:24,180  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 10:07:24,180  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:28), which has no missing parents
2019-09-12 10:07:24,227  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1 stored as values in memory (estimated size 5.5 KB, free 899.5 MB)
2019-09-12 10:07:24,227  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 899.5 MB)
2019-09-12 10:07:24,227  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_1_piece0 in memory on 10.6.1.14:4174 (size: 3.4 KB, free: 899.7 MB)
2019-09-12 10:07:24,227  INFO org.apache.spark.SparkContext 54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-09-12 10:07:24,258  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:28) (first 15 tasks are for partitions Vector(0))
2019-09-12 10:07:24,258  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 0.0 with 1 tasks
2019-09-12 10:07:24,320  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes)
2019-09-12 10:07:24,320  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 0.0 (TID 0)
2019-09-12 10:07:24,399  INFO org.apache.spark.rdd.BinaryFileRDD 54 - Input split: Paths:/D:/MyProject/splitCHword/target/classes/test.csv:0+326
2019-09-12 10:07:24,617  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 0.0 (TID 0). 853 bytes result sent to driver
2019-09-12 10:07:24,633  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 0.0 (TID 0) in 328 ms on localhost (executor driver) (1/1)
2019-09-12 10:07:24,633  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-09-12 10:07:24,633  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 0 (load at SparkUtil.java:28) finished in 0.359 s
2019-09-12 10:07:24,648  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 0 finished: load at SparkUtil.java:28, took 0.493478 s
2019-09-12 10:07:25,133  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_1_piece0 on 10.6.1.14:4174 in memory (size: 3.4 KB, free: 899.7 MB)
2019-09-12 10:07:25,133  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_0_piece0 on 10.6.1.14:4174 in memory (size: 20.7 KB, free: 899.7 MB)
2019-09-12 10:07:26,960  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Pruning directories with: 
2019-09-12 10:07:26,960  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Post-Scan Filters: 
2019-09-12 10:07:26,960  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Output Data Schema: struct<feature: string>
2019-09-12 10:07:26,976  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Pushed Filters: 
2019-09-12 10:07:27,476  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 257.758926 ms
2019-09-12 10:07:27,554  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 47.867693 ms
2019-09-12 10:07:27,570  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2 stored as values in memory (estimated size 220.5 KB, free 899.5 MB)
2019-09-12 10:07:27,585  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 10:07:27,585  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_2_piece0 in memory on 10.6.1.14:4174 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 10:07:27,585  INFO org.apache.spark.SparkContext 54 - Created broadcast 2 from count at SparkUtil.java:41
2019-09-12 10:07:27,601  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2019-09-12 10:07:27,679  INFO org.apache.spark.SparkContext 54 - Starting job: count at SparkUtil.java:41
2019-09-12 10:07:27,695  INFO org.apache.spark.scheduler.DAGScheduler 54 - Registering RDD 6 (count at SparkUtil.java:41)
2019-09-12 10:07:27,695  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 1 (count at SparkUtil.java:41) with 1 output partitions
2019-09-12 10:07:27,695  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 2 (count at SparkUtil.java:41)
2019-09-12 10:07:27,695  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List(ShuffleMapStage 1)
2019-09-12 10:07:27,695  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List(ShuffleMapStage 1)
2019-09-12 10:07:27,695  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ShuffleMapStage 1 (MapPartitionsRDD[6] at count at SparkUtil.java:41), which has no missing parents
2019-09-12 10:07:27,710  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3 stored as values in memory (estimated size 16.5 KB, free 899.4 MB)
2019-09-12 10:07:27,726  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.0 KB, free 899.4 MB)
2019-09-12 10:07:27,726  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_3_piece0 in memory on 10.6.1.14:4174 (size: 8.0 KB, free: 899.7 MB)
2019-09-12 10:07:27,726  INFO org.apache.spark.SparkContext 54 - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2019-09-12 10:07:27,726  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[6] at count at SparkUtil.java:41) (first 15 tasks are for partitions Vector(0))
2019-09-12 10:07:27,726  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 1.0 with 1 tasks
2019-09-12 10:07:27,741  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5282 bytes)
2019-09-12 10:07:27,741  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 1.0 (TID 1)
2019-09-12 10:07:27,773  INFO org.apache.spark.sql.execution.datasources.FileScanRDD 54 - Reading File path: file:///D:/MyProject/splitCHword/target/classes/test.csv, range: 0-326, partition values: [empty row]
2019-09-12 10:07:27,788  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 12.637429 ms
2019-09-12 10:07:28,038  INFO org.ansj.util.MyStaticValue 106 - init version.number to env value is : 2.11.8
2019-09-12 10:07:28,038  INFO org.ansj.util.MyStaticValue 106 - init copyright.string to env value is : Copyright 2002-2016, LAMP/EPFL
2019-09-12 10:07:28,038  INFO org.ansj.util.MyStaticValue 106 - init osgi.version.number to env value is : 2.11.8.v20160304-115712-1706a37eb8
2019-09-12 10:07:28,038  INFO org.ansj.util.MyStaticValue 106 - init maven.version.number to env value is : 2.11.8
2019-09-12 10:07:28,038  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/ambiguity.dic
2019-09-12 10:07:28,038 ERROR org.ansj.library.AmbiguityLibrary 117 - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:D:\MyProject\splitCHword\library\ambiguity.dic not found or can not to read, path: library/ambiguity.dic
2019-09-12 10:07:28,038  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/default.dic
2019-09-12 10:07:28,038 ERROR org.ansj.library.DicLibrary 221 - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:D:\MyProject\splitCHword\library\default.dic not found or can not to read, path: library/default.dic
2019-09-12 10:07:28,304  INFO org.apache.spark.ContextCleaner 54 - Cleaned accumulator 48
2019-09-12 10:07:29,085  INFO org.ansj.library.DATDictionary 90 - init core library ok use time : 937
2019-09-12 10:07:29,522  INFO org.ansj.library.NgramLibrary 17 - init ngram ok use time :437
2019-09-12 10:07:29,586  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
2019-09-12 10:07:29,586  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 1.0 (TID 1) in 1860 ms on localhost (executor driver) (1/1)
2019-09-12 10:07:29,586  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-09-12 10:07:29,586  INFO org.apache.spark.scheduler.DAGScheduler 54 - ShuffleMapStage 1 (count at SparkUtil.java:41) finished in 1.860 s
2019-09-12 10:07:29,586  INFO org.apache.spark.scheduler.DAGScheduler 54 - looking for newly runnable stages
2019-09-12 10:07:29,586  INFO org.apache.spark.scheduler.DAGScheduler 54 - running: Set()
2019-09-12 10:07:29,586  INFO org.apache.spark.scheduler.DAGScheduler 54 - waiting: Set(ResultStage 2)
2019-09-12 10:07:29,601  INFO org.apache.spark.scheduler.DAGScheduler 54 - failed: Set()
2019-09-12 10:07:29,601  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 2 (MapPartitionsRDD[9] at count at SparkUtil.java:41), which has no missing parents
2019-09-12 10:07:29,601  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_4 stored as values in memory (estimated size 7.0 KB, free 899.4 MB)
2019-09-12 10:07:29,617  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.7 KB, free 899.4 MB)
2019-09-12 10:07:29,617  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_4_piece0 in memory on 10.6.1.14:4174 (size: 3.7 KB, free: 899.7 MB)
2019-09-12 10:07:29,617  INFO org.apache.spark.SparkContext 54 - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
2019-09-12 10:07:29,617  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at count at SparkUtil.java:41) (first 15 tasks are for partitions Vector(0))
2019-09-12 10:07:29,617  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 2.0 with 1 tasks
2019-09-12 10:07:29,617  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4726 bytes)
2019-09-12 10:07:29,617  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 2.0 (TID 2)
2019-09-12 10:07:29,632  INFO org.apache.spark.storage.ShuffleBlockFetcherIterator 54 - Getting 1 non-empty blocks out of 1 blocks
2019-09-12 10:07:29,648  INFO org.apache.spark.storage.ShuffleBlockFetcherIterator 54 - Started 0 remote fetches in 16 ms
2019-09-12 10:07:29,664  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 2.0 (TID 2). 1581 bytes result sent to driver
2019-09-12 10:07:29,664  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 2.0 (TID 2) in 47 ms on localhost (executor driver) (1/1)
2019-09-12 10:07:29,664  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-09-12 10:07:29,664  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 2 (count at SparkUtil.java:41) finished in 0.047 s
2019-09-12 10:07:29,664  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 1 finished: count at SparkUtil.java:41, took 1.981824 s
2019-09-12 10:07:29,695  INFO org.apache.spark.SparkContext 54 - Invoking stop() from shutdown hook
2019-09-12 10:07:29,695  INFO org.spark_project.jetty.server.AbstractConnector 310 - Stopped Spark@2e029d61{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 10:07:29,695  INFO org.apache.spark.ui.SparkUI 54 - Stopped Spark web UI at http://10.6.1.14:4042
2019-09-12 10:07:29,710  INFO org.apache.spark.MapOutputTrackerMasterEndpoint 54 - MapOutputTrackerMasterEndpoint stopped!
2019-09-12 10:07:29,773  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore cleared
2019-09-12 10:07:29,773  INFO org.apache.spark.storage.BlockManager 54 - BlockManager stopped
2019-09-12 10:07:29,773  INFO org.apache.spark.storage.BlockManagerMaster 54 - BlockManagerMaster stopped
2019-09-12 10:07:29,773  INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint 54 - OutputCommitCoordinator stopped!
2019-09-12 10:07:29,773  INFO org.apache.spark.SparkContext 54 - Successfully stopped SparkContext
2019-09-12 10:07:29,773  INFO org.apache.spark.util.ShutdownHookManager 54 - Shutdown hook called
2019-09-12 10:07:29,773  INFO org.apache.spark.util.ShutdownHookManager 54 - Deleting directory C:\Users\OUY\AppData\Local\Temp\spark-f3cb3c89-e914-461f-affa-f36cd7289ba5
2019-09-12 10:18:37,571  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-12 10:18:38,448  WARN org.apache.hadoop.util.NativeCodeLoader 62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-12 10:18:38,532 ERROR org.apache.hadoop.util.Shell 396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2430)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
	at com.datamodel.SparkUtil.CreateSession(SparkUtil.java:19)
	at com.datamodel.SparkUtil.readDataFromCsv(SparkUtil.java:24)
	at com.datamodel.SparkUtil.main(SparkUtil.java:37)
2019-09-12 10:18:38,662  INFO org.apache.spark.SparkContext 54 - Submitted application: splitCHWord
2019-09-12 10:18:38,700  INFO org.apache.spark.SecurityManager 54 - Changing view acls to: OUY
2019-09-12 10:18:38,701  INFO org.apache.spark.SecurityManager 54 - Changing modify acls to: OUY
2019-09-12 10:18:38,702  INFO org.apache.spark.SecurityManager 54 - Changing view acls groups to: 
2019-09-12 10:18:38,704  INFO org.apache.spark.SecurityManager 54 - Changing modify acls groups to: 
2019-09-12 10:18:38,705  INFO org.apache.spark.SecurityManager 54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(OUY); groups with view permissions: Set(); users  with modify permissions: Set(OUY); groups with modify permissions: Set()
2019-09-12 10:18:39,601  INFO org.apache.spark.util.Utils 54 - Successfully started service 'sparkDriver' on port 4292.
2019-09-12 10:18:39,693  INFO org.apache.spark.SparkEnv 54 - Registering MapOutputTracker
2019-09-12 10:18:39,728  INFO org.apache.spark.SparkEnv 54 - Registering BlockManagerMaster
2019-09-12 10:18:39,733  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-09-12 10:18:39,734  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - BlockManagerMasterEndpoint up
2019-09-12 10:18:39,753  INFO org.apache.spark.storage.DiskBlockManager 54 - Created local directory at C:\Users\OUY\AppData\Local\Temp\blockmgr-9cde595c-5e62-411c-8866-0f9a7e68659d
2019-09-12 10:18:39,794  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore started with capacity 899.7 MB
2019-09-12 10:18:39,956  INFO org.apache.spark.SparkEnv 54 - Registering OutputCommitCoordinator
2019-09-12 10:18:40,076  INFO org.spark_project.jetty.util.log 192 - Logging initialized @4887ms
2019-09-12 10:18:40,144  INFO org.spark_project.jetty.server.Server 345 - jetty-9.3.z-SNAPSHOT
2019-09-12 10:18:40,161  INFO org.spark_project.jetty.server.Server 403 - Started @4973ms
2019-09-12 10:18:40,189  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2019-09-12 10:18:40,190  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
2019-09-12 10:18:40,201  INFO org.spark_project.jetty.server.AbstractConnector 270 - Started ServerConnector@148976cd{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 10:18:40,202  INFO org.apache.spark.util.Utils 54 - Successfully started service 'SparkUI' on port 4042.
2019-09-12 10:18:40,241  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@319988b0{/jobs,null,AVAILABLE,@Spark}
2019-09-12 10:18:40,242  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4c168660{/jobs/json,null,AVAILABLE,@Spark}
2019-09-12 10:18:40,244  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@fd0e5b6{/jobs/job,null,AVAILABLE,@Spark}
2019-09-12 10:18:40,247  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4fad94a7{/jobs/job/json,null,AVAILABLE,@Spark}
2019-09-12 10:18:40,248  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6326d182{/stages,null,AVAILABLE,@Spark}
2019-09-12 10:18:40,250  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@716a7124{/stages/json,null,AVAILABLE,@Spark}
2019-09-12 10:18:40,250  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@226642a5{/stages/stage,null,AVAILABLE,@Spark}
2019-09-12 10:18:40,252  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@72bd06ca{/stages/stage/json,null,AVAILABLE,@Spark}
2019-09-12 10:18:40,254  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5dbe30be{/stages/pool,null,AVAILABLE,@Spark}
2019-09-12 10:18:40,255  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@55795845{/stages/pool/json,null,AVAILABLE,@Spark}
2019-09-12 10:18:40,257  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@119f1f2a{/storage,null,AVAILABLE,@Spark}
2019-09-12 10:18:40,258  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5b970f7{/storage/json,null,AVAILABLE,@Spark}
2019-09-12 10:18:40,259  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@165b8a71{/storage/rdd,null,AVAILABLE,@Spark}
2019-09-12 10:18:40,261  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2f058b8a{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-09-12 10:18:40,262  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3f2ef586{/environment,null,AVAILABLE,@Spark}
2019-09-12 10:18:40,264  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@76c7beb3{/environment/json,null,AVAILABLE,@Spark}
2019-09-12 10:18:40,268  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2cf92cc7{/executors,null,AVAILABLE,@Spark}
2019-09-12 10:18:40,269  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7b139eab{/executors/json,null,AVAILABLE,@Spark}
2019-09-12 10:18:40,271  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@611df6e3{/executors/threadDump,null,AVAILABLE,@Spark}
2019-09-12 10:18:40,272  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6273c5a4{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-09-12 10:18:40,282  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@53e211ee{/static,null,AVAILABLE,@Spark}
2019-09-12 10:18:40,283  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4a9f80d3{/,null,AVAILABLE,@Spark}
2019-09-12 10:18:40,284  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@41fe9859{/api,null,AVAILABLE,@Spark}
2019-09-12 10:18:40,286  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@53ab0286{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-09-12 10:18:40,288  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2d10e0b1{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-09-12 10:18:40,292  INFO org.apache.spark.ui.SparkUI 54 - Bound SparkUI to 0.0.0.0, and started at http://10.6.1.14:4042
2019-09-12 10:18:40,627  INFO org.apache.spark.executor.Executor 54 - Starting executor ID driver on host localhost
2019-09-12 10:18:40,683  INFO org.apache.spark.util.Utils 54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 4307.
2019-09-12 10:18:40,684  INFO org.apache.spark.network.netty.NettyBlockTransferService 54 - Server created on 10.6.1.14:4307
2019-09-12 10:18:40,694  INFO org.apache.spark.storage.BlockManager 54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-09-12 10:18:40,697  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registering BlockManager BlockManagerId(driver, 10.6.1.14, 4307, None)
2019-09-12 10:18:40,704  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Registering block manager 10.6.1.14:4307 with 899.7 MB RAM, BlockManagerId(driver, 10.6.1.14, 4307, None)
2019-09-12 10:18:40,712  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registered BlockManager BlockManagerId(driver, 10.6.1.14, 4307, None)
2019-09-12 10:18:40,714  INFO org.apache.spark.storage.BlockManager 54 - Initialized BlockManager: BlockManagerId(driver, 10.6.1.14, 4307, None)
2019-09-12 10:18:41,021  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6c2f1700{/metrics/json,null,AVAILABLE,@Spark}
2019-09-12 10:18:41,140  INFO org.apache.spark.sql.internal.SharedState 54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/MyProject/splitCHword/spark-warehouse/').
2019-09-12 10:18:41,142  INFO org.apache.spark.sql.internal.SharedState 54 - Warehouse path is 'file:/D:/MyProject/splitCHword/spark-warehouse/'.
2019-09-12 10:18:41,176  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@36a7abe1{/SQL,null,AVAILABLE,@Spark}
2019-09-12 10:18:41,178  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@e044b4a{/SQL/json,null,AVAILABLE,@Spark}
2019-09-12 10:18:41,180  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4604b900{/SQL/execution,null,AVAILABLE,@Spark}
2019-09-12 10:18:41,183  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@e36bb2a{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-09-12 10:18:41,189  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1500e009{/static/sql,null,AVAILABLE,@Spark}
2019-09-12 10:18:42,501  INFO org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef 54 - Registered StateStoreCoordinator endpoint
2019-09-12 10:18:42,876  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0 stored as values in memory (estimated size 219.8 KB, free 899.5 MB)
2019-09-12 10:18:42,981  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 10:18:42,987  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_0_piece0 in memory on 10.6.1.14:4307 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 10:18:42,992  INFO org.apache.spark.SparkContext 54 - Created broadcast 0 from load at SparkUtil.java:28
2019-09-12 10:18:43,377  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 10:18:43,379  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 10:18:43,395  INFO org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 413 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 326
2019-09-12 10:18:43,479  INFO org.apache.spark.SparkContext 54 - Starting job: load at SparkUtil.java:28
2019-09-12 10:18:43,507  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 0 (load at SparkUtil.java:28) with 1 output partitions
2019-09-12 10:18:43,508  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 0 (load at SparkUtil.java:28)
2019-09-12 10:18:43,509  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 10:18:43,511  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 10:18:43,519  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:28), which has no missing parents
2019-09-12 10:18:43,575  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1 stored as values in memory (estimated size 5.5 KB, free 899.5 MB)
2019-09-12 10:18:43,600  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 899.5 MB)
2019-09-12 10:18:43,608  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_1_piece0 in memory on 10.6.1.14:4307 (size: 3.4 KB, free: 899.7 MB)
2019-09-12 10:18:43,610  INFO org.apache.spark.SparkContext 54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-09-12 10:18:43,675  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:28) (first 15 tasks are for partitions Vector(0))
2019-09-12 10:18:43,681  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 0.0 with 1 tasks
2019-09-12 10:18:43,784  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes)
2019-09-12 10:18:43,804  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 0.0 (TID 0)
2019-09-12 10:18:43,904  INFO org.apache.spark.rdd.BinaryFileRDD 54 - Input split: Paths:/D:/MyProject/splitCHword/target/classes/test.csv:0+326
2019-09-12 10:18:44,239  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 0.0 (TID 0). 896 bytes result sent to driver
2019-09-12 10:18:44,253  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 0.0 (TID 0) in 498 ms on localhost (executor driver) (1/1)
2019-09-12 10:18:44,259  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-09-12 10:18:44,268  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 0 (load at SparkUtil.java:28) finished in 0.551 s
2019-09-12 10:18:44,278  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 0 finished: load at SparkUtil.java:28, took 0.797359 s
2019-09-12 10:18:44,898  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_1_piece0 on 10.6.1.14:4307 in memory (size: 3.4 KB, free: 899.7 MB)
2019-09-12 10:18:47,194  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_0_piece0 on 10.6.1.14:4307 in memory (size: 20.7 KB, free: 899.7 MB)
2019-09-12 10:18:47,236  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Pruning directories with: 
2019-09-12 10:18:47,239  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Post-Scan Filters: 
2019-09-12 10:18:47,242  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Output Data Schema: struct<feature: string>
2019-09-12 10:18:47,252  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Pushed Filters: 
2019-09-12 10:18:47,808  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 278.860135 ms
2019-09-12 10:18:47,817  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2 stored as values in memory (estimated size 220.5 KB, free 899.5 MB)
2019-09-12 10:18:47,839  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 10:18:47,842  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_2_piece0 in memory on 10.6.1.14:4307 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 10:18:47,844  INFO org.apache.spark.SparkContext 54 - Created broadcast 2 from show at SparkUtil.java:40
2019-09-12 10:18:47,864  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2019-09-12 10:18:47,930  INFO org.apache.spark.SparkContext 54 - Starting job: show at SparkUtil.java:40
2019-09-12 10:18:47,931  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 1 (show at SparkUtil.java:40) with 1 output partitions
2019-09-12 10:18:47,932  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 1 (show at SparkUtil.java:40)
2019-09-12 10:18:47,932  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 10:18:47,932  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 10:18:47,933  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:40), which has no missing parents
2019-09-12 10:18:47,958  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3 stored as values in memory (estimated size 14.2 KB, free 899.5 MB)
2019-09-12 10:18:47,964  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.9 KB, free 899.4 MB)
2019-09-12 10:18:47,966  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_3_piece0 in memory on 10.6.1.14:4307 (size: 6.9 KB, free: 899.7 MB)
2019-09-12 10:18:47,967  INFO org.apache.spark.SparkContext 54 - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2019-09-12 10:18:47,968  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:40) (first 15 tasks are for partitions Vector(0))
2019-09-12 10:18:47,968  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 1.0 with 1 tasks
2019-09-12 10:18:47,977  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5293 bytes)
2019-09-12 10:18:47,978  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 1.0 (TID 1)
2019-09-12 10:18:48,000  INFO org.apache.spark.sql.execution.datasources.FileScanRDD 54 - Reading File path: file:///D:/MyProject/splitCHword/target/classes/test.csv, range: 0-326, partition values: [empty row]
2019-09-12 10:18:48,019  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 13.387082 ms
2019-09-12 10:18:48,290  INFO org.ansj.util.MyStaticValue 106 - init version.number to env value is : 2.11.8
2019-09-12 10:18:48,290  INFO org.ansj.util.MyStaticValue 106 - init copyright.string to env value is : Copyright 2002-2016, LAMP/EPFL
2019-09-12 10:18:48,291  INFO org.ansj.util.MyStaticValue 106 - init osgi.version.number to env value is : 2.11.8.v20160304-115712-1706a37eb8
2019-09-12 10:18:48,291  INFO org.ansj.util.MyStaticValue 106 - init maven.version.number to env value is : 2.11.8
2019-09-12 10:18:48,295  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/ambiguity.dic
2019-09-12 10:18:48,295 ERROR org.ansj.library.AmbiguityLibrary 117 - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:D:\MyProject\splitCHword\library\ambiguity.dic not found or can not to read, path: library/ambiguity.dic
2019-09-12 10:18:48,297  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/default.dic
2019-09-12 10:18:48,297 ERROR org.ansj.library.DicLibrary 221 - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:D:\MyProject\splitCHword\library\default.dic not found or can not to read, path: library/default.dic
2019-09-12 10:18:49,589  INFO org.ansj.library.DATDictionary 90 - init core library ok use time : 1183
2019-09-12 10:18:50,118  INFO org.ansj.library.NgramLibrary 17 - init ngram ok use time :524
2019-09-12 10:18:50,129  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 1.0 (TID 1). 1386 bytes result sent to driver
2019-09-12 10:18:50,131  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 1.0 (TID 1) in 2162 ms on localhost (executor driver) (1/1)
2019-09-12 10:18:50,131  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-09-12 10:18:50,132  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 1 (show at SparkUtil.java:40) finished in 2.162 s
2019-09-12 10:18:50,132  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 1 finished: show at SparkUtil.java:40, took 2.201766 s
2019-09-12 10:18:50,156  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 12.761163 ms
2019-09-12 10:18:50,178  INFO org.apache.spark.SparkContext 54 - Invoking stop() from shutdown hook
2019-09-12 10:18:50,188  INFO org.spark_project.jetty.server.AbstractConnector 310 - Stopped Spark@148976cd{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 10:18:50,191  INFO org.apache.spark.ui.SparkUI 54 - Stopped Spark web UI at http://10.6.1.14:4042
2019-09-12 10:18:50,207  INFO org.apache.spark.MapOutputTrackerMasterEndpoint 54 - MapOutputTrackerMasterEndpoint stopped!
2019-09-12 10:18:50,245  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore cleared
2019-09-12 10:18:50,245  INFO org.apache.spark.storage.BlockManager 54 - BlockManager stopped
2019-09-12 10:18:50,248  INFO org.apache.spark.storage.BlockManagerMaster 54 - BlockManagerMaster stopped
2019-09-12 10:18:50,252  INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint 54 - OutputCommitCoordinator stopped!
2019-09-12 10:18:50,257  INFO org.apache.spark.SparkContext 54 - Successfully stopped SparkContext
2019-09-12 10:18:50,258  INFO org.apache.spark.util.ShutdownHookManager 54 - Shutdown hook called
2019-09-12 10:18:50,259  INFO org.apache.spark.util.ShutdownHookManager 54 - Deleting directory C:\Users\OUY\AppData\Local\Temp\spark-e280288e-7db0-444d-98c6-e644d3959a8e
2019-09-12 10:20:10,858  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-12 10:20:11,469  WARN org.apache.hadoop.util.NativeCodeLoader 62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-12 10:20:11,558 ERROR org.apache.hadoop.util.Shell 396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2430)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
	at com.datamodel.SparkUtil.CreateSession(SparkUtil.java:19)
	at com.datamodel.SparkUtil.readDataFromCsv(SparkUtil.java:24)
	at com.datamodel.SparkUtil.main(SparkUtil.java:37)
2019-09-12 10:20:11,725  INFO org.apache.spark.SparkContext 54 - Submitted application: splitCHWord
2019-09-12 10:20:11,775  INFO org.apache.spark.SecurityManager 54 - Changing view acls to: OUY
2019-09-12 10:20:11,776  INFO org.apache.spark.SecurityManager 54 - Changing modify acls to: OUY
2019-09-12 10:20:11,778  INFO org.apache.spark.SecurityManager 54 - Changing view acls groups to: 
2019-09-12 10:20:11,780  INFO org.apache.spark.SecurityManager 54 - Changing modify acls groups to: 
2019-09-12 10:20:11,781  INFO org.apache.spark.SecurityManager 54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(OUY); groups with view permissions: Set(); users  with modify permissions: Set(OUY); groups with modify permissions: Set()
2019-09-12 10:20:12,606  INFO org.apache.spark.util.Utils 54 - Successfully started service 'sparkDriver' on port 4337.
2019-09-12 10:20:12,666  INFO org.apache.spark.SparkEnv 54 - Registering MapOutputTracker
2019-09-12 10:20:12,688  INFO org.apache.spark.SparkEnv 54 - Registering BlockManagerMaster
2019-09-12 10:20:12,691  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-09-12 10:20:12,692  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - BlockManagerMasterEndpoint up
2019-09-12 10:20:12,705  INFO org.apache.spark.storage.DiskBlockManager 54 - Created local directory at C:\Users\OUY\AppData\Local\Temp\blockmgr-9ad2e33b-472d-4cb6-8d4a-2dbd93c43330
2019-09-12 10:20:12,732  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore started with capacity 899.7 MB
2019-09-12 10:20:12,804  INFO org.apache.spark.SparkEnv 54 - Registering OutputCommitCoordinator
2019-09-12 10:20:12,918  INFO org.spark_project.jetty.util.log 192 - Logging initialized @4597ms
2019-09-12 10:20:12,987  INFO org.spark_project.jetty.server.Server 345 - jetty-9.3.z-SNAPSHOT
2019-09-12 10:20:13,005  INFO org.spark_project.jetty.server.Server 403 - Started @4686ms
2019-09-12 10:20:13,023  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2019-09-12 10:20:13,024  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
2019-09-12 10:20:13,033  INFO org.spark_project.jetty.server.AbstractConnector 270 - Started ServerConnector@1bbf3e54{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 10:20:13,033  INFO org.apache.spark.util.Utils 54 - Successfully started service 'SparkUI' on port 4042.
2019-09-12 10:20:13,067  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@319988b0{/jobs,null,AVAILABLE,@Spark}
2019-09-12 10:20:13,068  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4c168660{/jobs/json,null,AVAILABLE,@Spark}
2019-09-12 10:20:13,069  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@fd0e5b6{/jobs/job,null,AVAILABLE,@Spark}
2019-09-12 10:20:13,071  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4fad94a7{/jobs/job/json,null,AVAILABLE,@Spark}
2019-09-12 10:20:13,072  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6326d182{/stages,null,AVAILABLE,@Spark}
2019-09-12 10:20:13,073  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@716a7124{/stages/json,null,AVAILABLE,@Spark}
2019-09-12 10:20:13,075  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@226642a5{/stages/stage,null,AVAILABLE,@Spark}
2019-09-12 10:20:13,077  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@72bd06ca{/stages/stage/json,null,AVAILABLE,@Spark}
2019-09-12 10:20:13,079  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5dbe30be{/stages/pool,null,AVAILABLE,@Spark}
2019-09-12 10:20:13,080  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@55795845{/stages/pool/json,null,AVAILABLE,@Spark}
2019-09-12 10:20:13,081  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@119f1f2a{/storage,null,AVAILABLE,@Spark}
2019-09-12 10:20:13,082  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5b970f7{/storage/json,null,AVAILABLE,@Spark}
2019-09-12 10:20:13,083  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@165b8a71{/storage/rdd,null,AVAILABLE,@Spark}
2019-09-12 10:20:13,084  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2f058b8a{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-09-12 10:20:13,088  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3f2ef586{/environment,null,AVAILABLE,@Spark}
2019-09-12 10:20:13,089  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@76c7beb3{/environment/json,null,AVAILABLE,@Spark}
2019-09-12 10:20:13,091  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2cf92cc7{/executors,null,AVAILABLE,@Spark}
2019-09-12 10:20:13,092  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7b139eab{/executors/json,null,AVAILABLE,@Spark}
2019-09-12 10:20:13,093  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@611df6e3{/executors/threadDump,null,AVAILABLE,@Spark}
2019-09-12 10:20:13,095  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6273c5a4{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-09-12 10:20:13,105  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@53e211ee{/static,null,AVAILABLE,@Spark}
2019-09-12 10:20:13,108  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4a9f80d3{/,null,AVAILABLE,@Spark}
2019-09-12 10:20:13,110  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@41fe9859{/api,null,AVAILABLE,@Spark}
2019-09-12 10:20:13,111  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@53ab0286{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-09-12 10:20:13,112  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2d10e0b1{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-09-12 10:20:13,116  INFO org.apache.spark.ui.SparkUI 54 - Bound SparkUI to 0.0.0.0, and started at http://10.6.1.14:4042
2019-09-12 10:20:13,330  INFO org.apache.spark.executor.Executor 54 - Starting executor ID driver on host localhost
2019-09-12 10:20:13,368  INFO org.apache.spark.util.Utils 54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 4350.
2019-09-12 10:20:13,377  INFO org.apache.spark.network.netty.NettyBlockTransferService 54 - Server created on 10.6.1.14:4350
2019-09-12 10:20:13,378  INFO org.apache.spark.storage.BlockManager 54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-09-12 10:20:13,380  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registering BlockManager BlockManagerId(driver, 10.6.1.14, 4350, None)
2019-09-12 10:20:13,385  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Registering block manager 10.6.1.14:4350 with 899.7 MB RAM, BlockManagerId(driver, 10.6.1.14, 4350, None)
2019-09-12 10:20:13,390  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registered BlockManager BlockManagerId(driver, 10.6.1.14, 4350, None)
2019-09-12 10:20:13,390  INFO org.apache.spark.storage.BlockManager 54 - Initialized BlockManager: BlockManagerId(driver, 10.6.1.14, 4350, None)
2019-09-12 10:20:13,668  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6c2f1700{/metrics/json,null,AVAILABLE,@Spark}
2019-09-12 10:20:13,768  INFO org.apache.spark.sql.internal.SharedState 54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/MyProject/splitCHword/spark-warehouse/').
2019-09-12 10:20:13,770  INFO org.apache.spark.sql.internal.SharedState 54 - Warehouse path is 'file:/D:/MyProject/splitCHword/spark-warehouse/'.
2019-09-12 10:20:13,780  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@36a7abe1{/SQL,null,AVAILABLE,@Spark}
2019-09-12 10:20:13,781  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@e044b4a{/SQL/json,null,AVAILABLE,@Spark}
2019-09-12 10:20:13,783  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4604b900{/SQL/execution,null,AVAILABLE,@Spark}
2019-09-12 10:20:13,784  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@e36bb2a{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-09-12 10:20:13,788  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1500e009{/static/sql,null,AVAILABLE,@Spark}
2019-09-12 10:20:14,931  INFO org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef 54 - Registered StateStoreCoordinator endpoint
2019-09-12 10:20:15,262  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0 stored as values in memory (estimated size 219.8 KB, free 899.5 MB)
2019-09-12 10:20:15,355  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 10:20:15,360  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_0_piece0 in memory on 10.6.1.14:4350 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 10:20:15,364  INFO org.apache.spark.SparkContext 54 - Created broadcast 0 from load at SparkUtil.java:28
2019-09-12 10:20:15,730  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 10:20:15,733  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 10:20:15,749  INFO org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 413 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 326
2019-09-12 10:20:15,806  INFO org.apache.spark.SparkContext 54 - Starting job: load at SparkUtil.java:28
2019-09-12 10:20:15,828  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 0 (load at SparkUtil.java:28) with 1 output partitions
2019-09-12 10:20:15,829  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 0 (load at SparkUtil.java:28)
2019-09-12 10:20:15,829  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 10:20:15,831  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 10:20:15,837  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:28), which has no missing parents
2019-09-12 10:20:15,879  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1 stored as values in memory (estimated size 5.5 KB, free 899.5 MB)
2019-09-12 10:20:15,884  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 899.5 MB)
2019-09-12 10:20:15,887  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_1_piece0 in memory on 10.6.1.14:4350 (size: 3.4 KB, free: 899.7 MB)
2019-09-12 10:20:15,889  INFO org.apache.spark.SparkContext 54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-09-12 10:20:15,917  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:28) (first 15 tasks are for partitions Vector(0))
2019-09-12 10:20:15,919  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 0.0 with 1 tasks
2019-09-12 10:20:15,984  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes)
2019-09-12 10:20:15,998  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 0.0 (TID 0)
2019-09-12 10:20:16,098  INFO org.apache.spark.rdd.BinaryFileRDD 54 - Input split: Paths:/D:/MyProject/splitCHword/target/classes/test.csv:0+326
2019-09-12 10:20:16,276  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 0.0 (TID 0). 896 bytes result sent to driver
2019-09-12 10:20:16,288  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 0.0 (TID 0) in 321 ms on localhost (executor driver) (1/1)
2019-09-12 10:20:16,291  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-09-12 10:20:16,299  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 0 (load at SparkUtil.java:28) finished in 0.354 s
2019-09-12 10:20:16,306  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 0 finished: load at SparkUtil.java:28, took 0.500361 s
2019-09-12 10:20:16,912  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_1_piece0 on 10.6.1.14:4350 in memory (size: 3.4 KB, free: 899.7 MB)
2019-09-12 10:20:18,469  INFO org.apache.spark.SparkContext 54 - Invoking stop() from shutdown hook
2019-09-12 10:20:18,476  INFO org.spark_project.jetty.server.AbstractConnector 310 - Stopped Spark@1bbf3e54{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 10:20:18,479  INFO org.apache.spark.ui.SparkUI 54 - Stopped Spark web UI at http://10.6.1.14:4042
2019-09-12 10:20:18,507  INFO org.apache.spark.MapOutputTrackerMasterEndpoint 54 - MapOutputTrackerMasterEndpoint stopped!
2019-09-12 10:20:18,529  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore cleared
2019-09-12 10:20:18,530  INFO org.apache.spark.storage.BlockManager 54 - BlockManager stopped
2019-09-12 10:20:18,532  INFO org.apache.spark.storage.BlockManagerMaster 54 - BlockManagerMaster stopped
2019-09-12 10:20:18,535  INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint 54 - OutputCommitCoordinator stopped!
2019-09-12 10:20:18,542  INFO org.apache.spark.SparkContext 54 - Successfully stopped SparkContext
2019-09-12 10:20:18,542  INFO org.apache.spark.util.ShutdownHookManager 54 - Shutdown hook called
2019-09-12 10:20:18,544  INFO org.apache.spark.util.ShutdownHookManager 54 - Deleting directory C:\Users\OUY\AppData\Local\Temp\spark-86781573-60dc-4cea-8ecf-e0d4a977f7a4
2019-09-12 10:21:08,007  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-12 10:21:08,694  WARN org.apache.hadoop.util.NativeCodeLoader 62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-12 10:21:08,781 ERROR org.apache.hadoop.util.Shell 396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2430)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
	at com.datamodel.SparkUtil.CreateSession(SparkUtil.java:19)
	at com.datamodel.SparkUtil.readDataFromCsv(SparkUtil.java:24)
	at com.datamodel.SparkUtil.main(SparkUtil.java:37)
2019-09-12 10:21:08,910  INFO org.apache.spark.SparkContext 54 - Submitted application: splitCHWord
2019-09-12 10:21:08,956  INFO org.apache.spark.SecurityManager 54 - Changing view acls to: OUY
2019-09-12 10:21:08,957  INFO org.apache.spark.SecurityManager 54 - Changing modify acls to: OUY
2019-09-12 10:21:08,959  INFO org.apache.spark.SecurityManager 54 - Changing view acls groups to: 
2019-09-12 10:21:08,960  INFO org.apache.spark.SecurityManager 54 - Changing modify acls groups to: 
2019-09-12 10:21:08,962  INFO org.apache.spark.SecurityManager 54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(OUY); groups with view permissions: Set(); users  with modify permissions: Set(OUY); groups with modify permissions: Set()
2019-09-12 10:21:09,744  INFO org.apache.spark.util.Utils 54 - Successfully started service 'sparkDriver' on port 4375.
2019-09-12 10:21:09,805  INFO org.apache.spark.SparkEnv 54 - Registering MapOutputTracker
2019-09-12 10:21:09,828  INFO org.apache.spark.SparkEnv 54 - Registering BlockManagerMaster
2019-09-12 10:21:09,832  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-09-12 10:21:09,832  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - BlockManagerMasterEndpoint up
2019-09-12 10:21:09,845  INFO org.apache.spark.storage.DiskBlockManager 54 - Created local directory at C:\Users\OUY\AppData\Local\Temp\blockmgr-3c5f5c3b-4533-4a53-a279-8aa1cea56dc7
2019-09-12 10:21:09,872  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore started with capacity 899.7 MB
2019-09-12 10:21:09,940  INFO org.apache.spark.SparkEnv 54 - Registering OutputCommitCoordinator
2019-09-12 10:21:10,050  INFO org.spark_project.jetty.util.log 192 - Logging initialized @4654ms
2019-09-12 10:21:10,120  INFO org.spark_project.jetty.server.Server 345 - jetty-9.3.z-SNAPSHOT
2019-09-12 10:21:10,138  INFO org.spark_project.jetty.server.Server 403 - Started @4743ms
2019-09-12 10:21:10,154  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2019-09-12 10:21:10,155  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
2019-09-12 10:21:10,164  INFO org.spark_project.jetty.server.AbstractConnector 270 - Started ServerConnector@2e029d61{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 10:21:10,164  INFO org.apache.spark.util.Utils 54 - Successfully started service 'SparkUI' on port 4042.
2019-09-12 10:21:10,197  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@d5ae57e{/jobs,null,AVAILABLE,@Spark}
2019-09-12 10:21:10,198  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@52b56a3e{/jobs/json,null,AVAILABLE,@Spark}
2019-09-12 10:21:10,199  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4eed46ee{/jobs/job,null,AVAILABLE,@Spark}
2019-09-12 10:21:10,201  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@475835b1{/jobs/job/json,null,AVAILABLE,@Spark}
2019-09-12 10:21:10,202  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5241cf67{/stages,null,AVAILABLE,@Spark}
2019-09-12 10:21:10,203  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@77192705{/stages/json,null,AVAILABLE,@Spark}
2019-09-12 10:21:10,204  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7e809b79{/stages/stage,null,AVAILABLE,@Spark}
2019-09-12 10:21:10,207  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@89c10b7{/stages/stage/json,null,AVAILABLE,@Spark}
2019-09-12 10:21:10,208  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4fe89c24{/stages/pool,null,AVAILABLE,@Spark}
2019-09-12 10:21:10,210  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3d08f3f5{/stages/pool/json,null,AVAILABLE,@Spark}
2019-09-12 10:21:10,211  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1a1da881{/storage,null,AVAILABLE,@Spark}
2019-09-12 10:21:10,212  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7fd4acee{/storage/json,null,AVAILABLE,@Spark}
2019-09-12 10:21:10,213  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6175619b{/storage/rdd,null,AVAILABLE,@Spark}
2019-09-12 10:21:10,214  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@756cf158{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-09-12 10:21:10,216  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@751d3241{/environment,null,AVAILABLE,@Spark}
2019-09-12 10:21:10,217  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@64337702{/environment/json,null,AVAILABLE,@Spark}
2019-09-12 10:21:10,218  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@30ea8c23{/executors,null,AVAILABLE,@Spark}
2019-09-12 10:21:10,220  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4e76dac{/executors/json,null,AVAILABLE,@Spark}
2019-09-12 10:21:10,221  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5f2f577{/executors/threadDump,null,AVAILABLE,@Spark}
2019-09-12 10:21:10,222  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5d465e4b{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-09-12 10:21:10,234  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@41a90fa8{/static,null,AVAILABLE,@Spark}
2019-09-12 10:21:10,235  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@35beb15e{/,null,AVAILABLE,@Spark}
2019-09-12 10:21:10,237  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5ac86ba5{/api,null,AVAILABLE,@Spark}
2019-09-12 10:21:10,237  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@63c5efee{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-09-12 10:21:10,238  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1c98290c{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-09-12 10:21:10,241  INFO org.apache.spark.ui.SparkUI 54 - Bound SparkUI to 0.0.0.0, and started at http://10.6.1.14:4042
2019-09-12 10:21:10,440  INFO org.apache.spark.executor.Executor 54 - Starting executor ID driver on host localhost
2019-09-12 10:21:10,483  INFO org.apache.spark.util.Utils 54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 4388.
2019-09-12 10:21:10,484  INFO org.apache.spark.network.netty.NettyBlockTransferService 54 - Server created on 10.6.1.14:4388
2019-09-12 10:21:10,487  INFO org.apache.spark.storage.BlockManager 54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-09-12 10:21:10,497  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registering BlockManager BlockManagerId(driver, 10.6.1.14, 4388, None)
2019-09-12 10:21:10,504  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Registering block manager 10.6.1.14:4388 with 899.7 MB RAM, BlockManagerId(driver, 10.6.1.14, 4388, None)
2019-09-12 10:21:10,509  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registered BlockManager BlockManagerId(driver, 10.6.1.14, 4388, None)
2019-09-12 10:21:10,510  INFO org.apache.spark.storage.BlockManager 54 - Initialized BlockManager: BlockManagerId(driver, 10.6.1.14, 4388, None)
2019-09-12 10:21:10,824  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@350b3a17{/metrics/json,null,AVAILABLE,@Spark}
2019-09-12 10:21:10,983  INFO org.apache.spark.sql.internal.SharedState 54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/MyProject/splitCHword/spark-warehouse/').
2019-09-12 10:21:10,984  INFO org.apache.spark.sql.internal.SharedState 54 - Warehouse path is 'file:/D:/MyProject/splitCHword/spark-warehouse/'.
2019-09-12 10:21:11,005  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@11a82d0f{/SQL,null,AVAILABLE,@Spark}
2019-09-12 10:21:11,009  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3ae66c85{/SQL/json,null,AVAILABLE,@Spark}
2019-09-12 10:21:11,034  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3961a41a{/SQL/execution,null,AVAILABLE,@Spark}
2019-09-12 10:21:11,039  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@367795c7{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-09-12 10:21:11,043  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@29d334c{/static/sql,null,AVAILABLE,@Spark}
2019-09-12 10:21:12,205  INFO org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef 54 - Registered StateStoreCoordinator endpoint
2019-09-12 10:21:12,525  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0 stored as values in memory (estimated size 219.8 KB, free 899.5 MB)
2019-09-12 10:21:12,695  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 10:21:12,701  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_0_piece0 in memory on 10.6.1.14:4388 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 10:21:12,708  INFO org.apache.spark.SparkContext 54 - Created broadcast 0 from load at SparkUtil.java:28
2019-09-12 10:21:13,066  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 10:21:13,068  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 10:21:13,085  INFO org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 413 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 326
2019-09-12 10:21:13,142  INFO org.apache.spark.SparkContext 54 - Starting job: load at SparkUtil.java:28
2019-09-12 10:21:13,164  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 0 (load at SparkUtil.java:28) with 1 output partitions
2019-09-12 10:21:13,164  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 0 (load at SparkUtil.java:28)
2019-09-12 10:21:13,165  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 10:21:13,167  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 10:21:13,173  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:28), which has no missing parents
2019-09-12 10:21:13,218  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1 stored as values in memory (estimated size 5.5 KB, free 899.5 MB)
2019-09-12 10:21:13,224  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 899.5 MB)
2019-09-12 10:21:13,225  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_1_piece0 in memory on 10.6.1.14:4388 (size: 3.4 KB, free: 899.7 MB)
2019-09-12 10:21:13,226  INFO org.apache.spark.SparkContext 54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-09-12 10:21:13,253  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:28) (first 15 tasks are for partitions Vector(0))
2019-09-12 10:21:13,254  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 0.0 with 1 tasks
2019-09-12 10:21:13,322  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes)
2019-09-12 10:21:13,343  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 0.0 (TID 0)
2019-09-12 10:21:13,436  INFO org.apache.spark.rdd.BinaryFileRDD 54 - Input split: Paths:/D:/MyProject/splitCHword/target/classes/test.csv:0+326
2019-09-12 10:21:13,628  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 0.0 (TID 0). 896 bytes result sent to driver
2019-09-12 10:21:13,637  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 0.0 (TID 0) in 338 ms on localhost (executor driver) (1/1)
2019-09-12 10:21:13,640  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-09-12 10:21:13,645  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 0 (load at SparkUtil.java:28) finished in 0.371 s
2019-09-12 10:21:13,651  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 0 finished: load at SparkUtil.java:28, took 0.508808 s
2019-09-12 10:21:14,193  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_0_piece0 on 10.6.1.14:4388 in memory (size: 20.7 KB, free: 899.7 MB)
2019-09-12 10:21:14,199  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_1_piece0 on 10.6.1.14:4388 in memory (size: 3.4 KB, free: 899.7 MB)
2019-09-12 10:21:15,797  INFO org.apache.spark.SparkContext 54 - Invoking stop() from shutdown hook
2019-09-12 10:21:15,802  INFO org.spark_project.jetty.server.AbstractConnector 310 - Stopped Spark@2e029d61{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 10:21:15,805  INFO org.apache.spark.ui.SparkUI 54 - Stopped Spark web UI at http://10.6.1.14:4042
2019-09-12 10:21:15,817  INFO org.apache.spark.MapOutputTrackerMasterEndpoint 54 - MapOutputTrackerMasterEndpoint stopped!
2019-09-12 10:21:15,840  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore cleared
2019-09-12 10:21:15,841  INFO org.apache.spark.storage.BlockManager 54 - BlockManager stopped
2019-09-12 10:21:15,843  INFO org.apache.spark.storage.BlockManagerMaster 54 - BlockManagerMaster stopped
2019-09-12 10:21:15,846  INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint 54 - OutputCommitCoordinator stopped!
2019-09-12 10:21:15,852  INFO org.apache.spark.SparkContext 54 - Successfully stopped SparkContext
2019-09-12 10:21:15,852  INFO org.apache.spark.util.ShutdownHookManager 54 - Shutdown hook called
2019-09-12 10:21:15,853  INFO org.apache.spark.util.ShutdownHookManager 54 - Deleting directory C:\Users\OUY\AppData\Local\Temp\spark-3864424a-1776-44e9-8991-f2b08573b7b8
2019-09-12 10:27:38,541  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-12 10:27:39,235  WARN org.apache.hadoop.util.NativeCodeLoader 62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-12 10:27:39,332 ERROR org.apache.hadoop.util.Shell 396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2430)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
	at com.datamodel.SparkUtil.CreateSession(SparkUtil.java:19)
	at com.datamodel.SparkUtil.readDataFromCsv(SparkUtil.java:24)
	at com.datamodel.SparkUtil.main(SparkUtil.java:37)
2019-09-12 10:27:39,517  INFO org.apache.spark.SparkContext 54 - Submitted application: splitCHWord
2019-09-12 10:27:39,559  INFO org.apache.spark.SecurityManager 54 - Changing view acls to: OUY
2019-09-12 10:27:39,560  INFO org.apache.spark.SecurityManager 54 - Changing modify acls to: OUY
2019-09-12 10:27:39,561  INFO org.apache.spark.SecurityManager 54 - Changing view acls groups to: 
2019-09-12 10:27:39,562  INFO org.apache.spark.SecurityManager 54 - Changing modify acls groups to: 
2019-09-12 10:27:39,563  INFO org.apache.spark.SecurityManager 54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(OUY); groups with view permissions: Set(); users  with modify permissions: Set(OUY); groups with modify permissions: Set()
2019-09-12 10:27:40,403  INFO org.apache.spark.util.Utils 54 - Successfully started service 'sparkDriver' on port 4502.
2019-09-12 10:27:40,484  INFO org.apache.spark.SparkEnv 54 - Registering MapOutputTracker
2019-09-12 10:27:40,518  INFO org.apache.spark.SparkEnv 54 - Registering BlockManagerMaster
2019-09-12 10:27:40,524  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-09-12 10:27:40,526  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - BlockManagerMasterEndpoint up
2019-09-12 10:27:40,543  INFO org.apache.spark.storage.DiskBlockManager 54 - Created local directory at C:\Users\OUY\AppData\Local\Temp\blockmgr-7def27d6-3e83-49e2-94b2-7223d20dbf3a
2019-09-12 10:27:40,574  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore started with capacity 899.7 MB
2019-09-12 10:27:40,660  INFO org.apache.spark.SparkEnv 54 - Registering OutputCommitCoordinator
2019-09-12 10:27:40,773  INFO org.spark_project.jetty.util.log 192 - Logging initialized @4715ms
2019-09-12 10:27:40,841  INFO org.spark_project.jetty.server.Server 345 - jetty-9.3.z-SNAPSHOT
2019-09-12 10:27:40,857  INFO org.spark_project.jetty.server.Server 403 - Started @4801ms
2019-09-12 10:27:40,877  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2019-09-12 10:27:40,878  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
2019-09-12 10:27:40,887  INFO org.spark_project.jetty.server.AbstractConnector 270 - Started ServerConnector@1643d68f{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 10:27:40,887  INFO org.apache.spark.util.Utils 54 - Successfully started service 'SparkUI' on port 4042.
2019-09-12 10:27:40,921  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@410ae9a3{/jobs,null,AVAILABLE,@Spark}
2019-09-12 10:27:40,921  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3c0fae6c{/jobs/json,null,AVAILABLE,@Spark}
2019-09-12 10:27:40,922  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@52b56a3e{/jobs/job,null,AVAILABLE,@Spark}
2019-09-12 10:27:40,925  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@36b0fcd5{/jobs/job/json,null,AVAILABLE,@Spark}
2019-09-12 10:27:40,927  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@475835b1{/stages,null,AVAILABLE,@Spark}
2019-09-12 10:27:40,929  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5241cf67{/stages/json,null,AVAILABLE,@Spark}
2019-09-12 10:27:40,932  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@77192705{/stages/stage,null,AVAILABLE,@Spark}
2019-09-12 10:27:40,934  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@625e134e{/stages/stage/json,null,AVAILABLE,@Spark}
2019-09-12 10:27:40,935  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@89c10b7{/stages/pool,null,AVAILABLE,@Spark}
2019-09-12 10:27:40,937  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4fe89c24{/stages/pool/json,null,AVAILABLE,@Spark}
2019-09-12 10:27:40,938  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3d08f3f5{/storage,null,AVAILABLE,@Spark}
2019-09-12 10:27:40,939  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1a1da881{/storage/json,null,AVAILABLE,@Spark}
2019-09-12 10:27:40,941  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7fd4acee{/storage/rdd,null,AVAILABLE,@Spark}
2019-09-12 10:27:40,942  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6175619b{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-09-12 10:27:40,944  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@756cf158{/environment,null,AVAILABLE,@Spark}
2019-09-12 10:27:40,945  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@751d3241{/environment/json,null,AVAILABLE,@Spark}
2019-09-12 10:27:40,946  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@64337702{/executors,null,AVAILABLE,@Spark}
2019-09-12 10:27:40,948  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@30ea8c23{/executors/json,null,AVAILABLE,@Spark}
2019-09-12 10:27:40,949  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4e76dac{/executors/threadDump,null,AVAILABLE,@Spark}
2019-09-12 10:27:40,950  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5f2f577{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-09-12 10:27:40,960  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5d465e4b{/static,null,AVAILABLE,@Spark}
2019-09-12 10:27:40,961  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@70dd7e15{/,null,AVAILABLE,@Spark}
2019-09-12 10:27:40,964  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@35beb15e{/api,null,AVAILABLE,@Spark}
2019-09-12 10:27:40,965  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@9635fa{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-09-12 10:27:40,966  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@63c5efee{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-09-12 10:27:40,969  INFO org.apache.spark.ui.SparkUI 54 - Bound SparkUI to 0.0.0.0, and started at http://10.6.1.14:4042
2019-09-12 10:27:41,171  INFO org.apache.spark.executor.Executor 54 - Starting executor ID driver on host localhost
2019-09-12 10:27:41,230  INFO org.apache.spark.util.Utils 54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 4515.
2019-09-12 10:27:41,231  INFO org.apache.spark.network.netty.NettyBlockTransferService 54 - Server created on 10.6.1.14:4515
2019-09-12 10:27:41,234  INFO org.apache.spark.storage.BlockManager 54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-09-12 10:27:41,244  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registering BlockManager BlockManagerId(driver, 10.6.1.14, 4515, None)
2019-09-12 10:27:41,249  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Registering block manager 10.6.1.14:4515 with 899.7 MB RAM, BlockManagerId(driver, 10.6.1.14, 4515, None)
2019-09-12 10:27:41,255  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registered BlockManager BlockManagerId(driver, 10.6.1.14, 4515, None)
2019-09-12 10:27:41,258  INFO org.apache.spark.storage.BlockManager 54 - Initialized BlockManager: BlockManagerId(driver, 10.6.1.14, 4515, None)
2019-09-12 10:27:41,679  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@79c3f01f{/metrics/json,null,AVAILABLE,@Spark}
2019-09-12 10:27:41,787  INFO org.apache.spark.sql.internal.SharedState 54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/MyProject/splitCHword/spark-warehouse/').
2019-09-12 10:27:41,790  INFO org.apache.spark.sql.internal.SharedState 54 - Warehouse path is 'file:/D:/MyProject/splitCHword/spark-warehouse/'.
2019-09-12 10:27:41,802  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@15515c51{/SQL,null,AVAILABLE,@Spark}
2019-09-12 10:27:41,803  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@64a896b0{/SQL/json,null,AVAILABLE,@Spark}
2019-09-12 10:27:41,806  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@16943e88{/SQL/execution,null,AVAILABLE,@Spark}
2019-09-12 10:27:41,809  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@73d6d0c{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-09-12 10:27:41,812  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3956b302{/static/sql,null,AVAILABLE,@Spark}
2019-09-12 10:27:43,103  INFO org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef 54 - Registered StateStoreCoordinator endpoint
2019-09-12 10:27:43,471  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0 stored as values in memory (estimated size 219.8 KB, free 899.5 MB)
2019-09-12 10:27:43,573  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 10:27:43,578  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_0_piece0 in memory on 10.6.1.14:4515 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 10:27:43,584  INFO org.apache.spark.SparkContext 54 - Created broadcast 0 from load at SparkUtil.java:28
2019-09-12 10:27:43,955  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 10:27:43,958  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 10:27:43,974  INFO org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 413 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 326
2019-09-12 10:27:44,033  INFO org.apache.spark.SparkContext 54 - Starting job: load at SparkUtil.java:28
2019-09-12 10:27:44,069  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 0 (load at SparkUtil.java:28) with 1 output partitions
2019-09-12 10:27:44,070  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 0 (load at SparkUtil.java:28)
2019-09-12 10:27:44,070  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 10:27:44,072  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 10:27:44,078  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:28), which has no missing parents
2019-09-12 10:27:44,098  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1 stored as values in memory (estimated size 5.5 KB, free 899.5 MB)
2019-09-12 10:27:44,105  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 899.5 MB)
2019-09-12 10:27:44,107  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_1_piece0 in memory on 10.6.1.14:4515 (size: 3.4 KB, free: 899.7 MB)
2019-09-12 10:27:44,108  INFO org.apache.spark.SparkContext 54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-09-12 10:27:44,140  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:28) (first 15 tasks are for partitions Vector(0))
2019-09-12 10:27:44,141  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 0.0 with 1 tasks
2019-09-12 10:27:44,201  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes)
2019-09-12 10:27:44,215  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 0.0 (TID 0)
2019-09-12 10:27:44,308  INFO org.apache.spark.rdd.BinaryFileRDD 54 - Input split: Paths:/D:/MyProject/splitCHword/target/classes/test.csv:0+326
2019-09-12 10:27:44,511  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 0.0 (TID 0). 853 bytes result sent to driver
2019-09-12 10:27:44,520  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 0.0 (TID 0) in 335 ms on localhost (executor driver) (1/1)
2019-09-12 10:27:44,522  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-09-12 10:27:44,528  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 0 (load at SparkUtil.java:28) finished in 0.365 s
2019-09-12 10:27:44,534  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 0 finished: load at SparkUtil.java:28, took 0.500642 s
2019-09-12 10:27:45,049  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_1_piece0 on 10.6.1.14:4515 in memory (size: 3.4 KB, free: 899.7 MB)
2019-09-12 10:27:45,054  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_0_piece0 on 10.6.1.14:4515 in memory (size: 20.7 KB, free: 899.7 MB)
2019-09-12 10:27:46,796  INFO org.apache.spark.SparkContext 54 - Invoking stop() from shutdown hook
2019-09-12 10:27:46,801  INFO org.spark_project.jetty.server.AbstractConnector 310 - Stopped Spark@1643d68f{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 10:27:46,804  INFO org.apache.spark.ui.SparkUI 54 - Stopped Spark web UI at http://10.6.1.14:4042
2019-09-12 10:27:46,818  INFO org.apache.spark.MapOutputTrackerMasterEndpoint 54 - MapOutputTrackerMasterEndpoint stopped!
2019-09-12 10:27:46,841  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore cleared
2019-09-12 10:27:46,841  INFO org.apache.spark.storage.BlockManager 54 - BlockManager stopped
2019-09-12 10:27:46,843  INFO org.apache.spark.storage.BlockManagerMaster 54 - BlockManagerMaster stopped
2019-09-12 10:27:46,846  INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint 54 - OutputCommitCoordinator stopped!
2019-09-12 10:27:46,852  INFO org.apache.spark.SparkContext 54 - Successfully stopped SparkContext
2019-09-12 10:27:46,853  INFO org.apache.spark.util.ShutdownHookManager 54 - Shutdown hook called
2019-09-12 10:27:46,854  INFO org.apache.spark.util.ShutdownHookManager 54 - Deleting directory C:\Users\OUY\AppData\Local\Temp\spark-20207c43-1a6d-49e8-a420-26371a8c3702
2019-09-12 10:57:32,965  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-12 10:57:33,605  WARN org.apache.hadoop.util.NativeCodeLoader 62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-12 10:57:33,777 ERROR org.apache.hadoop.util.Shell 396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2430)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
	at com.datamodel.SparkUtil.CreateSession(SparkUtil.java:25)
	at com.datamodel.SparkUtil.readDataFromCsv(SparkUtil.java:30)
	at com.datamodel.SparkUtil.main(SparkUtil.java:43)
2019-09-12 10:57:33,902  INFO org.apache.spark.SparkContext 54 - Submitted application: splitCHWord
2019-09-12 10:57:33,949  INFO org.apache.spark.SecurityManager 54 - Changing view acls to: OUY
2019-09-12 10:57:33,949  INFO org.apache.spark.SecurityManager 54 - Changing modify acls to: OUY
2019-09-12 10:57:33,949  INFO org.apache.spark.SecurityManager 54 - Changing view acls groups to: 
2019-09-12 10:57:33,949  INFO org.apache.spark.SecurityManager 54 - Changing modify acls groups to: 
2019-09-12 10:57:33,949  INFO org.apache.spark.SecurityManager 54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(OUY); groups with view permissions: Set(); users  with modify permissions: Set(OUY); groups with modify permissions: Set()
2019-09-12 10:57:34,652  INFO org.apache.spark.util.Utils 54 - Successfully started service 'sparkDriver' on port 5045.
2019-09-12 10:57:34,698  INFO org.apache.spark.SparkEnv 54 - Registering MapOutputTracker
2019-09-12 10:57:34,730  INFO org.apache.spark.SparkEnv 54 - Registering BlockManagerMaster
2019-09-12 10:57:34,730  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-09-12 10:57:34,730  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - BlockManagerMasterEndpoint up
2019-09-12 10:57:34,745  INFO org.apache.spark.storage.DiskBlockManager 54 - Created local directory at C:\Users\OUY\AppData\Local\Temp\blockmgr-51881cf7-8d0b-4a11-a3f4-8f72ccc1b159
2019-09-12 10:57:34,777  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore started with capacity 899.7 MB
2019-09-12 10:57:34,839  INFO org.apache.spark.SparkEnv 54 - Registering OutputCommitCoordinator
2019-09-12 10:57:34,933  INFO org.spark_project.jetty.util.log 192 - Logging initialized @4060ms
2019-09-12 10:57:35,011  INFO org.spark_project.jetty.server.Server 345 - jetty-9.3.z-SNAPSHOT
2019-09-12 10:57:35,011  INFO org.spark_project.jetty.server.Server 403 - Started @4142ms
2019-09-12 10:57:35,042  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2019-09-12 10:57:35,042  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
2019-09-12 10:57:35,058  INFO org.spark_project.jetty.server.AbstractConnector 270 - Started ServerConnector@7c55f2c{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 10:57:35,058  INFO org.apache.spark.util.Utils 54 - Successfully started service 'SparkUI' on port 4042.
2019-09-12 10:57:35,105  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@305f031{/jobs,null,AVAILABLE,@Spark}
2019-09-12 10:57:35,105  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@36b0fcd5{/jobs/json,null,AVAILABLE,@Spark}
2019-09-12 10:57:35,120  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@475835b1{/jobs/job,null,AVAILABLE,@Spark}
2019-09-12 10:57:35,120  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@716a7124{/jobs/job/json,null,AVAILABLE,@Spark}
2019-09-12 10:57:35,120  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@226642a5{/stages,null,AVAILABLE,@Spark}
2019-09-12 10:57:35,120  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5cc126dc{/stages/json,null,AVAILABLE,@Spark}
2019-09-12 10:57:35,120  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@72bd06ca{/stages/stage,null,AVAILABLE,@Spark}
2019-09-12 10:57:35,136  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@55795845{/stages/stage/json,null,AVAILABLE,@Spark}
2019-09-12 10:57:35,136  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@119f1f2a{/stages/pool,null,AVAILABLE,@Spark}
2019-09-12 10:57:35,339  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5b970f7{/stages/pool/json,null,AVAILABLE,@Spark}
2019-09-12 10:57:35,339  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@165b8a71{/storage,null,AVAILABLE,@Spark}
2019-09-12 10:57:35,339  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2f058b8a{/storage/json,null,AVAILABLE,@Spark}
2019-09-12 10:57:35,339  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3f2ef586{/storage/rdd,null,AVAILABLE,@Spark}
2019-09-12 10:57:35,339  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@76c7beb3{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-09-12 10:57:35,339  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2cf92cc7{/environment,null,AVAILABLE,@Spark}
2019-09-12 10:57:35,339  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7b139eab{/environment/json,null,AVAILABLE,@Spark}
2019-09-12 10:57:35,339  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@611df6e3{/executors,null,AVAILABLE,@Spark}
2019-09-12 10:57:35,339  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6273c5a4{/executors/json,null,AVAILABLE,@Spark}
2019-09-12 10:57:35,339  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@53e211ee{/executors/threadDump,null,AVAILABLE,@Spark}
2019-09-12 10:57:35,339  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3d8bbcdc{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-09-12 10:57:35,355  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@117e0fe5{/static,null,AVAILABLE,@Spark}
2019-09-12 10:57:35,355  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6c67e137{/,null,AVAILABLE,@Spark}
2019-09-12 10:57:35,355  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@191ae03f{/api,null,AVAILABLE,@Spark}
2019-09-12 10:57:35,355  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@172ca72b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-09-12 10:57:35,355  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@71e5f61d{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-09-12 10:57:35,370  INFO org.apache.spark.ui.SparkUI 54 - Bound SparkUI to 0.0.0.0, and started at http://10.6.1.14:4042
2019-09-12 10:57:35,526  INFO org.apache.spark.executor.Executor 54 - Starting executor ID driver on host localhost
2019-09-12 10:57:35,573  INFO org.apache.spark.util.Utils 54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 5058.
2019-09-12 10:57:35,573  INFO org.apache.spark.network.netty.NettyBlockTransferService 54 - Server created on 10.6.1.14:5058
2019-09-12 10:57:35,573  INFO org.apache.spark.storage.BlockManager 54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-09-12 10:57:35,589  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registering BlockManager BlockManagerId(driver, 10.6.1.14, 5058, None)
2019-09-12 10:57:35,589  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Registering block manager 10.6.1.14:5058 with 899.7 MB RAM, BlockManagerId(driver, 10.6.1.14, 5058, None)
2019-09-12 10:57:35,605  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registered BlockManager BlockManagerId(driver, 10.6.1.14, 5058, None)
2019-09-12 10:57:35,605  INFO org.apache.spark.storage.BlockManager 54 - Initialized BlockManager: BlockManagerId(driver, 10.6.1.14, 5058, None)
2019-09-12 10:57:35,917  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@721eb7df{/metrics/json,null,AVAILABLE,@Spark}
2019-09-12 10:57:36,022  INFO org.apache.spark.sql.internal.SharedState 54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/MyProject/splitCHword/spark-warehouse/').
2019-09-12 10:57:36,024  INFO org.apache.spark.sql.internal.SharedState 54 - Warehouse path is 'file:/D:/MyProject/splitCHword/spark-warehouse/'.
2019-09-12 10:57:36,034  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1adb7478{/SQL,null,AVAILABLE,@Spark}
2019-09-12 10:57:36,035  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@16943e88{/SQL/json,null,AVAILABLE,@Spark}
2019-09-12 10:57:36,037  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5a4ed68f{/SQL/execution,null,AVAILABLE,@Spark}
2019-09-12 10:57:36,038  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@d2387c8{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-09-12 10:57:36,041  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@100f9bbe{/static/sql,null,AVAILABLE,@Spark}
2019-09-12 10:57:37,087  INFO org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef 54 - Registered StateStoreCoordinator endpoint
2019-09-12 10:57:37,368  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0 stored as values in memory (estimated size 219.8 KB, free 899.5 MB)
2019-09-12 10:57:37,509  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 10:57:37,524  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_0_piece0 in memory on 10.6.1.14:5058 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 10:57:37,524  INFO org.apache.spark.SparkContext 54 - Created broadcast 0 from load at SparkUtil.java:34
2019-09-12 10:57:37,837  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 10:57:37,852  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 10:57:37,868  INFO org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 413 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 326
2019-09-12 10:57:37,930  INFO org.apache.spark.SparkContext 54 - Starting job: load at SparkUtil.java:34
2019-09-12 10:57:37,946  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 0 (load at SparkUtil.java:34) with 1 output partitions
2019-09-12 10:57:37,946  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 0 (load at SparkUtil.java:34)
2019-09-12 10:57:37,946  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 10:57:37,946  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 10:57:37,962  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:34), which has no missing parents
2019-09-12 10:57:37,977  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1 stored as values in memory (estimated size 5.5 KB, free 899.5 MB)
2019-09-12 10:57:37,993  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 899.5 MB)
2019-09-12 10:57:37,993  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_1_piece0 in memory on 10.6.1.14:5058 (size: 3.4 KB, free: 899.7 MB)
2019-09-12 10:57:37,993  INFO org.apache.spark.SparkContext 54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-09-12 10:57:38,008  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:34) (first 15 tasks are for partitions Vector(0))
2019-09-12 10:57:38,024  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 0.0 with 1 tasks
2019-09-12 10:57:38,087  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes)
2019-09-12 10:57:38,102  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 0.0 (TID 0)
2019-09-12 10:57:38,196  INFO org.apache.spark.rdd.BinaryFileRDD 54 - Input split: Paths:/D:/MyProject/splitCHword/target/classes/test.csv:0+326
2019-09-12 10:57:38,368  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 0.0 (TID 0). 853 bytes result sent to driver
2019-09-12 10:57:38,383  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 0.0 (TID 0) in 312 ms on localhost (executor driver) (1/1)
2019-09-12 10:57:38,383  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-09-12 10:57:38,383  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 0 (load at SparkUtil.java:34) finished in 0.343 s
2019-09-12 10:57:38,399  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 0 finished: load at SparkUtil.java:34, took 0.465282 s
2019-09-12 10:57:38,805  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_1_piece0 on 10.6.1.14:5058 in memory (size: 3.4 KB, free: 899.7 MB)
2019-09-12 10:57:39,946  INFO org.apache.spark.SparkContext 54 - Invoking stop() from shutdown hook
2019-09-12 10:57:39,946  INFO org.spark_project.jetty.server.AbstractConnector 310 - Stopped Spark@7c55f2c{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 10:57:39,946  INFO org.apache.spark.ui.SparkUI 54 - Stopped Spark web UI at http://10.6.1.14:4042
2019-09-12 10:57:39,961  INFO org.apache.spark.MapOutputTrackerMasterEndpoint 54 - MapOutputTrackerMasterEndpoint stopped!
2019-09-12 10:57:39,992  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore cleared
2019-09-12 10:57:39,992  INFO org.apache.spark.storage.BlockManager 54 - BlockManager stopped
2019-09-12 10:57:39,992  INFO org.apache.spark.storage.BlockManagerMaster 54 - BlockManagerMaster stopped
2019-09-12 10:57:39,992  INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint 54 - OutputCommitCoordinator stopped!
2019-09-12 10:57:39,992  INFO org.apache.spark.SparkContext 54 - Successfully stopped SparkContext
2019-09-12 10:57:40,008  INFO org.apache.spark.util.ShutdownHookManager 54 - Shutdown hook called
2019-09-12 10:57:40,008  INFO org.apache.spark.util.ShutdownHookManager 54 - Deleting directory C:\Users\OUY\AppData\Local\Temp\spark-d5d42e52-373e-4dea-b060-7c1c65b65834
2019-09-12 10:59:00,029  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-12 10:59:00,832  WARN org.apache.hadoop.util.NativeCodeLoader 62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-12 10:59:00,926 ERROR org.apache.hadoop.util.Shell 396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2430)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
	at com.datamodel.SparkUtil.CreateSession(SparkUtil.java:25)
	at com.datamodel.SparkUtil.readDataFromCsv(SparkUtil.java:30)
	at com.datamodel.SparkUtil.main(SparkUtil.java:43)
2019-09-12 10:59:01,082  INFO org.apache.spark.SparkContext 54 - Submitted application: splitCHWord
2019-09-12 10:59:01,129  INFO org.apache.spark.SecurityManager 54 - Changing view acls to: OUY
2019-09-12 10:59:01,129  INFO org.apache.spark.SecurityManager 54 - Changing modify acls to: OUY
2019-09-12 10:59:01,144  INFO org.apache.spark.SecurityManager 54 - Changing view acls groups to: 
2019-09-12 10:59:01,144  INFO org.apache.spark.SecurityManager 54 - Changing modify acls groups to: 
2019-09-12 10:59:01,144  INFO org.apache.spark.SecurityManager 54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(OUY); groups with view permissions: Set(); users  with modify permissions: Set(OUY); groups with modify permissions: Set()
2019-09-12 10:59:02,113  INFO org.apache.spark.util.Utils 54 - Successfully started service 'sparkDriver' on port 5090.
2019-09-12 10:59:02,207  INFO org.apache.spark.SparkEnv 54 - Registering MapOutputTracker
2019-09-12 10:59:02,222  INFO org.apache.spark.SparkEnv 54 - Registering BlockManagerMaster
2019-09-12 10:59:02,222  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-09-12 10:59:02,222  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - BlockManagerMasterEndpoint up
2019-09-12 10:59:02,238  INFO org.apache.spark.storage.DiskBlockManager 54 - Created local directory at C:\Users\OUY\AppData\Local\Temp\blockmgr-593f9bd5-6f1b-4a81-93cd-c91f51e8b4b5
2019-09-12 10:59:02,253  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore started with capacity 899.7 MB
2019-09-12 10:59:02,316  INFO org.apache.spark.SparkEnv 54 - Registering OutputCommitCoordinator
2019-09-12 10:59:02,425  INFO org.spark_project.jetty.util.log 192 - Logging initialized @4401ms
2019-09-12 10:59:02,503  INFO org.spark_project.jetty.server.Server 345 - jetty-9.3.z-SNAPSHOT
2019-09-12 10:59:02,519  INFO org.spark_project.jetty.server.Server 403 - Started @4481ms
2019-09-12 10:59:02,519  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2019-09-12 10:59:02,519  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
2019-09-12 10:59:02,535  INFO org.spark_project.jetty.server.AbstractConnector 270 - Started ServerConnector@5439fad8{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 10:59:02,535  INFO org.apache.spark.util.Utils 54 - Successfully started service 'SparkUI' on port 4042.
2019-09-12 10:59:02,566  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@305f031{/jobs,null,AVAILABLE,@Spark}
2019-09-12 10:59:02,566  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@36b0fcd5{/jobs/json,null,AVAILABLE,@Spark}
2019-09-12 10:59:02,566  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@475835b1{/jobs/job,null,AVAILABLE,@Spark}
2019-09-12 10:59:02,566  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@716a7124{/jobs/job/json,null,AVAILABLE,@Spark}
2019-09-12 10:59:02,566  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@226642a5{/stages,null,AVAILABLE,@Spark}
2019-09-12 10:59:02,566  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5cc126dc{/stages/json,null,AVAILABLE,@Spark}
2019-09-12 10:59:02,566  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@72bd06ca{/stages/stage,null,AVAILABLE,@Spark}
2019-09-12 10:59:02,566  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@55795845{/stages/stage/json,null,AVAILABLE,@Spark}
2019-09-12 10:59:02,566  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@119f1f2a{/stages/pool,null,AVAILABLE,@Spark}
2019-09-12 10:59:02,566  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5b970f7{/stages/pool/json,null,AVAILABLE,@Spark}
2019-09-12 10:59:02,566  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@165b8a71{/storage,null,AVAILABLE,@Spark}
2019-09-12 10:59:02,581  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2f058b8a{/storage/json,null,AVAILABLE,@Spark}
2019-09-12 10:59:02,581  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3f2ef586{/storage/rdd,null,AVAILABLE,@Spark}
2019-09-12 10:59:02,581  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@76c7beb3{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-09-12 10:59:02,581  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2cf92cc7{/environment,null,AVAILABLE,@Spark}
2019-09-12 10:59:02,581  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7b139eab{/environment/json,null,AVAILABLE,@Spark}
2019-09-12 10:59:02,581  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@611df6e3{/executors,null,AVAILABLE,@Spark}
2019-09-12 10:59:02,581  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6273c5a4{/executors/json,null,AVAILABLE,@Spark}
2019-09-12 10:59:02,581  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@53e211ee{/executors/threadDump,null,AVAILABLE,@Spark}
2019-09-12 10:59:02,581  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3d8bbcdc{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-09-12 10:59:02,597  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@117e0fe5{/static,null,AVAILABLE,@Spark}
2019-09-12 10:59:02,597  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6c67e137{/,null,AVAILABLE,@Spark}
2019-09-12 10:59:02,597  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@191ae03f{/api,null,AVAILABLE,@Spark}
2019-09-12 10:59:02,597  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@172ca72b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-09-12 10:59:02,597  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@71e5f61d{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-09-12 10:59:02,597  INFO org.apache.spark.ui.SparkUI 54 - Bound SparkUI to 0.0.0.0, and started at http://10.6.1.14:4042
2019-09-12 10:59:02,753  INFO org.apache.spark.executor.Executor 54 - Starting executor ID driver on host localhost
2019-09-12 10:59:02,785  INFO org.apache.spark.util.Utils 54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 5103.
2019-09-12 10:59:02,785  INFO org.apache.spark.network.netty.NettyBlockTransferService 54 - Server created on 10.6.1.14:5103
2019-09-12 10:59:02,785  INFO org.apache.spark.storage.BlockManager 54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-09-12 10:59:02,800  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registering BlockManager BlockManagerId(driver, 10.6.1.14, 5103, None)
2019-09-12 10:59:02,800  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Registering block manager 10.6.1.14:5103 with 899.7 MB RAM, BlockManagerId(driver, 10.6.1.14, 5103, None)
2019-09-12 10:59:02,816  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registered BlockManager BlockManagerId(driver, 10.6.1.14, 5103, None)
2019-09-12 10:59:02,816  INFO org.apache.spark.storage.BlockManager 54 - Initialized BlockManager: BlockManagerId(driver, 10.6.1.14, 5103, None)
2019-09-12 10:59:03,097  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@721eb7df{/metrics/json,null,AVAILABLE,@Spark}
2019-09-12 10:59:03,175  INFO org.apache.spark.sql.internal.SharedState 54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/MyProject/splitCHword/spark-warehouse/').
2019-09-12 10:59:03,175  INFO org.apache.spark.sql.internal.SharedState 54 - Warehouse path is 'file:/D:/MyProject/splitCHword/spark-warehouse/'.
2019-09-12 10:59:03,191  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1adb7478{/SQL,null,AVAILABLE,@Spark}
2019-09-12 10:59:03,191  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@16943e88{/SQL/json,null,AVAILABLE,@Spark}
2019-09-12 10:59:03,191  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5a4ed68f{/SQL/execution,null,AVAILABLE,@Spark}
2019-09-12 10:59:03,191  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@d2387c8{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-09-12 10:59:03,191  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@100f9bbe{/static/sql,null,AVAILABLE,@Spark}
2019-09-12 10:59:04,534  INFO org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef 54 - Registered StateStoreCoordinator endpoint
2019-09-12 10:59:04,815  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0 stored as values in memory (estimated size 219.8 KB, free 899.5 MB)
2019-09-12 10:59:04,925  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 10:59:04,940  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_0_piece0 in memory on 10.6.1.14:5103 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 10:59:04,940  INFO org.apache.spark.SparkContext 54 - Created broadcast 0 from load at SparkUtil.java:34
2019-09-12 10:59:05,253  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 10:59:05,268  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 10:59:05,284  INFO org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 413 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 326
2019-09-12 10:59:05,331  INFO org.apache.spark.SparkContext 54 - Starting job: load at SparkUtil.java:34
2019-09-12 10:59:05,346  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 0 (load at SparkUtil.java:34) with 1 output partitions
2019-09-12 10:59:05,346  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 0 (load at SparkUtil.java:34)
2019-09-12 10:59:05,346  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 10:59:05,346  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 10:59:05,346  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:34), which has no missing parents
2019-09-12 10:59:05,378  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1 stored as values in memory (estimated size 5.5 KB, free 899.5 MB)
2019-09-12 10:59:05,378  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 899.5 MB)
2019-09-12 10:59:05,378  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_1_piece0 in memory on 10.6.1.14:5103 (size: 3.4 KB, free: 899.7 MB)
2019-09-12 10:59:05,378  INFO org.apache.spark.SparkContext 54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-09-12 10:59:05,409  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:34) (first 15 tasks are for partitions Vector(0))
2019-09-12 10:59:05,409  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 0.0 with 1 tasks
2019-09-12 10:59:05,456  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes)
2019-09-12 10:59:05,456  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 0.0 (TID 0)
2019-09-12 10:59:05,565  INFO org.apache.spark.rdd.BinaryFileRDD 54 - Input split: Paths:/D:/MyProject/splitCHword/target/classes/test.csv:0+326
2019-09-12 10:59:05,768  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 0.0 (TID 0). 853 bytes result sent to driver
2019-09-12 10:59:05,784  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 0.0 (TID 0) in 344 ms on localhost (executor driver) (1/1)
2019-09-12 10:59:05,784  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-09-12 10:59:05,784  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 0 (load at SparkUtil.java:34) finished in 0.359 s
2019-09-12 10:59:05,799  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 0 finished: load at SparkUtil.java:34, took 0.469120 s
2019-09-12 10:59:06,081  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_0_piece0 on 10.6.1.14:5103 in memory (size: 20.7 KB, free: 899.7 MB)
2019-09-12 10:59:06,081  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_1_piece0 on 10.6.1.14:5103 in memory (size: 3.4 KB, free: 899.7 MB)
2019-09-12 10:59:07,940  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Pruning directories with: 
2019-09-12 10:59:07,940  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Post-Scan Filters: 
2019-09-12 10:59:07,955  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Output Data Schema: struct<feature: string>
2019-09-12 10:59:07,955  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Pushed Filters: 
2019-09-12 10:59:08,564  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 263.771081 ms
2019-09-12 10:59:08,580  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2 stored as values in memory (estimated size 220.5 KB, free 899.5 MB)
2019-09-12 10:59:08,596  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 10:59:08,596  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_2_piece0 in memory on 10.6.1.14:5103 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 10:59:08,596  INFO org.apache.spark.SparkContext 54 - Created broadcast 2 from show at SparkUtil.java:55
2019-09-12 10:59:08,611  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2019-09-12 10:59:08,674  INFO org.apache.spark.SparkContext 54 - Starting job: show at SparkUtil.java:55
2019-09-12 10:59:08,674  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 1 (show at SparkUtil.java:55) with 1 output partitions
2019-09-12 10:59:08,674  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 1 (show at SparkUtil.java:55)
2019-09-12 10:59:08,674  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 10:59:08,674  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 10:59:08,674  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:55), which has no missing parents
2019-09-12 10:59:08,689  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3 stored as values in memory (estimated size 12.3 KB, free 899.5 MB)
2019-09-12 10:59:08,705  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.3 KB, free 899.4 MB)
2019-09-12 10:59:08,705  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_3_piece0 in memory on 10.6.1.14:5103 (size: 6.3 KB, free: 899.7 MB)
2019-09-12 10:59:08,705  INFO org.apache.spark.SparkContext 54 - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2019-09-12 10:59:08,705  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:55) (first 15 tasks are for partitions Vector(0))
2019-09-12 10:59:08,705  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 1.0 with 1 tasks
2019-09-12 10:59:08,705  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5293 bytes)
2019-09-12 10:59:08,705  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 1.0 (TID 1)
2019-09-12 10:59:08,736  INFO org.apache.spark.sql.execution.datasources.FileScanRDD 54 - Reading File path: file:///D:/MyProject/splitCHword/target/classes/test.csv, range: 0-326, partition values: [empty row]
2019-09-12 10:59:08,767  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 12.028576 ms
2019-09-12 10:59:09,017  INFO org.ansj.util.MyStaticValue 106 - init version.number to env value is : 2.11.8
2019-09-12 10:59:09,017  INFO org.ansj.util.MyStaticValue 106 - init copyright.string to env value is : Copyright 2002-2016, LAMP/EPFL
2019-09-12 10:59:09,017  INFO org.ansj.util.MyStaticValue 106 - init osgi.version.number to env value is : 2.11.8.v20160304-115712-1706a37eb8
2019-09-12 10:59:09,017  INFO org.ansj.util.MyStaticValue 106 - init maven.version.number to env value is : 2.11.8
2019-09-12 10:59:09,017  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/ambiguity.dic
2019-09-12 10:59:09,017 ERROR org.ansj.library.AmbiguityLibrary 117 - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:D:\MyProject\splitCHword\library\ambiguity.dic not found or can not to read, path: library/ambiguity.dic
2019-09-12 10:59:09,017  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/default.dic
2019-09-12 10:59:09,017 ERROR org.ansj.library.DicLibrary 221 - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:D:\MyProject\splitCHword\library\default.dic not found or can not to read, path: library/default.dic
2019-09-12 10:59:10,048  INFO org.ansj.library.DATDictionary 90 - init core library ok use time : 921
2019-09-12 10:59:10,455  INFO org.ansj.library.NgramLibrary 17 - init ngram ok use time :407
2019-09-12 10:59:10,470  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 1.0 (TID 1). 1230 bytes result sent to driver
2019-09-12 10:59:10,470  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 1.0 (TID 1) in 1765 ms on localhost (executor driver) (1/1)
2019-09-12 10:59:10,470  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 1 (show at SparkUtil.java:55) finished in 1.765 s
2019-09-12 10:59:10,470  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 1 finished: show at SparkUtil.java:55, took 1.796774 s
2019-09-12 10:59:10,470  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-09-12 10:59:10,486  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 8.464206 ms
2019-09-12 10:59:10,517  INFO org.apache.spark.SparkContext 54 - Invoking stop() from shutdown hook
2019-09-12 10:59:10,533  INFO org.spark_project.jetty.server.AbstractConnector 310 - Stopped Spark@5439fad8{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 10:59:10,533  INFO org.apache.spark.ui.SparkUI 54 - Stopped Spark web UI at http://10.6.1.14:4042
2019-09-12 10:59:10,548  INFO org.apache.spark.MapOutputTrackerMasterEndpoint 54 - MapOutputTrackerMasterEndpoint stopped!
2019-09-12 10:59:10,580  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore cleared
2019-09-12 10:59:10,642  INFO org.apache.spark.storage.BlockManager 54 - BlockManager stopped
2019-09-12 10:59:10,642  INFO org.apache.spark.storage.BlockManagerMaster 54 - BlockManagerMaster stopped
2019-09-12 10:59:10,642  INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint 54 - OutputCommitCoordinator stopped!
2019-09-12 10:59:10,642  INFO org.apache.spark.SparkContext 54 - Successfully stopped SparkContext
2019-09-12 10:59:10,642  INFO org.apache.spark.util.ShutdownHookManager 54 - Shutdown hook called
2019-09-12 10:59:10,658  INFO org.apache.spark.util.ShutdownHookManager 54 - Deleting directory C:\Users\OUY\AppData\Local\Temp\spark-8b990f91-e364-4a1b-87d7-25b0cc968eb7
2019-09-12 11:00:54,363  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-12 11:00:55,004  WARN org.apache.hadoop.util.NativeCodeLoader 62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-12 11:00:55,113 ERROR org.apache.hadoop.util.Shell 396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2430)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
	at com.datamodel.SparkUtil.CreateSession(SparkUtil.java:25)
	at com.datamodel.SparkUtil.readDataFromCsv(SparkUtil.java:30)
	at com.datamodel.SparkUtil.main(SparkUtil.java:43)
2019-09-12 11:00:55,223  INFO org.apache.spark.SparkContext 54 - Submitted application: splitCHWord
2019-09-12 11:00:55,254  INFO org.apache.spark.SecurityManager 54 - Changing view acls to: OUY
2019-09-12 11:00:55,254  INFO org.apache.spark.SecurityManager 54 - Changing modify acls to: OUY
2019-09-12 11:00:55,254  INFO org.apache.spark.SecurityManager 54 - Changing view acls groups to: 
2019-09-12 11:00:55,254  INFO org.apache.spark.SecurityManager 54 - Changing modify acls groups to: 
2019-09-12 11:00:55,254  INFO org.apache.spark.SecurityManager 54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(OUY); groups with view permissions: Set(); users  with modify permissions: Set(OUY); groups with modify permissions: Set()
2019-09-12 11:00:55,988  INFO org.apache.spark.util.Utils 54 - Successfully started service 'sparkDriver' on port 5145.
2019-09-12 11:00:56,051  INFO org.apache.spark.SparkEnv 54 - Registering MapOutputTracker
2019-09-12 11:00:56,066  INFO org.apache.spark.SparkEnv 54 - Registering BlockManagerMaster
2019-09-12 11:00:56,066  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-09-12 11:00:56,066  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - BlockManagerMasterEndpoint up
2019-09-12 11:00:56,082  INFO org.apache.spark.storage.DiskBlockManager 54 - Created local directory at C:\Users\OUY\AppData\Local\Temp\blockmgr-008d3ed2-61f8-46b7-ad99-c290530be7ec
2019-09-12 11:00:56,097  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore started with capacity 899.7 MB
2019-09-12 11:00:56,160  INFO org.apache.spark.SparkEnv 54 - Registering OutputCommitCoordinator
2019-09-12 11:00:56,254  INFO org.spark_project.jetty.util.log 192 - Logging initialized @3878ms
2019-09-12 11:00:56,332  INFO org.spark_project.jetty.server.Server 345 - jetty-9.3.z-SNAPSHOT
2019-09-12 11:00:56,347  INFO org.spark_project.jetty.server.Server 403 - Started @3968ms
2019-09-12 11:00:56,379  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2019-09-12 11:00:56,379  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
2019-09-12 11:00:56,394  INFO org.spark_project.jetty.server.AbstractConnector 270 - Started ServerConnector@6c25a0e3{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 11:00:56,394  INFO org.apache.spark.util.Utils 54 - Successfully started service 'SparkUI' on port 4042.
2019-09-12 11:00:56,488  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@305f031{/jobs,null,AVAILABLE,@Spark}
2019-09-12 11:00:56,488  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@36b0fcd5{/jobs/json,null,AVAILABLE,@Spark}
2019-09-12 11:00:56,488  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@475835b1{/jobs/job,null,AVAILABLE,@Spark}
2019-09-12 11:00:56,488  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@716a7124{/jobs/job/json,null,AVAILABLE,@Spark}
2019-09-12 11:00:56,504  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@226642a5{/stages,null,AVAILABLE,@Spark}
2019-09-12 11:00:56,504  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5cc126dc{/stages/json,null,AVAILABLE,@Spark}
2019-09-12 11:00:56,504  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@72bd06ca{/stages/stage,null,AVAILABLE,@Spark}
2019-09-12 11:00:56,504  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@55795845{/stages/stage/json,null,AVAILABLE,@Spark}
2019-09-12 11:00:56,504  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@119f1f2a{/stages/pool,null,AVAILABLE,@Spark}
2019-09-12 11:00:56,504  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5b970f7{/stages/pool/json,null,AVAILABLE,@Spark}
2019-09-12 11:00:56,504  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@165b8a71{/storage,null,AVAILABLE,@Spark}
2019-09-12 11:00:56,504  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2f058b8a{/storage/json,null,AVAILABLE,@Spark}
2019-09-12 11:00:56,504  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3f2ef586{/storage/rdd,null,AVAILABLE,@Spark}
2019-09-12 11:00:56,504  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@76c7beb3{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-09-12 11:00:56,504  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2cf92cc7{/environment,null,AVAILABLE,@Spark}
2019-09-12 11:00:56,504  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7b139eab{/environment/json,null,AVAILABLE,@Spark}
2019-09-12 11:00:56,519  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@611df6e3{/executors,null,AVAILABLE,@Spark}
2019-09-12 11:00:56,519  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6273c5a4{/executors/json,null,AVAILABLE,@Spark}
2019-09-12 11:00:56,519  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@53e211ee{/executors/threadDump,null,AVAILABLE,@Spark}
2019-09-12 11:00:56,519  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3d8bbcdc{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-09-12 11:00:56,535  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@117e0fe5{/static,null,AVAILABLE,@Spark}
2019-09-12 11:00:56,535  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6c67e137{/,null,AVAILABLE,@Spark}
2019-09-12 11:00:56,535  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@191ae03f{/api,null,AVAILABLE,@Spark}
2019-09-12 11:00:56,535  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@172ca72b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-09-12 11:00:56,535  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@71e5f61d{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-09-12 11:00:56,535  INFO org.apache.spark.ui.SparkUI 54 - Bound SparkUI to 0.0.0.0, and started at http://10.6.1.14:4042
2019-09-12 11:00:56,675  INFO org.apache.spark.executor.Executor 54 - Starting executor ID driver on host localhost
2019-09-12 11:00:56,707  INFO org.apache.spark.util.Utils 54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 5158.
2019-09-12 11:00:56,707  INFO org.apache.spark.network.netty.NettyBlockTransferService 54 - Server created on 10.6.1.14:5158
2019-09-12 11:00:56,707  INFO org.apache.spark.storage.BlockManager 54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-09-12 11:00:56,722  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registering BlockManager BlockManagerId(driver, 10.6.1.14, 5158, None)
2019-09-12 11:00:56,722  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Registering block manager 10.6.1.14:5158 with 899.7 MB RAM, BlockManagerId(driver, 10.6.1.14, 5158, None)
2019-09-12 11:00:56,738  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registered BlockManager BlockManagerId(driver, 10.6.1.14, 5158, None)
2019-09-12 11:00:56,738  INFO org.apache.spark.storage.BlockManager 54 - Initialized BlockManager: BlockManagerId(driver, 10.6.1.14, 5158, None)
2019-09-12 11:00:57,003  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@721eb7df{/metrics/json,null,AVAILABLE,@Spark}
2019-09-12 11:00:57,097  INFO org.apache.spark.sql.internal.SharedState 54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/MyProject/splitCHword/spark-warehouse/').
2019-09-12 11:00:57,113  INFO org.apache.spark.sql.internal.SharedState 54 - Warehouse path is 'file:/D:/MyProject/splitCHword/spark-warehouse/'.
2019-09-12 11:00:57,113  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1adb7478{/SQL,null,AVAILABLE,@Spark}
2019-09-12 11:00:57,113  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@16943e88{/SQL/json,null,AVAILABLE,@Spark}
2019-09-12 11:00:57,113  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5a4ed68f{/SQL/execution,null,AVAILABLE,@Spark}
2019-09-12 11:00:57,128  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@d2387c8{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-09-12 11:00:57,128  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@100f9bbe{/static/sql,null,AVAILABLE,@Spark}
2019-09-12 11:00:58,159  INFO org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef 54 - Registered StateStoreCoordinator endpoint
2019-09-12 11:00:58,456  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0 stored as values in memory (estimated size 219.8 KB, free 899.5 MB)
2019-09-12 11:00:58,550  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 11:00:58,566  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_0_piece0 in memory on 10.6.1.14:5158 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 11:00:58,566  INFO org.apache.spark.SparkContext 54 - Created broadcast 0 from load at SparkUtil.java:34
2019-09-12 11:00:58,894  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 11:00:58,894  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 11:00:58,909  INFO org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 413 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 326
2019-09-12 11:00:58,956  INFO org.apache.spark.SparkContext 54 - Starting job: load at SparkUtil.java:34
2019-09-12 11:00:58,972  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 0 (load at SparkUtil.java:34) with 1 output partitions
2019-09-12 11:00:58,972  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 0 (load at SparkUtil.java:34)
2019-09-12 11:00:58,972  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 11:00:58,972  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 11:00:59,003  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:34), which has no missing parents
2019-09-12 11:00:59,034  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1 stored as values in memory (estimated size 5.5 KB, free 899.5 MB)
2019-09-12 11:00:59,034  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 899.5 MB)
2019-09-12 11:00:59,034  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_1_piece0 in memory on 10.6.1.14:5158 (size: 3.4 KB, free: 899.7 MB)
2019-09-12 11:00:59,034  INFO org.apache.spark.SparkContext 54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-09-12 11:00:59,066  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:34) (first 15 tasks are for partitions Vector(0))
2019-09-12 11:00:59,066  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 0.0 with 1 tasks
2019-09-12 11:00:59,128  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes)
2019-09-12 11:00:59,144  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 0.0 (TID 0)
2019-09-12 11:00:59,237  INFO org.apache.spark.rdd.BinaryFileRDD 54 - Input split: Paths:/D:/MyProject/splitCHword/target/classes/test.csv:0+326
2019-09-12 11:00:59,425  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 0.0 (TID 0). 853 bytes result sent to driver
2019-09-12 11:00:59,440  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 0.0 (TID 0) in 328 ms on localhost (executor driver) (1/1)
2019-09-12 11:00:59,440  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-09-12 11:00:59,440  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 0 (load at SparkUtil.java:34) finished in 0.343 s
2019-09-12 11:00:59,440  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 0 finished: load at SparkUtil.java:34, took 0.490827 s
2019-09-12 11:00:59,956  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_1_piece0 on 10.6.1.14:5158 in memory (size: 3.4 KB, free: 899.7 MB)
2019-09-12 11:01:01,252  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Pruning directories with: 
2019-09-12 11:01:01,252  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Post-Scan Filters: 
2019-09-12 11:01:01,252  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Output Data Schema: struct<feature: string>
2019-09-12 11:01:01,268  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Pushed Filters: 
2019-09-12 11:01:01,456  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_0_piece0 on 10.6.1.14:5158 in memory (size: 20.7 KB, free: 899.7 MB)
2019-09-12 11:01:01,909  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 259.515938 ms
2019-09-12 11:01:01,909  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2 stored as values in memory (estimated size 220.5 KB, free 899.5 MB)
2019-09-12 11:01:01,940  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 11:01:01,940  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_2_piece0 in memory on 10.6.1.14:5158 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 11:01:01,955  INFO org.apache.spark.SparkContext 54 - Created broadcast 2 from show at SparkUtil.java:56
2019-09-12 11:01:01,955  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2019-09-12 11:01:02,018  INFO org.apache.spark.SparkContext 54 - Starting job: show at SparkUtil.java:56
2019-09-12 11:01:02,018  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 1 (show at SparkUtil.java:56) with 1 output partitions
2019-09-12 11:01:02,018  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 1 (show at SparkUtil.java:56)
2019-09-12 11:01:02,018  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 11:01:02,018  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 11:01:02,018  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:56), which has no missing parents
2019-09-12 11:01:02,049  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3 stored as values in memory (estimated size 12.3 KB, free 899.5 MB)
2019-09-12 11:01:02,049  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.3 KB, free 899.4 MB)
2019-09-12 11:01:02,049  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_3_piece0 in memory on 10.6.1.14:5158 (size: 6.3 KB, free: 899.7 MB)
2019-09-12 11:01:02,049  INFO org.apache.spark.SparkContext 54 - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2019-09-12 11:01:02,049  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:56) (first 15 tasks are for partitions Vector(0))
2019-09-12 11:01:02,049  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 1.0 with 1 tasks
2019-09-12 11:01:02,065  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5293 bytes)
2019-09-12 11:01:02,065  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 1.0 (TID 1)
2019-09-12 11:01:02,096  INFO org.apache.spark.sql.execution.datasources.FileScanRDD 54 - Reading File path: file:///D:/MyProject/splitCHword/target/classes/test.csv, range: 0-326, partition values: [empty row]
2019-09-12 11:01:02,112  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 13.535989 ms
2019-09-12 11:01:02,362  INFO org.ansj.util.MyStaticValue 106 - init version.number to env value is : 2.11.8
2019-09-12 11:01:02,362  INFO org.ansj.util.MyStaticValue 106 - init copyright.string to env value is : Copyright 2002-2016, LAMP/EPFL
2019-09-12 11:01:02,362  INFO org.ansj.util.MyStaticValue 106 - init osgi.version.number to env value is : 2.11.8.v20160304-115712-1706a37eb8
2019-09-12 11:01:02,362  INFO org.ansj.util.MyStaticValue 106 - init maven.version.number to env value is : 2.11.8
2019-09-12 11:01:02,362  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/ambiguity.dic
2019-09-12 11:01:02,362 ERROR org.ansj.library.AmbiguityLibrary 117 - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:D:\MyProject\splitCHword\library\ambiguity.dic not found or can not to read, path: library/ambiguity.dic
2019-09-12 11:01:02,362  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/default.dic
2019-09-12 11:01:02,362 ERROR org.ansj.library.DicLibrary 221 - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:D:\MyProject\splitCHword\library\default.dic not found or can not to read, path: library/default.dic
2019-09-12 11:01:03,377  INFO org.ansj.library.DATDictionary 90 - init core library ok use time : 922
2019-09-12 11:01:03,799  INFO org.ansj.library.NgramLibrary 17 - init ngram ok use time :422
2019-09-12 11:01:03,814  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 1.0 (TID 1). 1144 bytes result sent to driver
2019-09-12 11:01:03,814  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 1.0 (TID 1) in 1749 ms on localhost (executor driver) (1/1)
2019-09-12 11:01:03,814  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-09-12 11:01:03,814  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 1 (show at SparkUtil.java:56) finished in 1.749 s
2019-09-12 11:01:03,814  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 1 finished: show at SparkUtil.java:56, took 1.798122 s
2019-09-12 11:01:03,846  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 11.458124 ms
2019-09-12 11:01:03,877  INFO org.apache.spark.SparkContext 54 - Invoking stop() from shutdown hook
2019-09-12 11:01:03,892  INFO org.spark_project.jetty.server.AbstractConnector 310 - Stopped Spark@6c25a0e3{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 11:01:03,892  INFO org.apache.spark.ui.SparkUI 54 - Stopped Spark web UI at http://10.6.1.14:4042
2019-09-12 11:01:03,955  INFO org.apache.spark.MapOutputTrackerMasterEndpoint 54 - MapOutputTrackerMasterEndpoint stopped!
2019-09-12 11:01:03,986  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore cleared
2019-09-12 11:01:03,986  INFO org.apache.spark.storage.BlockManager 54 - BlockManager stopped
2019-09-12 11:01:03,986  INFO org.apache.spark.storage.BlockManagerMaster 54 - BlockManagerMaster stopped
2019-09-12 11:01:03,986  INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint 54 - OutputCommitCoordinator stopped!
2019-09-12 11:01:04,002  INFO org.apache.spark.SparkContext 54 - Successfully stopped SparkContext
2019-09-12 11:01:04,002  INFO org.apache.spark.util.ShutdownHookManager 54 - Shutdown hook called
2019-09-12 11:01:04,002  INFO org.apache.spark.util.ShutdownHookManager 54 - Deleting directory C:\Users\OUY\AppData\Local\Temp\spark-e8f1fa78-10f1-4bbe-bd28-b1edfd8ac878
2019-09-12 11:02:50,017  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-12 11:02:50,657  WARN org.apache.hadoop.util.NativeCodeLoader 62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-12 11:02:50,798 ERROR org.apache.hadoop.util.Shell 396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2430)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
	at com.datamodel.SparkUtil.CreateSession(SparkUtil.java:25)
	at com.datamodel.SparkUtil.readDataFromCsv(SparkUtil.java:30)
	at com.datamodel.SparkUtil.main(SparkUtil.java:43)
2019-09-12 11:02:50,923  INFO org.apache.spark.SparkContext 54 - Submitted application: splitCHWord
2019-09-12 11:02:50,954  INFO org.apache.spark.SecurityManager 54 - Changing view acls to: OUY
2019-09-12 11:02:50,954  INFO org.apache.spark.SecurityManager 54 - Changing modify acls to: OUY
2019-09-12 11:02:50,954  INFO org.apache.spark.SecurityManager 54 - Changing view acls groups to: 
2019-09-12 11:02:50,954  INFO org.apache.spark.SecurityManager 54 - Changing modify acls groups to: 
2019-09-12 11:02:50,954  INFO org.apache.spark.SecurityManager 54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(OUY); groups with view permissions: Set(); users  with modify permissions: Set(OUY); groups with modify permissions: Set()
2019-09-12 11:02:51,641  INFO org.apache.spark.util.Utils 54 - Successfully started service 'sparkDriver' on port 5195.
2019-09-12 11:02:51,704  INFO org.apache.spark.SparkEnv 54 - Registering MapOutputTracker
2019-09-12 11:02:51,719  INFO org.apache.spark.SparkEnv 54 - Registering BlockManagerMaster
2019-09-12 11:02:51,735  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-09-12 11:02:51,735  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - BlockManagerMasterEndpoint up
2019-09-12 11:02:51,735  INFO org.apache.spark.storage.DiskBlockManager 54 - Created local directory at C:\Users\OUY\AppData\Local\Temp\blockmgr-8f9aebd4-da4e-4051-819b-28545293538f
2019-09-12 11:02:51,766  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore started with capacity 899.7 MB
2019-09-12 11:02:51,813  INFO org.apache.spark.SparkEnv 54 - Registering OutputCommitCoordinator
2019-09-12 11:02:51,907  INFO org.spark_project.jetty.util.log 192 - Logging initialized @3876ms
2019-09-12 11:02:52,032  INFO org.spark_project.jetty.server.Server 345 - jetty-9.3.z-SNAPSHOT
2019-09-12 11:02:52,047  INFO org.spark_project.jetty.server.Server 403 - Started @4017ms
2019-09-12 11:02:52,063  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2019-09-12 11:02:52,063  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
2019-09-12 11:02:52,063  INFO org.spark_project.jetty.server.AbstractConnector 270 - Started ServerConnector@60843569{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 11:02:52,063  INFO org.apache.spark.util.Utils 54 - Successfully started service 'SparkUI' on port 4042.
2019-09-12 11:02:52,110  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@305f031{/jobs,null,AVAILABLE,@Spark}
2019-09-12 11:02:52,110  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@36b0fcd5{/jobs/json,null,AVAILABLE,@Spark}
2019-09-12 11:02:52,110  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@475835b1{/jobs/job,null,AVAILABLE,@Spark}
2019-09-12 11:02:52,110  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@716a7124{/jobs/job/json,null,AVAILABLE,@Spark}
2019-09-12 11:02:52,110  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@226642a5{/stages,null,AVAILABLE,@Spark}
2019-09-12 11:02:52,110  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5cc126dc{/stages/json,null,AVAILABLE,@Spark}
2019-09-12 11:02:52,110  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@72bd06ca{/stages/stage,null,AVAILABLE,@Spark}
2019-09-12 11:02:52,110  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@55795845{/stages/stage/json,null,AVAILABLE,@Spark}
2019-09-12 11:02:52,110  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@119f1f2a{/stages/pool,null,AVAILABLE,@Spark}
2019-09-12 11:02:52,110  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5b970f7{/stages/pool/json,null,AVAILABLE,@Spark}
2019-09-12 11:02:52,110  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@165b8a71{/storage,null,AVAILABLE,@Spark}
2019-09-12 11:02:52,110  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2f058b8a{/storage/json,null,AVAILABLE,@Spark}
2019-09-12 11:02:52,110  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3f2ef586{/storage/rdd,null,AVAILABLE,@Spark}
2019-09-12 11:02:52,110  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@76c7beb3{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-09-12 11:02:52,110  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2cf92cc7{/environment,null,AVAILABLE,@Spark}
2019-09-12 11:02:52,126  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7b139eab{/environment/json,null,AVAILABLE,@Spark}
2019-09-12 11:02:52,126  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@611df6e3{/executors,null,AVAILABLE,@Spark}
2019-09-12 11:02:52,126  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6273c5a4{/executors/json,null,AVAILABLE,@Spark}
2019-09-12 11:02:52,126  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@53e211ee{/executors/threadDump,null,AVAILABLE,@Spark}
2019-09-12 11:02:52,126  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3d8bbcdc{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-09-12 11:02:52,141  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@117e0fe5{/static,null,AVAILABLE,@Spark}
2019-09-12 11:02:52,141  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6c67e137{/,null,AVAILABLE,@Spark}
2019-09-12 11:02:52,141  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@191ae03f{/api,null,AVAILABLE,@Spark}
2019-09-12 11:02:52,141  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@172ca72b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-09-12 11:02:52,141  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@71e5f61d{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-09-12 11:02:52,141  INFO org.apache.spark.ui.SparkUI 54 - Bound SparkUI to 0.0.0.0, and started at http://10.6.1.14:4042
2019-09-12 11:02:52,297  INFO org.apache.spark.executor.Executor 54 - Starting executor ID driver on host localhost
2019-09-12 11:02:52,344  INFO org.apache.spark.util.Utils 54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 5208.
2019-09-12 11:02:52,344  INFO org.apache.spark.network.netty.NettyBlockTransferService 54 - Server created on 10.6.1.14:5208
2019-09-12 11:02:52,344  INFO org.apache.spark.storage.BlockManager 54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-09-12 11:02:52,360  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registering BlockManager BlockManagerId(driver, 10.6.1.14, 5208, None)
2019-09-12 11:02:52,360  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Registering block manager 10.6.1.14:5208 with 899.7 MB RAM, BlockManagerId(driver, 10.6.1.14, 5208, None)
2019-09-12 11:02:52,376  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registered BlockManager BlockManagerId(driver, 10.6.1.14, 5208, None)
2019-09-12 11:02:52,376  INFO org.apache.spark.storage.BlockManager 54 - Initialized BlockManager: BlockManagerId(driver, 10.6.1.14, 5208, None)
2019-09-12 11:02:52,657  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@721eb7df{/metrics/json,null,AVAILABLE,@Spark}
2019-09-12 11:02:52,766  INFO org.apache.spark.sql.internal.SharedState 54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/MyProject/splitCHword/spark-warehouse/').
2019-09-12 11:02:52,766  INFO org.apache.spark.sql.internal.SharedState 54 - Warehouse path is 'file:/D:/MyProject/splitCHword/spark-warehouse/'.
2019-09-12 11:02:52,766  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1adb7478{/SQL,null,AVAILABLE,@Spark}
2019-09-12 11:02:52,766  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@16943e88{/SQL/json,null,AVAILABLE,@Spark}
2019-09-12 11:02:52,766  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5a4ed68f{/SQL/execution,null,AVAILABLE,@Spark}
2019-09-12 11:02:52,766  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@d2387c8{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-09-12 11:02:52,782  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@100f9bbe{/static/sql,null,AVAILABLE,@Spark}
2019-09-12 11:02:53,797  INFO org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef 54 - Registered StateStoreCoordinator endpoint
2019-09-12 11:02:54,078  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0 stored as values in memory (estimated size 219.8 KB, free 899.5 MB)
2019-09-12 11:02:54,172  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 11:02:54,172  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_0_piece0 in memory on 10.6.1.14:5208 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 11:02:54,188  INFO org.apache.spark.SparkContext 54 - Created broadcast 0 from load at SparkUtil.java:34
2019-09-12 11:02:54,516  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 11:02:54,516  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 11:02:54,531  INFO org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 413 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 326
2019-09-12 11:02:54,594  INFO org.apache.spark.SparkContext 54 - Starting job: load at SparkUtil.java:34
2019-09-12 11:02:54,625  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 0 (load at SparkUtil.java:34) with 1 output partitions
2019-09-12 11:02:54,625  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 0 (load at SparkUtil.java:34)
2019-09-12 11:02:54,625  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 11:02:54,625  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 11:02:54,625  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:34), which has no missing parents
2019-09-12 11:02:54,641  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1 stored as values in memory (estimated size 5.5 KB, free 899.5 MB)
2019-09-12 11:02:54,656  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 899.5 MB)
2019-09-12 11:02:54,656  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_1_piece0 in memory on 10.6.1.14:5208 (size: 3.4 KB, free: 899.7 MB)
2019-09-12 11:02:54,656  INFO org.apache.spark.SparkContext 54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-09-12 11:02:54,687  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:34) (first 15 tasks are for partitions Vector(0))
2019-09-12 11:02:54,687  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 0.0 with 1 tasks
2019-09-12 11:02:54,734  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes)
2019-09-12 11:02:54,750  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 0.0 (TID 0)
2019-09-12 11:02:54,859  INFO org.apache.spark.rdd.BinaryFileRDD 54 - Input split: Paths:/D:/MyProject/splitCHword/target/classes/test.csv:0+326
2019-09-12 11:02:55,031  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 0.0 (TID 0). 853 bytes result sent to driver
2019-09-12 11:02:55,031  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 0.0 (TID 0) in 312 ms on localhost (executor driver) (1/1)
2019-09-12 11:02:55,047  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-09-12 11:02:55,047  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 0 (load at SparkUtil.java:34) finished in 0.344 s
2019-09-12 11:02:55,047  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 0 finished: load at SparkUtil.java:34, took 0.449800 s
2019-09-12 11:02:55,500  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_1_piece0 on 10.6.1.14:5208 in memory (size: 3.4 KB, free: 899.7 MB)
2019-09-12 11:02:56,734  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Pruning directories with: 
2019-09-12 11:02:56,750  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Post-Scan Filters: 
2019-09-12 11:02:56,750  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Output Data Schema: struct<feature: string>
2019-09-12 11:02:56,765  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Pushed Filters: 
2019-09-12 11:02:56,937  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_0_piece0 on 10.6.1.14:5208 in memory (size: 20.7 KB, free: 899.7 MB)
2019-09-12 11:02:57,406  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 276.271977 ms
2019-09-12 11:02:57,406  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2 stored as values in memory (estimated size 220.5 KB, free 899.5 MB)
2019-09-12 11:02:57,421  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 11:02:57,437  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_2_piece0 in memory on 10.6.1.14:5208 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 11:02:57,437  INFO org.apache.spark.SparkContext 54 - Created broadcast 2 from show at SparkUtil.java:56
2019-09-12 11:02:57,452  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2019-09-12 11:02:57,515  INFO org.apache.spark.SparkContext 54 - Starting job: show at SparkUtil.java:56
2019-09-12 11:02:57,515  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 1 (show at SparkUtil.java:56) with 1 output partitions
2019-09-12 11:02:57,515  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 1 (show at SparkUtil.java:56)
2019-09-12 11:02:57,515  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 11:02:57,515  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 11:02:57,515  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:56), which has no missing parents
2019-09-12 11:02:57,531  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3 stored as values in memory (estimated size 12.3 KB, free 899.5 MB)
2019-09-12 11:02:57,531  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.3 KB, free 899.4 MB)
2019-09-12 11:02:57,546  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_3_piece0 in memory on 10.6.1.14:5208 (size: 6.3 KB, free: 899.7 MB)
2019-09-12 11:02:57,546  INFO org.apache.spark.SparkContext 54 - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2019-09-12 11:02:57,546  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:56) (first 15 tasks are for partitions Vector(0))
2019-09-12 11:02:57,546  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 1.0 with 1 tasks
2019-09-12 11:02:57,546  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5293 bytes)
2019-09-12 11:02:57,546  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 1.0 (TID 1)
2019-09-12 11:02:57,562  INFO org.apache.spark.sql.execution.datasources.FileScanRDD 54 - Reading File path: file:///D:/MyProject/splitCHword/target/classes/test.csv, range: 0-326, partition values: [empty row]
2019-09-12 11:02:57,577  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 11.622816 ms
2019-09-12 11:02:57,827  INFO org.ansj.util.MyStaticValue 106 - init version.number to env value is : 2.11.8
2019-09-12 11:02:57,827  INFO org.ansj.util.MyStaticValue 106 - init copyright.string to env value is : Copyright 2002-2016, LAMP/EPFL
2019-09-12 11:02:57,827  INFO org.ansj.util.MyStaticValue 106 - init osgi.version.number to env value is : 2.11.8.v20160304-115712-1706a37eb8
2019-09-12 11:02:57,827  INFO org.ansj.util.MyStaticValue 106 - init maven.version.number to env value is : 2.11.8
2019-09-12 11:02:57,827  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/ambiguity.dic
2019-09-12 11:02:57,827 ERROR org.ansj.library.AmbiguityLibrary 117 - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:D:\MyProject\splitCHword\library\ambiguity.dic not found or can not to read, path: library/ambiguity.dic
2019-09-12 11:02:57,827  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/default.dic
2019-09-12 11:02:57,827 ERROR org.ansj.library.DicLibrary 221 - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:D:\MyProject\splitCHword\library\default.dic not found or can not to read, path: library/default.dic
2019-09-12 11:02:58,908  INFO org.ansj.library.DATDictionary 90 - init core library ok use time : 971
2019-09-12 11:02:59,329  INFO org.ansj.library.NgramLibrary 17 - init ngram ok use time :419
2019-09-12 11:02:59,344  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 1.0 (TID 1). 1230 bytes result sent to driver
2019-09-12 11:02:59,344  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 1.0 (TID 1) in 1798 ms on localhost (executor driver) (1/1)
2019-09-12 11:02:59,344  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-09-12 11:02:59,344  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 1 (show at SparkUtil.java:56) finished in 1.798 s
2019-09-12 11:02:59,344  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 1 finished: show at SparkUtil.java:56, took 1.842996 s
2019-09-12 11:02:59,376  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 13.537695 ms
2019-09-12 11:02:59,423  INFO org.apache.spark.SparkContext 54 - Invoking stop() from shutdown hook
2019-09-12 11:02:59,423  INFO org.spark_project.jetty.server.AbstractConnector 310 - Stopped Spark@60843569{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 11:02:59,423  INFO org.apache.spark.ui.SparkUI 54 - Stopped Spark web UI at http://10.6.1.14:4042
2019-09-12 11:02:59,485  INFO org.apache.spark.MapOutputTrackerMasterEndpoint 54 - MapOutputTrackerMasterEndpoint stopped!
2019-09-12 11:02:59,516  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore cleared
2019-09-12 11:02:59,516  INFO org.apache.spark.storage.BlockManager 54 - BlockManager stopped
2019-09-12 11:02:59,516  INFO org.apache.spark.storage.BlockManagerMaster 54 - BlockManagerMaster stopped
2019-09-12 11:02:59,532  INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint 54 - OutputCommitCoordinator stopped!
2019-09-12 11:02:59,532  INFO org.apache.spark.SparkContext 54 - Successfully stopped SparkContext
2019-09-12 11:02:59,532  INFO org.apache.spark.util.ShutdownHookManager 54 - Shutdown hook called
2019-09-12 11:02:59,532  INFO org.apache.spark.util.ShutdownHookManager 54 - Deleting directory C:\Users\OUY\AppData\Local\Temp\spark-8a90079e-eb7f-4a9f-a158-22f6855ddcc4
2019-09-12 11:06:38,600  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-12 11:06:39,240  WARN org.apache.hadoop.util.NativeCodeLoader 62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-12 11:06:39,365 ERROR org.apache.hadoop.util.Shell 396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2430)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
	at com.datamodel.SparkUtil.CreateSession(SparkUtil.java:25)
	at com.datamodel.SparkUtil.readDataFromCsv(SparkUtil.java:30)
	at com.datamodel.SparkUtil.main(SparkUtil.java:43)
2019-09-12 11:06:39,537  INFO org.apache.spark.SparkContext 54 - Submitted application: splitCHWord
2019-09-12 11:06:39,584  INFO org.apache.spark.SecurityManager 54 - Changing view acls to: OUY
2019-09-12 11:06:39,584  INFO org.apache.spark.SecurityManager 54 - Changing modify acls to: OUY
2019-09-12 11:06:39,584  INFO org.apache.spark.SecurityManager 54 - Changing view acls groups to: 
2019-09-12 11:06:39,584  INFO org.apache.spark.SecurityManager 54 - Changing modify acls groups to: 
2019-09-12 11:06:39,584  INFO org.apache.spark.SecurityManager 54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(OUY); groups with view permissions: Set(); users  with modify permissions: Set(OUY); groups with modify permissions: Set()
2019-09-12 11:06:40,318  INFO org.apache.spark.util.Utils 54 - Successfully started service 'sparkDriver' on port 5253.
2019-09-12 11:06:40,365  INFO org.apache.spark.SparkEnv 54 - Registering MapOutputTracker
2019-09-12 11:06:40,396  INFO org.apache.spark.SparkEnv 54 - Registering BlockManagerMaster
2019-09-12 11:06:40,396  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-09-12 11:06:40,396  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - BlockManagerMasterEndpoint up
2019-09-12 11:06:40,412  INFO org.apache.spark.storage.DiskBlockManager 54 - Created local directory at C:\Users\OUY\AppData\Local\Temp\blockmgr-2269fa0c-d798-4f11-813f-2ce00eb3243b
2019-09-12 11:06:40,428  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore started with capacity 899.7 MB
2019-09-12 11:06:40,490  INFO org.apache.spark.SparkEnv 54 - Registering OutputCommitCoordinator
2019-09-12 11:06:40,599  INFO org.spark_project.jetty.util.log 192 - Logging initialized @4269ms
2019-09-12 11:06:40,662  INFO org.spark_project.jetty.server.Server 345 - jetty-9.3.z-SNAPSHOT
2019-09-12 11:06:40,678  INFO org.spark_project.jetty.server.Server 403 - Started @4350ms
2019-09-12 11:06:40,693  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2019-09-12 11:06:40,693  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
2019-09-12 11:06:40,709  INFO org.spark_project.jetty.server.AbstractConnector 270 - Started ServerConnector@7311ec46{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 11:06:40,709  INFO org.apache.spark.util.Utils 54 - Successfully started service 'SparkUI' on port 4042.
2019-09-12 11:06:40,740  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@305f031{/jobs,null,AVAILABLE,@Spark}
2019-09-12 11:06:40,740  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@36b0fcd5{/jobs/json,null,AVAILABLE,@Spark}
2019-09-12 11:06:40,740  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@475835b1{/jobs/job,null,AVAILABLE,@Spark}
2019-09-12 11:06:40,740  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@716a7124{/jobs/job/json,null,AVAILABLE,@Spark}
2019-09-12 11:06:40,756  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@226642a5{/stages,null,AVAILABLE,@Spark}
2019-09-12 11:06:40,756  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5cc126dc{/stages/json,null,AVAILABLE,@Spark}
2019-09-12 11:06:40,756  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@72bd06ca{/stages/stage,null,AVAILABLE,@Spark}
2019-09-12 11:06:40,756  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@55795845{/stages/stage/json,null,AVAILABLE,@Spark}
2019-09-12 11:06:40,756  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@119f1f2a{/stages/pool,null,AVAILABLE,@Spark}
2019-09-12 11:06:40,756  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5b970f7{/stages/pool/json,null,AVAILABLE,@Spark}
2019-09-12 11:06:40,756  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@165b8a71{/storage,null,AVAILABLE,@Spark}
2019-09-12 11:06:40,756  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2f058b8a{/storage/json,null,AVAILABLE,@Spark}
2019-09-12 11:06:40,756  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3f2ef586{/storage/rdd,null,AVAILABLE,@Spark}
2019-09-12 11:06:40,756  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@76c7beb3{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-09-12 11:06:40,756  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2cf92cc7{/environment,null,AVAILABLE,@Spark}
2019-09-12 11:06:40,756  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7b139eab{/environment/json,null,AVAILABLE,@Spark}
2019-09-12 11:06:40,756  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@611df6e3{/executors,null,AVAILABLE,@Spark}
2019-09-12 11:06:40,756  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6273c5a4{/executors/json,null,AVAILABLE,@Spark}
2019-09-12 11:06:40,771  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@53e211ee{/executors/threadDump,null,AVAILABLE,@Spark}
2019-09-12 11:06:40,771  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3d8bbcdc{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-09-12 11:06:40,771  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@117e0fe5{/static,null,AVAILABLE,@Spark}
2019-09-12 11:06:40,771  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6c67e137{/,null,AVAILABLE,@Spark}
2019-09-12 11:06:40,771  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@191ae03f{/api,null,AVAILABLE,@Spark}
2019-09-12 11:06:40,771  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@172ca72b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-09-12 11:06:40,771  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@71e5f61d{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-09-12 11:06:40,787  INFO org.apache.spark.ui.SparkUI 54 - Bound SparkUI to 0.0.0.0, and started at http://10.6.1.14:4042
2019-09-12 11:06:40,943  INFO org.apache.spark.executor.Executor 54 - Starting executor ID driver on host localhost
2019-09-12 11:06:40,974  INFO org.apache.spark.util.Utils 54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 5266.
2019-09-12 11:06:40,990  INFO org.apache.spark.network.netty.NettyBlockTransferService 54 - Server created on 10.6.1.14:5266
2019-09-12 11:06:40,990  INFO org.apache.spark.storage.BlockManager 54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-09-12 11:06:40,990  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registering BlockManager BlockManagerId(driver, 10.6.1.14, 5266, None)
2019-09-12 11:06:41,006  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Registering block manager 10.6.1.14:5266 with 899.7 MB RAM, BlockManagerId(driver, 10.6.1.14, 5266, None)
2019-09-12 11:06:41,006  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registered BlockManager BlockManagerId(driver, 10.6.1.14, 5266, None)
2019-09-12 11:06:41,006  INFO org.apache.spark.storage.BlockManager 54 - Initialized BlockManager: BlockManagerId(driver, 10.6.1.14, 5266, None)
2019-09-12 11:06:41,240  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@721eb7df{/metrics/json,null,AVAILABLE,@Spark}
2019-09-12 11:06:41,334  INFO org.apache.spark.sql.internal.SharedState 54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/MyProject/splitCHword/spark-warehouse/').
2019-09-12 11:06:41,334  INFO org.apache.spark.sql.internal.SharedState 54 - Warehouse path is 'file:/D:/MyProject/splitCHword/spark-warehouse/'.
2019-09-12 11:06:41,349  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1adb7478{/SQL,null,AVAILABLE,@Spark}
2019-09-12 11:06:41,349  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@16943e88{/SQL/json,null,AVAILABLE,@Spark}
2019-09-12 11:06:41,349  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5a4ed68f{/SQL/execution,null,AVAILABLE,@Spark}
2019-09-12 11:06:41,349  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@d2387c8{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-09-12 11:06:41,349  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@100f9bbe{/static/sql,null,AVAILABLE,@Spark}
2019-09-12 11:06:42,376  INFO org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef 54 - Registered StateStoreCoordinator endpoint
2019-09-12 11:06:42,675  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0 stored as values in memory (estimated size 219.8 KB, free 899.5 MB)
2019-09-12 11:06:42,819  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 11:06:42,825  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_0_piece0 in memory on 10.6.1.14:5266 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 11:06:42,833  INFO org.apache.spark.SparkContext 54 - Created broadcast 0 from load at SparkUtil.java:34
2019-09-12 11:06:43,196  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 11:06:43,212  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 11:06:43,227  INFO org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 413 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 326
2019-09-12 11:06:43,274  INFO org.apache.spark.SparkContext 54 - Starting job: load at SparkUtil.java:34
2019-09-12 11:06:43,290  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 0 (load at SparkUtil.java:34) with 1 output partitions
2019-09-12 11:06:43,290  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 0 (load at SparkUtil.java:34)
2019-09-12 11:06:43,290  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 11:06:43,290  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 11:06:43,305  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:34), which has no missing parents
2019-09-12 11:06:43,321  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1 stored as values in memory (estimated size 5.5 KB, free 899.5 MB)
2019-09-12 11:06:43,337  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 899.5 MB)
2019-09-12 11:06:43,337  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_1_piece0 in memory on 10.6.1.14:5266 (size: 3.4 KB, free: 899.7 MB)
2019-09-12 11:06:43,337  INFO org.apache.spark.SparkContext 54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-09-12 11:06:43,368  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:34) (first 15 tasks are for partitions Vector(0))
2019-09-12 11:06:43,368  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 0.0 with 1 tasks
2019-09-12 11:06:43,415  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes)
2019-09-12 11:06:43,430  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 0.0 (TID 0)
2019-09-12 11:06:43,540  INFO org.apache.spark.rdd.BinaryFileRDD 54 - Input split: Paths:/D:/MyProject/splitCHword/target/classes/test.csv:0+326
2019-09-12 11:06:43,743  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 0.0 (TID 0). 853 bytes result sent to driver
2019-09-12 11:06:43,743  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 0.0 (TID 0) in 344 ms on localhost (executor driver) (1/1)
2019-09-12 11:06:43,743  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-09-12 11:06:43,759  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 0 (load at SparkUtil.java:34) finished in 0.359 s
2019-09-12 11:06:43,759  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 0 finished: load at SparkUtil.java:34, took 0.488267 s
2019-09-12 11:06:44,337  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_1_piece0 on 10.6.1.14:5266 in memory (size: 3.4 KB, free: 899.7 MB)
2019-09-12 11:06:45,586  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Pruning directories with: 
2019-09-12 11:06:45,586  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Post-Scan Filters: 
2019-09-12 11:06:45,602  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Output Data Schema: struct<feature: string>
2019-09-12 11:06:45,602  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Pushed Filters: 
2019-09-12 11:06:45,821  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_0_piece0 on 10.6.1.14:5266 in memory (size: 20.7 KB, free: 899.7 MB)
2019-09-12 11:06:46,274  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 308.082937 ms
2019-09-12 11:06:46,289  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2 stored as values in memory (estimated size 220.5 KB, free 899.5 MB)
2019-09-12 11:06:46,305  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 11:06:46,305  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_2_piece0 in memory on 10.6.1.14:5266 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 11:06:46,320  INFO org.apache.spark.SparkContext 54 - Created broadcast 2 from show at SparkUtil.java:55
2019-09-12 11:06:46,336  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2019-09-12 11:06:46,383  INFO org.apache.spark.SparkContext 54 - Starting job: show at SparkUtil.java:55
2019-09-12 11:06:46,399  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 1 (show at SparkUtil.java:55) with 1 output partitions
2019-09-12 11:06:46,399  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 1 (show at SparkUtil.java:55)
2019-09-12 11:06:46,399  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 11:06:46,399  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 11:06:46,399  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:55), which has no missing parents
2019-09-12 11:06:46,414  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3 stored as values in memory (estimated size 12.3 KB, free 899.5 MB)
2019-09-12 11:06:46,414  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.3 KB, free 899.4 MB)
2019-09-12 11:06:46,430  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_3_piece0 in memory on 10.6.1.14:5266 (size: 6.3 KB, free: 899.7 MB)
2019-09-12 11:06:46,430  INFO org.apache.spark.SparkContext 54 - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2019-09-12 11:06:46,430  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:55) (first 15 tasks are for partitions Vector(0))
2019-09-12 11:06:46,430  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 1.0 with 1 tasks
2019-09-12 11:06:46,430  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5293 bytes)
2019-09-12 11:06:46,430  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 1.0 (TID 1)
2019-09-12 11:06:46,445  INFO org.apache.spark.sql.execution.datasources.FileScanRDD 54 - Reading File path: file:///D:/MyProject/splitCHword/target/classes/test.csv, range: 0-326, partition values: [empty row]
2019-09-12 11:06:46,477  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 14.013002 ms
2019-09-12 11:06:46,711  INFO org.ansj.util.MyStaticValue 106 - init version.number to env value is : 2.11.8
2019-09-12 11:06:46,711  INFO org.ansj.util.MyStaticValue 106 - init copyright.string to env value is : Copyright 2002-2016, LAMP/EPFL
2019-09-12 11:06:46,711  INFO org.ansj.util.MyStaticValue 106 - init osgi.version.number to env value is : 2.11.8.v20160304-115712-1706a37eb8
2019-09-12 11:06:46,711  INFO org.ansj.util.MyStaticValue 106 - init maven.version.number to env value is : 2.11.8
2019-09-12 11:06:46,711  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/ambiguity.dic
2019-09-12 11:06:46,711 ERROR org.ansj.library.AmbiguityLibrary 117 - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:D:\MyProject\splitCHword\library\ambiguity.dic not found or can not to read, path: library/ambiguity.dic
2019-09-12 11:06:46,727  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/default.dic
2019-09-12 11:06:46,727 ERROR org.ansj.library.DicLibrary 221 - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:D:\MyProject\splitCHword\library\default.dic not found or can not to read, path: library/default.dic
2019-09-12 11:06:47,820  INFO org.ansj.library.DATDictionary 90 - init core library ok use time : 1000
2019-09-12 11:06:48,242  INFO org.ansj.library.NgramLibrary 17 - init ngram ok use time :422
2019-09-12 11:06:48,257  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 1.0 (TID 1). 1187 bytes result sent to driver
2019-09-12 11:06:48,257  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 1.0 (TID 1) in 1827 ms on localhost (executor driver) (1/1)
2019-09-12 11:06:48,257  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-09-12 11:06:48,257  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 1 (show at SparkUtil.java:55) finished in 1.827 s
2019-09-12 11:06:48,257  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 1 finished: show at SparkUtil.java:55, took 1.865897 s
2019-09-12 11:06:48,273  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 8.674553 ms
2019-09-12 11:06:48,289  INFO org.apache.spark.SparkContext 54 - Invoking stop() from shutdown hook
2019-09-12 11:06:48,304  INFO org.spark_project.jetty.server.AbstractConnector 310 - Stopped Spark@7311ec46{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 11:06:48,304  INFO org.apache.spark.ui.SparkUI 54 - Stopped Spark web UI at http://10.6.1.14:4042
2019-09-12 11:06:48,320  INFO org.apache.spark.MapOutputTrackerMasterEndpoint 54 - MapOutputTrackerMasterEndpoint stopped!
2019-09-12 11:06:48,351  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore cleared
2019-09-12 11:06:48,351  INFO org.apache.spark.storage.BlockManager 54 - BlockManager stopped
2019-09-12 11:06:48,351  INFO org.apache.spark.storage.BlockManagerMaster 54 - BlockManagerMaster stopped
2019-09-12 11:06:48,351  INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint 54 - OutputCommitCoordinator stopped!
2019-09-12 11:06:48,351  INFO org.apache.spark.SparkContext 54 - Successfully stopped SparkContext
2019-09-12 11:06:48,351  INFO org.apache.spark.util.ShutdownHookManager 54 - Shutdown hook called
2019-09-12 11:06:48,367  INFO org.apache.spark.util.ShutdownHookManager 54 - Deleting directory C:\Users\OUY\AppData\Local\Temp\spark-5f2951a0-5d3e-4f39-a450-6a7c6f995285
2019-09-12 11:10:46,267  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-12 11:10:47,084  WARN org.apache.hadoop.util.NativeCodeLoader 62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-12 11:10:47,180 ERROR org.apache.hadoop.util.Shell 396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2430)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
	at com.datamodel.SparkUtil.CreateSession(SparkUtil.java:25)
	at com.datamodel.SparkUtil.readDataFromCsv(SparkUtil.java:30)
	at com.datamodel.SparkUtil.main(SparkUtil.java:43)
2019-09-12 11:10:47,346  INFO org.apache.spark.SparkContext 54 - Submitted application: splitCHWord
2019-09-12 11:10:47,397  INFO org.apache.spark.SecurityManager 54 - Changing view acls to: OUY
2019-09-12 11:10:47,398  INFO org.apache.spark.SecurityManager 54 - Changing modify acls to: OUY
2019-09-12 11:10:47,399  INFO org.apache.spark.SecurityManager 54 - Changing view acls groups to: 
2019-09-12 11:10:47,400  INFO org.apache.spark.SecurityManager 54 - Changing modify acls groups to: 
2019-09-12 11:10:47,403  INFO org.apache.spark.SecurityManager 54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(OUY); groups with view permissions: Set(); users  with modify permissions: Set(OUY); groups with modify permissions: Set()
2019-09-12 11:10:48,184  INFO org.apache.spark.util.Utils 54 - Successfully started service 'sparkDriver' on port 5324.
2019-09-12 11:10:48,248  INFO org.apache.spark.SparkEnv 54 - Registering MapOutputTracker
2019-09-12 11:10:48,283  INFO org.apache.spark.SparkEnv 54 - Registering BlockManagerMaster
2019-09-12 11:10:48,290  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-09-12 11:10:48,291  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - BlockManagerMasterEndpoint up
2019-09-12 11:10:48,310  INFO org.apache.spark.storage.DiskBlockManager 54 - Created local directory at C:\Users\OUY\AppData\Local\Temp\blockmgr-d51dbc4b-0851-471b-b99a-58d934f9d432
2019-09-12 11:10:48,351  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore started with capacity 899.7 MB
2019-09-12 11:10:48,481  INFO org.apache.spark.SparkEnv 54 - Registering OutputCommitCoordinator
2019-09-12 11:10:48,638  INFO org.spark_project.jetty.util.log 192 - Logging initialized @5408ms
2019-09-12 11:10:48,708  INFO org.spark_project.jetty.server.Server 345 - jetty-9.3.z-SNAPSHOT
2019-09-12 11:10:48,729  INFO org.spark_project.jetty.server.Server 403 - Started @5501ms
2019-09-12 11:10:48,754  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2019-09-12 11:10:48,755  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
2019-09-12 11:10:48,768  INFO org.spark_project.jetty.server.AbstractConnector 270 - Started ServerConnector@791933f7{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 11:10:48,768  INFO org.apache.spark.util.Utils 54 - Successfully started service 'SparkUI' on port 4042.
2019-09-12 11:10:48,813  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@305f031{/jobs,null,AVAILABLE,@Spark}
2019-09-12 11:10:48,814  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@36b0fcd5{/jobs/json,null,AVAILABLE,@Spark}
2019-09-12 11:10:48,815  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@475835b1{/jobs/job,null,AVAILABLE,@Spark}
2019-09-12 11:10:48,817  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@716a7124{/jobs/job/json,null,AVAILABLE,@Spark}
2019-09-12 11:10:48,818  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@226642a5{/stages,null,AVAILABLE,@Spark}
2019-09-12 11:10:48,819  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5cc126dc{/stages/json,null,AVAILABLE,@Spark}
2019-09-12 11:10:48,822  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@72bd06ca{/stages/stage,null,AVAILABLE,@Spark}
2019-09-12 11:10:48,825  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@55795845{/stages/stage/json,null,AVAILABLE,@Spark}
2019-09-12 11:10:48,827  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@119f1f2a{/stages/pool,null,AVAILABLE,@Spark}
2019-09-12 11:10:48,828  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5b970f7{/stages/pool/json,null,AVAILABLE,@Spark}
2019-09-12 11:10:48,830  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@165b8a71{/storage,null,AVAILABLE,@Spark}
2019-09-12 11:10:48,831  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2f058b8a{/storage/json,null,AVAILABLE,@Spark}
2019-09-12 11:10:48,833  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3f2ef586{/storage/rdd,null,AVAILABLE,@Spark}
2019-09-12 11:10:48,835  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@76c7beb3{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-09-12 11:10:48,836  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2cf92cc7{/environment,null,AVAILABLE,@Spark}
2019-09-12 11:10:48,838  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7b139eab{/environment/json,null,AVAILABLE,@Spark}
2019-09-12 11:10:48,839  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@611df6e3{/executors,null,AVAILABLE,@Spark}
2019-09-12 11:10:48,841  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6273c5a4{/executors/json,null,AVAILABLE,@Spark}
2019-09-12 11:10:48,842  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@53e211ee{/executors/threadDump,null,AVAILABLE,@Spark}
2019-09-12 11:10:48,843  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3d8bbcdc{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-09-12 11:10:48,857  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@117e0fe5{/static,null,AVAILABLE,@Spark}
2019-09-12 11:10:48,858  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6c67e137{/,null,AVAILABLE,@Spark}
2019-09-12 11:10:48,860  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@191ae03f{/api,null,AVAILABLE,@Spark}
2019-09-12 11:10:48,861  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@172ca72b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-09-12 11:10:48,863  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@71e5f61d{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-09-12 11:10:48,866  INFO org.apache.spark.ui.SparkUI 54 - Bound SparkUI to 0.0.0.0, and started at http://10.6.1.14:4042
2019-09-12 11:10:49,113  INFO org.apache.spark.executor.Executor 54 - Starting executor ID driver on host localhost
2019-09-12 11:10:49,319  INFO org.apache.spark.util.Utils 54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 5337.
2019-09-12 11:10:49,322  INFO org.apache.spark.network.netty.NettyBlockTransferService 54 - Server created on 10.6.1.14:5337
2019-09-12 11:10:49,336  INFO org.apache.spark.storage.BlockManager 54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-09-12 11:10:49,339  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registering BlockManager BlockManagerId(driver, 10.6.1.14, 5337, None)
2019-09-12 11:10:49,371  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Registering block manager 10.6.1.14:5337 with 899.7 MB RAM, BlockManagerId(driver, 10.6.1.14, 5337, None)
2019-09-12 11:10:49,383  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registered BlockManager BlockManagerId(driver, 10.6.1.14, 5337, None)
2019-09-12 11:10:49,386  INFO org.apache.spark.storage.BlockManager 54 - Initialized BlockManager: BlockManagerId(driver, 10.6.1.14, 5337, None)
2019-09-12 11:10:49,770  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@721eb7df{/metrics/json,null,AVAILABLE,@Spark}
2019-09-12 11:10:49,890  INFO org.apache.spark.sql.internal.SharedState 54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/MyProject/splitCHword/spark-warehouse/').
2019-09-12 11:10:49,892  INFO org.apache.spark.sql.internal.SharedState 54 - Warehouse path is 'file:/D:/MyProject/splitCHword/spark-warehouse/'.
2019-09-12 11:10:49,904  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1adb7478{/SQL,null,AVAILABLE,@Spark}
2019-09-12 11:10:49,905  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@16943e88{/SQL/json,null,AVAILABLE,@Spark}
2019-09-12 11:10:49,907  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5a4ed68f{/SQL/execution,null,AVAILABLE,@Spark}
2019-09-12 11:10:49,908  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@d2387c8{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-09-12 11:10:49,912  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@100f9bbe{/static/sql,null,AVAILABLE,@Spark}
2019-09-12 11:10:51,388  INFO org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef 54 - Registered StateStoreCoordinator endpoint
2019-09-12 11:10:51,764  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0 stored as values in memory (estimated size 219.8 KB, free 899.5 MB)
2019-09-12 11:10:51,857  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 11:10:51,862  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_0_piece0 in memory on 10.6.1.14:5337 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 11:10:51,867  INFO org.apache.spark.SparkContext 54 - Created broadcast 0 from load at SparkUtil.java:34
2019-09-12 11:10:52,463  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 11:10:52,467  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 11:10:52,492  INFO org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 413 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 326
2019-09-12 11:10:52,577  INFO org.apache.spark.SparkContext 54 - Starting job: load at SparkUtil.java:34
2019-09-12 11:10:52,599  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 0 (load at SparkUtil.java:34) with 1 output partitions
2019-09-12 11:10:52,599  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 0 (load at SparkUtil.java:34)
2019-09-12 11:10:52,600  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 11:10:52,602  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 11:10:52,622  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:34), which has no missing parents
2019-09-12 11:10:52,644  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1 stored as values in memory (estimated size 5.5 KB, free 899.5 MB)
2019-09-12 11:10:52,652  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 899.5 MB)
2019-09-12 11:10:52,653  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_1_piece0 in memory on 10.6.1.14:5337 (size: 3.4 KB, free: 899.7 MB)
2019-09-12 11:10:52,655  INFO org.apache.spark.SparkContext 54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-09-12 11:10:52,695  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:34) (first 15 tasks are for partitions Vector(0))
2019-09-12 11:10:52,697  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 0.0 with 1 tasks
2019-09-12 11:10:52,761  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes)
2019-09-12 11:10:52,777  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 0.0 (TID 0)
2019-09-12 11:10:52,851  INFO org.apache.spark.rdd.BinaryFileRDD 54 - Input split: Paths:/D:/MyProject/splitCHword/target/classes/test.csv:0+326
2019-09-12 11:10:53,061  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 0.0 (TID 0). 853 bytes result sent to driver
2019-09-12 11:10:53,069  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 0.0 (TID 0) in 325 ms on localhost (executor driver) (1/1)
2019-09-12 11:10:53,072  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-09-12 11:10:53,077  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 0 (load at SparkUtil.java:34) finished in 0.354 s
2019-09-12 11:10:53,083  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 0 finished: load at SparkUtil.java:34, took 0.506150 s
2019-09-12 11:10:53,610  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_1_piece0 on 10.6.1.14:5337 in memory (size: 3.4 KB, free: 899.7 MB)
2019-09-12 11:10:53,616  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_0_piece0 on 10.6.1.14:5337 in memory (size: 20.7 KB, free: 899.7 MB)
2019-09-12 11:10:55,199  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Pruning directories with: 
2019-09-12 11:10:55,203  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Post-Scan Filters: 
2019-09-12 11:10:55,206  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Output Data Schema: struct<feature: string>
2019-09-12 11:10:55,215  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Pushed Filters: 
2019-09-12 11:10:56,021  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 443.611355 ms
2019-09-12 11:10:56,042  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2 stored as values in memory (estimated size 220.5 KB, free 899.5 MB)
2019-09-12 11:10:56,066  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 11:10:56,069  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_2_piece0 in memory on 10.6.1.14:5337 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 11:10:56,073  INFO org.apache.spark.SparkContext 54 - Created broadcast 2 from show at SparkUtil.java:57
2019-09-12 11:10:56,091  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2019-09-12 11:10:56,206  INFO org.apache.spark.SparkContext 54 - Starting job: show at SparkUtil.java:57
2019-09-12 11:10:56,211  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 1 (show at SparkUtil.java:57) with 1 output partitions
2019-09-12 11:10:56,211  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 1 (show at SparkUtil.java:57)
2019-09-12 11:10:56,212  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 11:10:56,212  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 11:10:56,213  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:57), which has no missing parents
2019-09-12 11:10:56,249  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3 stored as values in memory (estimated size 15.4 KB, free 899.4 MB)
2019-09-12 11:10:56,256  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.3 KB, free 899.4 MB)
2019-09-12 11:10:56,257  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_3_piece0 in memory on 10.6.1.14:5337 (size: 7.3 KB, free: 899.7 MB)
2019-09-12 11:10:56,258  INFO org.apache.spark.SparkContext 54 - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2019-09-12 11:10:56,260  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:57) (first 15 tasks are for partitions Vector(0))
2019-09-12 11:10:56,261  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 1.0 with 1 tasks
2019-09-12 11:10:56,272  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5293 bytes)
2019-09-12 11:10:56,273  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 1.0 (TID 1)
2019-09-12 11:10:56,299  INFO org.apache.spark.sql.execution.datasources.FileScanRDD 54 - Reading File path: file:///D:/MyProject/splitCHword/target/classes/test.csv, range: 0-326, partition values: [empty row]
2019-09-12 11:10:56,327  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 19.776836 ms
2019-09-12 11:10:56,601  INFO org.ansj.util.MyStaticValue 106 - init version.number to env value is : 2.11.8
2019-09-12 11:10:56,602  INFO org.ansj.util.MyStaticValue 106 - init copyright.string to env value is : Copyright 2002-2016, LAMP/EPFL
2019-09-12 11:10:56,602  INFO org.ansj.util.MyStaticValue 106 - init osgi.version.number to env value is : 2.11.8.v20160304-115712-1706a37eb8
2019-09-12 11:10:56,602  INFO org.ansj.util.MyStaticValue 106 - init maven.version.number to env value is : 2.11.8
2019-09-12 11:10:56,605  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/ambiguity.dic
2019-09-12 11:10:56,606 ERROR org.ansj.library.AmbiguityLibrary 117 - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:D:\MyProject\splitCHword\library\ambiguity.dic not found or can not to read, path: library/ambiguity.dic
2019-09-12 11:10:56,608  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/default.dic
2019-09-12 11:10:56,608 ERROR org.ansj.library.DicLibrary 221 - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:D:\MyProject\splitCHword\library\default.dic not found or can not to read, path: library/default.dic
2019-09-12 11:10:57,802  INFO org.ansj.library.DATDictionary 90 - init core library ok use time : 1088
2019-09-12 11:10:58,339  INFO org.ansj.library.NgramLibrary 17 - init ngram ok use time :532
2019-09-12 11:10:58,351  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 1.0 (TID 1). 1472 bytes result sent to driver
2019-09-12 11:10:58,353  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 1.0 (TID 1) in 2091 ms on localhost (executor driver) (1/1)
2019-09-12 11:10:58,354  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-09-12 11:10:58,356  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 1 (show at SparkUtil.java:57) finished in 2.093 s
2019-09-12 11:10:58,356  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 1 finished: show at SparkUtil.java:57, took 2.148121 s
2019-09-12 11:10:58,385  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 13.850015 ms
2019-09-12 11:10:58,497  INFO org.apache.spark.SparkContext 54 - Invoking stop() from shutdown hook
2019-09-12 11:10:58,503  INFO org.spark_project.jetty.server.AbstractConnector 310 - Stopped Spark@791933f7{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 11:10:58,507  INFO org.apache.spark.ui.SparkUI 54 - Stopped Spark web UI at http://10.6.1.14:4042
2019-09-12 11:10:58,521  INFO org.apache.spark.MapOutputTrackerMasterEndpoint 54 - MapOutputTrackerMasterEndpoint stopped!
2019-09-12 11:10:58,560  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore cleared
2019-09-12 11:10:58,561  INFO org.apache.spark.storage.BlockManager 54 - BlockManager stopped
2019-09-12 11:10:58,563  INFO org.apache.spark.storage.BlockManagerMaster 54 - BlockManagerMaster stopped
2019-09-12 11:10:58,568  INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint 54 - OutputCommitCoordinator stopped!
2019-09-12 11:10:58,575  INFO org.apache.spark.SparkContext 54 - Successfully stopped SparkContext
2019-09-12 11:10:58,576  INFO org.apache.spark.util.ShutdownHookManager 54 - Shutdown hook called
2019-09-12 11:10:58,577  INFO org.apache.spark.util.ShutdownHookManager 54 - Deleting directory C:\Users\OUY\AppData\Local\Temp\spark-32d28ba3-39fe-40c1-8f8c-b2328a6acc00
2019-09-12 11:13:06,601  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-12 11:13:07,364  WARN org.apache.hadoop.util.NativeCodeLoader 62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-12 11:13:07,464 ERROR org.apache.hadoop.util.Shell 396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2430)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
	at com.datamodel.SparkUtil.CreateSession(SparkUtil.java:25)
	at com.datamodel.SparkUtil.readDataFromCsv(SparkUtil.java:30)
	at com.datamodel.SparkUtil.main(SparkUtil.java:43)
2019-09-12 11:13:07,602  INFO org.apache.spark.SparkContext 54 - Submitted application: splitCHWord
2019-09-12 11:13:07,646  INFO org.apache.spark.SecurityManager 54 - Changing view acls to: OUY
2019-09-12 11:13:07,648  INFO org.apache.spark.SecurityManager 54 - Changing modify acls to: OUY
2019-09-12 11:13:07,649  INFO org.apache.spark.SecurityManager 54 - Changing view acls groups to: 
2019-09-12 11:13:07,651  INFO org.apache.spark.SecurityManager 54 - Changing modify acls groups to: 
2019-09-12 11:13:07,652  INFO org.apache.spark.SecurityManager 54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(OUY); groups with view permissions: Set(); users  with modify permissions: Set(OUY); groups with modify permissions: Set()
2019-09-12 11:13:08,449  INFO org.apache.spark.util.Utils 54 - Successfully started service 'sparkDriver' on port 5388.
2019-09-12 11:13:08,509  INFO org.apache.spark.SparkEnv 54 - Registering MapOutputTracker
2019-09-12 11:13:08,532  INFO org.apache.spark.SparkEnv 54 - Registering BlockManagerMaster
2019-09-12 11:13:08,536  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-09-12 11:13:08,536  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - BlockManagerMasterEndpoint up
2019-09-12 11:13:08,549  INFO org.apache.spark.storage.DiskBlockManager 54 - Created local directory at C:\Users\OUY\AppData\Local\Temp\blockmgr-588fb3bd-333e-4cff-b2ba-cddb1fd6e49b
2019-09-12 11:13:08,576  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore started with capacity 899.7 MB
2019-09-12 11:13:08,638  INFO org.apache.spark.SparkEnv 54 - Registering OutputCommitCoordinator
2019-09-12 11:13:08,755  INFO org.spark_project.jetty.util.log 192 - Logging initialized @4365ms
2019-09-12 11:13:08,829  INFO org.spark_project.jetty.server.Server 345 - jetty-9.3.z-SNAPSHOT
2019-09-12 11:13:08,847  INFO org.spark_project.jetty.server.Server 403 - Started @4459ms
2019-09-12 11:13:08,864  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2019-09-12 11:13:08,865  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
2019-09-12 11:13:08,874  INFO org.spark_project.jetty.server.AbstractConnector 270 - Started ServerConnector@2bde5b68{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 11:13:08,874  INFO org.apache.spark.util.Utils 54 - Successfully started service 'SparkUI' on port 4042.
2019-09-12 11:13:08,907  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@305f031{/jobs,null,AVAILABLE,@Spark}
2019-09-12 11:13:08,908  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@36b0fcd5{/jobs/json,null,AVAILABLE,@Spark}
2019-09-12 11:13:08,909  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@475835b1{/jobs/job,null,AVAILABLE,@Spark}
2019-09-12 11:13:08,910  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@716a7124{/jobs/job/json,null,AVAILABLE,@Spark}
2019-09-12 11:13:08,911  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@226642a5{/stages,null,AVAILABLE,@Spark}
2019-09-12 11:13:08,912  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5cc126dc{/stages/json,null,AVAILABLE,@Spark}
2019-09-12 11:13:08,914  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@72bd06ca{/stages/stage,null,AVAILABLE,@Spark}
2019-09-12 11:13:08,915  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@55795845{/stages/stage/json,null,AVAILABLE,@Spark}
2019-09-12 11:13:08,916  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@119f1f2a{/stages/pool,null,AVAILABLE,@Spark}
2019-09-12 11:13:08,918  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5b970f7{/stages/pool/json,null,AVAILABLE,@Spark}
2019-09-12 11:13:08,919  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@165b8a71{/storage,null,AVAILABLE,@Spark}
2019-09-12 11:13:08,920  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2f058b8a{/storage/json,null,AVAILABLE,@Spark}
2019-09-12 11:13:08,922  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3f2ef586{/storage/rdd,null,AVAILABLE,@Spark}
2019-09-12 11:13:08,923  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@76c7beb3{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-09-12 11:13:08,925  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2cf92cc7{/environment,null,AVAILABLE,@Spark}
2019-09-12 11:13:08,926  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7b139eab{/environment/json,null,AVAILABLE,@Spark}
2019-09-12 11:13:08,927  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@611df6e3{/executors,null,AVAILABLE,@Spark}
2019-09-12 11:13:08,928  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6273c5a4{/executors/json,null,AVAILABLE,@Spark}
2019-09-12 11:13:08,930  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@53e211ee{/executors/threadDump,null,AVAILABLE,@Spark}
2019-09-12 11:13:08,931  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3d8bbcdc{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-09-12 11:13:08,942  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@117e0fe5{/static,null,AVAILABLE,@Spark}
2019-09-12 11:13:08,943  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6c67e137{/,null,AVAILABLE,@Spark}
2019-09-12 11:13:08,945  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@191ae03f{/api,null,AVAILABLE,@Spark}
2019-09-12 11:13:08,946  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@172ca72b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-09-12 11:13:08,947  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@71e5f61d{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-09-12 11:13:08,949  INFO org.apache.spark.ui.SparkUI 54 - Bound SparkUI to 0.0.0.0, and started at http://10.6.1.14:4042
2019-09-12 11:13:09,189  INFO org.apache.spark.executor.Executor 54 - Starting executor ID driver on host localhost
2019-09-12 11:13:09,269  INFO org.apache.spark.util.Utils 54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 5401.
2019-09-12 11:13:09,271  INFO org.apache.spark.network.netty.NettyBlockTransferService 54 - Server created on 10.6.1.14:5401
2019-09-12 11:13:09,273  INFO org.apache.spark.storage.BlockManager 54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-09-12 11:13:09,286  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registering BlockManager BlockManagerId(driver, 10.6.1.14, 5401, None)
2019-09-12 11:13:09,307  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Registering block manager 10.6.1.14:5401 with 899.7 MB RAM, BlockManagerId(driver, 10.6.1.14, 5401, None)
2019-09-12 11:13:09,312  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registered BlockManager BlockManagerId(driver, 10.6.1.14, 5401, None)
2019-09-12 11:13:09,313  INFO org.apache.spark.storage.BlockManager 54 - Initialized BlockManager: BlockManagerId(driver, 10.6.1.14, 5401, None)
2019-09-12 11:13:09,887  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@721eb7df{/metrics/json,null,AVAILABLE,@Spark}
2019-09-12 11:13:10,011  INFO org.apache.spark.sql.internal.SharedState 54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/MyProject/splitCHword/spark-warehouse/').
2019-09-12 11:13:10,013  INFO org.apache.spark.sql.internal.SharedState 54 - Warehouse path is 'file:/D:/MyProject/splitCHword/spark-warehouse/'.
2019-09-12 11:13:10,026  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1adb7478{/SQL,null,AVAILABLE,@Spark}
2019-09-12 11:13:10,027  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@16943e88{/SQL/json,null,AVAILABLE,@Spark}
2019-09-12 11:13:10,029  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5a4ed68f{/SQL/execution,null,AVAILABLE,@Spark}
2019-09-12 11:13:10,030  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@d2387c8{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-09-12 11:13:10,033  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@100f9bbe{/static/sql,null,AVAILABLE,@Spark}
2019-09-12 11:13:11,186  INFO org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef 54 - Registered StateStoreCoordinator endpoint
2019-09-12 11:13:11,606  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0 stored as values in memory (estimated size 219.8 KB, free 899.5 MB)
2019-09-12 11:13:11,716  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 11:13:11,722  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_0_piece0 in memory on 10.6.1.14:5401 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 11:13:11,728  INFO org.apache.spark.SparkContext 54 - Created broadcast 0 from load at SparkUtil.java:34
2019-09-12 11:13:12,124  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 11:13:12,126  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 11:13:12,143  INFO org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 413 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 326
2019-09-12 11:13:12,196  INFO org.apache.spark.SparkContext 54 - Starting job: load at SparkUtil.java:34
2019-09-12 11:13:12,216  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 0 (load at SparkUtil.java:34) with 1 output partitions
2019-09-12 11:13:12,217  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 0 (load at SparkUtil.java:34)
2019-09-12 11:13:12,217  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 11:13:12,219  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 11:13:12,225  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:34), which has no missing parents
2019-09-12 11:13:12,266  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1 stored as values in memory (estimated size 5.5 KB, free 899.5 MB)
2019-09-12 11:13:12,310  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 899.5 MB)
2019-09-12 11:13:12,311  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_1_piece0 in memory on 10.6.1.14:5401 (size: 3.4 KB, free: 899.7 MB)
2019-09-12 11:13:12,313  INFO org.apache.spark.SparkContext 54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-09-12 11:13:12,337  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:34) (first 15 tasks are for partitions Vector(0))
2019-09-12 11:13:12,341  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 0.0 with 1 tasks
2019-09-12 11:13:12,405  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes)
2019-09-12 11:13:12,419  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 0.0 (TID 0)
2019-09-12 11:13:12,504  INFO org.apache.spark.rdd.BinaryFileRDD 54 - Input split: Paths:/D:/MyProject/splitCHword/target/classes/test.csv:0+326
2019-09-12 11:13:12,712  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 0.0 (TID 0). 853 bytes result sent to driver
2019-09-12 11:13:12,721  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 0.0 (TID 0) in 333 ms on localhost (executor driver) (1/1)
2019-09-12 11:13:12,724  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-09-12 11:13:12,730  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 0 (load at SparkUtil.java:34) finished in 0.366 s
2019-09-12 11:13:12,737  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 0 finished: load at SparkUtil.java:34, took 0.540796 s
2019-09-12 11:13:13,277  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_1_piece0 on 10.6.1.14:5401 in memory (size: 3.4 KB, free: 899.7 MB)
2019-09-12 11:13:14,669  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Pruning directories with: 
2019-09-12 11:13:14,672  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Post-Scan Filters: 
2019-09-12 11:13:14,677  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Output Data Schema: struct<feature: string>
2019-09-12 11:13:14,689  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Pushed Filters: 
2019-09-12 11:13:14,914  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_0_piece0 on 10.6.1.14:5401 in memory (size: 20.7 KB, free: 899.7 MB)
2019-09-12 11:13:15,410  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 335.1067 ms
2019-09-12 11:13:15,420  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2 stored as values in memory (estimated size 220.5 KB, free 899.5 MB)
2019-09-12 11:13:15,446  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 11:13:15,450  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_2_piece0 in memory on 10.6.1.14:5401 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 11:13:15,451  INFO org.apache.spark.SparkContext 54 - Created broadcast 2 from show at SparkUtil.java:57
2019-09-12 11:13:15,469  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2019-09-12 11:13:15,542  INFO org.apache.spark.SparkContext 54 - Starting job: show at SparkUtil.java:57
2019-09-12 11:13:15,544  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 1 (show at SparkUtil.java:57) with 1 output partitions
2019-09-12 11:13:15,544  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 1 (show at SparkUtil.java:57)
2019-09-12 11:13:15,544  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 11:13:15,545  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 11:13:15,545  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:57), which has no missing parents
2019-09-12 11:13:15,570  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3 stored as values in memory (estimated size 15.4 KB, free 899.4 MB)
2019-09-12 11:13:15,576  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.3 KB, free 899.4 MB)
2019-09-12 11:13:15,577  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_3_piece0 in memory on 10.6.1.14:5401 (size: 7.3 KB, free: 899.7 MB)
2019-09-12 11:13:15,578  INFO org.apache.spark.SparkContext 54 - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2019-09-12 11:13:15,579  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:57) (first 15 tasks are for partitions Vector(0))
2019-09-12 11:13:15,579  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 1.0 with 1 tasks
2019-09-12 11:13:15,587  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5293 bytes)
2019-09-12 11:13:15,588  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 1.0 (TID 1)
2019-09-12 11:13:15,607  INFO org.apache.spark.sql.execution.datasources.FileScanRDD 54 - Reading File path: file:///D:/MyProject/splitCHword/target/classes/test.csv, range: 0-326, partition values: [empty row]
2019-09-12 11:13:15,642  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 30.8804 ms
2019-09-12 11:13:15,922  INFO org.ansj.util.MyStaticValue 106 - init version.number to env value is : 2.11.8
2019-09-12 11:13:15,922  INFO org.ansj.util.MyStaticValue 106 - init copyright.string to env value is : Copyright 2002-2016, LAMP/EPFL
2019-09-12 11:13:15,922  INFO org.ansj.util.MyStaticValue 106 - init osgi.version.number to env value is : 2.11.8.v20160304-115712-1706a37eb8
2019-09-12 11:13:15,922  INFO org.ansj.util.MyStaticValue 106 - init maven.version.number to env value is : 2.11.8
2019-09-12 11:13:15,927  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/ambiguity.dic
2019-09-12 11:13:15,928 ERROR org.ansj.library.AmbiguityLibrary 117 - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:D:\MyProject\splitCHword\library\ambiguity.dic not found or can not to read, path: library/ambiguity.dic
2019-09-12 11:13:15,930  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/default.dic
2019-09-12 11:13:15,931 ERROR org.ansj.library.DicLibrary 221 - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:D:\MyProject\splitCHword\library\default.dic not found or can not to read, path: library/default.dic
2019-09-12 11:13:16,984  INFO org.ansj.library.DATDictionary 90 - init core library ok use time : 946
2019-09-12 11:13:17,441  INFO org.ansj.library.NgramLibrary 17 - init ngram ok use time :455
2019-09-12 11:13:17,455  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 1.0 (TID 1). 1386 bytes result sent to driver
2019-09-12 11:13:17,457  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 1.0 (TID 1) in 1877 ms on localhost (executor driver) (1/1)
2019-09-12 11:13:17,461  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 1 (show at SparkUtil.java:57) finished in 1.878 s
2019-09-12 11:13:17,462  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-09-12 11:13:17,463  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 1 finished: show at SparkUtil.java:57, took 1.920350 s
2019-09-12 11:13:17,490  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 11.77855 ms
2019-09-12 11:13:17,546  INFO org.apache.spark.SparkContext 54 - Invoking stop() from shutdown hook
2019-09-12 11:13:17,554  INFO org.spark_project.jetty.server.AbstractConnector 310 - Stopped Spark@2bde5b68{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 11:13:17,557  INFO org.apache.spark.ui.SparkUI 54 - Stopped Spark web UI at http://10.6.1.14:4042
2019-09-12 11:13:17,627  INFO org.apache.spark.MapOutputTrackerMasterEndpoint 54 - MapOutputTrackerMasterEndpoint stopped!
2019-09-12 11:13:17,666  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore cleared
2019-09-12 11:13:17,667  INFO org.apache.spark.storage.BlockManager 54 - BlockManager stopped
2019-09-12 11:13:17,669  INFO org.apache.spark.storage.BlockManagerMaster 54 - BlockManagerMaster stopped
2019-09-12 11:13:17,672  INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint 54 - OutputCommitCoordinator stopped!
2019-09-12 11:13:17,680  INFO org.apache.spark.SparkContext 54 - Successfully stopped SparkContext
2019-09-12 11:13:17,680  INFO org.apache.spark.util.ShutdownHookManager 54 - Shutdown hook called
2019-09-12 11:13:17,681  INFO org.apache.spark.util.ShutdownHookManager 54 - Deleting directory C:\Users\OUY\AppData\Local\Temp\spark-c8aa83ae-d0be-40e0-8fdd-d6bbe02fe93c
2019-09-12 11:14:10,894  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-12 11:14:11,529  WARN org.apache.hadoop.util.NativeCodeLoader 62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-12 11:14:11,630 ERROR org.apache.hadoop.util.Shell 396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2430)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
	at com.datamodel.SparkUtil.CreateSession(SparkUtil.java:25)
	at com.datamodel.SparkUtil.readDataFromCsv(SparkUtil.java:30)
	at com.datamodel.SparkUtil.main(SparkUtil.java:43)
2019-09-12 11:14:11,782  INFO org.apache.spark.SparkContext 54 - Submitted application: splitCHWord
2019-09-12 11:14:11,822  INFO org.apache.spark.SecurityManager 54 - Changing view acls to: OUY
2019-09-12 11:14:11,823  INFO org.apache.spark.SecurityManager 54 - Changing modify acls to: OUY
2019-09-12 11:14:11,826  INFO org.apache.spark.SecurityManager 54 - Changing view acls groups to: 
2019-09-12 11:14:11,826  INFO org.apache.spark.SecurityManager 54 - Changing modify acls groups to: 
2019-09-12 11:14:11,828  INFO org.apache.spark.SecurityManager 54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(OUY); groups with view permissions: Set(); users  with modify permissions: Set(OUY); groups with modify permissions: Set()
2019-09-12 11:14:12,618  INFO org.apache.spark.util.Utils 54 - Successfully started service 'sparkDriver' on port 5428.
2019-09-12 11:14:12,676  INFO org.apache.spark.SparkEnv 54 - Registering MapOutputTracker
2019-09-12 11:14:12,700  INFO org.apache.spark.SparkEnv 54 - Registering BlockManagerMaster
2019-09-12 11:14:12,704  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-09-12 11:14:12,704  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - BlockManagerMasterEndpoint up
2019-09-12 11:14:12,717  INFO org.apache.spark.storage.DiskBlockManager 54 - Created local directory at C:\Users\OUY\AppData\Local\Temp\blockmgr-8377a3fb-fd62-4197-b0b8-01646accdb4f
2019-09-12 11:14:12,744  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore started with capacity 899.7 MB
2019-09-12 11:14:12,821  INFO org.apache.spark.SparkEnv 54 - Registering OutputCommitCoordinator
2019-09-12 11:14:12,945  INFO org.spark_project.jetty.util.log 192 - Logging initialized @4356ms
2019-09-12 11:14:13,014  INFO org.spark_project.jetty.server.Server 345 - jetty-9.3.z-SNAPSHOT
2019-09-12 11:14:13,034  INFO org.spark_project.jetty.server.Server 403 - Started @4447ms
2019-09-12 11:14:13,057  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2019-09-12 11:14:13,058  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
2019-09-12 11:14:13,069  INFO org.spark_project.jetty.server.AbstractConnector 270 - Started ServerConnector@297ea53a{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 11:14:13,070  INFO org.apache.spark.util.Utils 54 - Successfully started service 'SparkUI' on port 4042.
2019-09-12 11:14:13,108  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@592e843a{/jobs,null,AVAILABLE,@Spark}
2019-09-12 11:14:13,110  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4fad94a7{/jobs/json,null,AVAILABLE,@Spark}
2019-09-12 11:14:13,111  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6326d182{/jobs/job,null,AVAILABLE,@Spark}
2019-09-12 11:14:13,113  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@77192705{/jobs/job/json,null,AVAILABLE,@Spark}
2019-09-12 11:14:13,115  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7e809b79{/stages,null,AVAILABLE,@Spark}
2019-09-12 11:14:13,116  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@625e134e{/stages/json,null,AVAILABLE,@Spark}
2019-09-12 11:14:13,117  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@89c10b7{/stages/stage,null,AVAILABLE,@Spark}
2019-09-12 11:14:13,122  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3d08f3f5{/stages/stage/json,null,AVAILABLE,@Spark}
2019-09-12 11:14:13,123  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1a1da881{/stages/pool,null,AVAILABLE,@Spark}
2019-09-12 11:14:13,125  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7fd4acee{/stages/pool/json,null,AVAILABLE,@Spark}
2019-09-12 11:14:13,126  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6175619b{/storage,null,AVAILABLE,@Spark}
2019-09-12 11:14:13,127  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@756cf158{/storage/json,null,AVAILABLE,@Spark}
2019-09-12 11:14:13,129  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@751d3241{/storage/rdd,null,AVAILABLE,@Spark}
2019-09-12 11:14:13,130  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@64337702{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-09-12 11:14:13,132  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@30ea8c23{/environment,null,AVAILABLE,@Spark}
2019-09-12 11:14:13,133  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4e76dac{/environment/json,null,AVAILABLE,@Spark}
2019-09-12 11:14:13,135  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5f2f577{/executors,null,AVAILABLE,@Spark}
2019-09-12 11:14:13,136  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5d465e4b{/executors/json,null,AVAILABLE,@Spark}
2019-09-12 11:14:13,138  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@41a90fa8{/executors/threadDump,null,AVAILABLE,@Spark}
2019-09-12 11:14:13,140  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@52500920{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-09-12 11:14:13,153  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@18a3962d{/static,null,AVAILABLE,@Spark}
2019-09-12 11:14:13,154  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2c9399a4{/,null,AVAILABLE,@Spark}
2019-09-12 11:14:13,157  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@9635fa{/api,null,AVAILABLE,@Spark}
2019-09-12 11:14:13,158  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5bda80bf{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-09-12 11:14:13,159  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2ce86164{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-09-12 11:14:13,163  INFO org.apache.spark.ui.SparkUI 54 - Bound SparkUI to 0.0.0.0, and started at http://10.6.1.14:4042
2019-09-12 11:14:13,390  INFO org.apache.spark.executor.Executor 54 - Starting executor ID driver on host localhost
2019-09-12 11:14:13,434  INFO org.apache.spark.util.Utils 54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 5441.
2019-09-12 11:14:13,435  INFO org.apache.spark.network.netty.NettyBlockTransferService 54 - Server created on 10.6.1.14:5441
2019-09-12 11:14:13,437  INFO org.apache.spark.storage.BlockManager 54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-09-12 11:14:13,445  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registering BlockManager BlockManagerId(driver, 10.6.1.14, 5441, None)
2019-09-12 11:14:13,448  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Registering block manager 10.6.1.14:5441 with 899.7 MB RAM, BlockManagerId(driver, 10.6.1.14, 5441, None)
2019-09-12 11:14:13,452  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registered BlockManager BlockManagerId(driver, 10.6.1.14, 5441, None)
2019-09-12 11:14:13,453  INFO org.apache.spark.storage.BlockManager 54 - Initialized BlockManager: BlockManagerId(driver, 10.6.1.14, 5441, None)
2019-09-12 11:14:13,654  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1ea9f009{/metrics/json,null,AVAILABLE,@Spark}
2019-09-12 11:14:13,752  INFO org.apache.spark.sql.internal.SharedState 54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/MyProject/splitCHword/spark-warehouse/').
2019-09-12 11:14:13,753  INFO org.apache.spark.sql.internal.SharedState 54 - Warehouse path is 'file:/D:/MyProject/splitCHword/spark-warehouse/'.
2019-09-12 11:14:13,769  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3ae66c85{/SQL,null,AVAILABLE,@Spark}
2019-09-12 11:14:13,770  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4604b900{/SQL/json,null,AVAILABLE,@Spark}
2019-09-12 11:14:13,772  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@367795c7{/SQL/execution,null,AVAILABLE,@Spark}
2019-09-12 11:14:13,773  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3956b302{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-09-12 11:14:13,776  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@13e9f2e2{/static/sql,null,AVAILABLE,@Spark}
2019-09-12 11:14:15,029  INFO org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef 54 - Registered StateStoreCoordinator endpoint
2019-09-12 11:14:15,337  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0 stored as values in memory (estimated size 219.8 KB, free 899.5 MB)
2019-09-12 11:14:15,429  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 11:14:15,435  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_0_piece0 in memory on 10.6.1.14:5441 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 11:14:15,442  INFO org.apache.spark.SparkContext 54 - Created broadcast 0 from load at SparkUtil.java:34
2019-09-12 11:14:15,828  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 11:14:15,831  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 11:14:15,853  INFO org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 413 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 326
2019-09-12 11:14:15,918  INFO org.apache.spark.SparkContext 54 - Starting job: load at SparkUtil.java:34
2019-09-12 11:14:15,951  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 0 (load at SparkUtil.java:34) with 1 output partitions
2019-09-12 11:14:15,952  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 0 (load at SparkUtil.java:34)
2019-09-12 11:14:15,952  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 11:14:15,954  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 11:14:15,960  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:34), which has no missing parents
2019-09-12 11:14:15,982  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1 stored as values in memory (estimated size 5.5 KB, free 899.5 MB)
2019-09-12 11:14:15,989  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 899.5 MB)
2019-09-12 11:14:15,992  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_1_piece0 in memory on 10.6.1.14:5441 (size: 3.4 KB, free: 899.7 MB)
2019-09-12 11:14:15,993  INFO org.apache.spark.SparkContext 54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-09-12 11:14:16,023  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:34) (first 15 tasks are for partitions Vector(0))
2019-09-12 11:14:16,025  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 0.0 with 1 tasks
2019-09-12 11:14:16,095  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes)
2019-09-12 11:14:16,110  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 0.0 (TID 0)
2019-09-12 11:14:16,206  INFO org.apache.spark.rdd.BinaryFileRDD 54 - Input split: Paths:/D:/MyProject/splitCHword/target/classes/test.csv:0+326
2019-09-12 11:14:16,391  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 0.0 (TID 0). 896 bytes result sent to driver
2019-09-12 11:14:16,398  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 0.0 (TID 0) in 323 ms on localhost (executor driver) (1/1)
2019-09-12 11:14:16,401  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-09-12 11:14:16,407  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 0 (load at SparkUtil.java:34) finished in 0.350 s
2019-09-12 11:14:16,413  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 0 finished: load at SparkUtil.java:34, took 0.495502 s
2019-09-12 11:14:16,950  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_1_piece0 on 10.6.1.14:5441 in memory (size: 3.4 KB, free: 899.7 MB)
2019-09-12 11:14:18,534  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Pruning directories with: 
2019-09-12 11:14:18,538  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Post-Scan Filters: 
2019-09-12 11:14:18,541  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Output Data Schema: struct<feature: string>
2019-09-12 11:14:18,550  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Pushed Filters: 
2019-09-12 11:14:18,721  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_0_piece0 on 10.6.1.14:5441 in memory (size: 20.7 KB, free: 899.7 MB)
2019-09-12 11:14:19,251  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 311.407094 ms
2019-09-12 11:14:19,262  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2 stored as values in memory (estimated size 220.5 KB, free 899.5 MB)
2019-09-12 11:14:19,280  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 11:14:19,284  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_2_piece0 in memory on 10.6.1.14:5441 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 11:14:19,285  INFO org.apache.spark.SparkContext 54 - Created broadcast 2 from show at SparkUtil.java:57
2019-09-12 11:14:19,305  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2019-09-12 11:14:19,381  INFO org.apache.spark.SparkContext 54 - Starting job: show at SparkUtil.java:57
2019-09-12 11:14:19,383  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 1 (show at SparkUtil.java:57) with 1 output partitions
2019-09-12 11:14:19,383  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 1 (show at SparkUtil.java:57)
2019-09-12 11:14:19,383  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 11:14:19,383  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 11:14:19,384  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:57), which has no missing parents
2019-09-12 11:14:19,425  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3 stored as values in memory (estimated size 15.4 KB, free 899.4 MB)
2019-09-12 11:14:19,432  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.3 KB, free 899.4 MB)
2019-09-12 11:14:19,434  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_3_piece0 in memory on 10.6.1.14:5441 (size: 7.3 KB, free: 899.7 MB)
2019-09-12 11:14:19,435  INFO org.apache.spark.SparkContext 54 - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2019-09-12 11:14:19,436  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:57) (first 15 tasks are for partitions Vector(0))
2019-09-12 11:14:19,436  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 1.0 with 1 tasks
2019-09-12 11:14:19,449  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5293 bytes)
2019-09-12 11:14:19,450  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 1.0 (TID 1)
2019-09-12 11:14:19,479  INFO org.apache.spark.sql.execution.datasources.FileScanRDD 54 - Reading File path: file:///D:/MyProject/splitCHword/target/classes/test.csv, range: 0-326, partition values: [empty row]
2019-09-12 11:14:19,504  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 16.866545 ms
2019-09-12 11:14:19,768  INFO org.ansj.util.MyStaticValue 106 - init version.number to env value is : 2.11.8
2019-09-12 11:14:19,769  INFO org.ansj.util.MyStaticValue 106 - init copyright.string to env value is : Copyright 2002-2016, LAMP/EPFL
2019-09-12 11:14:19,769  INFO org.ansj.util.MyStaticValue 106 - init osgi.version.number to env value is : 2.11.8.v20160304-115712-1706a37eb8
2019-09-12 11:14:19,769  INFO org.ansj.util.MyStaticValue 106 - init maven.version.number to env value is : 2.11.8
2019-09-12 11:14:19,773  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/ambiguity.dic
2019-09-12 11:14:19,774 ERROR org.ansj.library.AmbiguityLibrary 117 - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:D:\MyProject\splitCHword\library\ambiguity.dic not found or can not to read, path: library/ambiguity.dic
2019-09-12 11:14:19,776  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/default.dic
2019-09-12 11:14:19,776 ERROR org.ansj.library.DicLibrary 221 - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:D:\MyProject\splitCHword\library\default.dic not found or can not to read, path: library/default.dic
2019-09-12 11:14:20,827  INFO org.ansj.library.DATDictionary 90 - init core library ok use time : 943
2019-09-12 11:14:21,304  INFO org.ansj.library.NgramLibrary 17 - init ngram ok use time :473
2019-09-12 11:14:21,319  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 1.0 (TID 1). 1429 bytes result sent to driver
2019-09-12 11:14:21,321  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 1.0 (TID 1) in 1884 ms on localhost (executor driver) (1/1)
2019-09-12 11:14:21,322  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-09-12 11:14:21,322  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 1 (show at SparkUtil.java:57) finished in 1.885 s
2019-09-12 11:14:21,326  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 1 finished: show at SparkUtil.java:57, took 1.943036 s
2019-09-12 11:14:21,350  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 10.473377 ms
2019-09-12 11:14:21,387  INFO org.apache.spark.SparkContext 54 - Invoking stop() from shutdown hook
2019-09-12 11:14:21,393  INFO org.spark_project.jetty.server.AbstractConnector 310 - Stopped Spark@297ea53a{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 11:14:21,396  INFO org.apache.spark.ui.SparkUI 54 - Stopped Spark web UI at http://10.6.1.14:4042
2019-09-12 11:14:21,458  INFO org.apache.spark.MapOutputTrackerMasterEndpoint 54 - MapOutputTrackerMasterEndpoint stopped!
2019-09-12 11:14:21,490  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore cleared
2019-09-12 11:14:21,490  INFO org.apache.spark.storage.BlockManager 54 - BlockManager stopped
2019-09-12 11:14:21,493  INFO org.apache.spark.storage.BlockManagerMaster 54 - BlockManagerMaster stopped
2019-09-12 11:14:21,495  INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint 54 - OutputCommitCoordinator stopped!
2019-09-12 11:14:21,502  INFO org.apache.spark.SparkContext 54 - Successfully stopped SparkContext
2019-09-12 11:14:21,502  INFO org.apache.spark.util.ShutdownHookManager 54 - Shutdown hook called
2019-09-12 11:14:21,503  INFO org.apache.spark.util.ShutdownHookManager 54 - Deleting directory C:\Users\OUY\AppData\Local\Temp\spark-1c97b767-1945-45dd-a40d-87e8401b761c
2019-09-12 11:14:39,070  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-12 11:14:39,756  WARN org.apache.hadoop.util.NativeCodeLoader 62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-12 11:14:39,850 ERROR org.apache.hadoop.util.Shell 396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2430)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
	at com.datamodel.SparkUtil.CreateSession(SparkUtil.java:25)
	at com.datamodel.SparkUtil.readDataFromCsv(SparkUtil.java:30)
	at com.datamodel.SparkUtil.main(SparkUtil.java:43)
2019-09-12 11:14:40,076  INFO org.apache.spark.SparkContext 54 - Submitted application: splitCHWord
2019-09-12 11:14:40,131  INFO org.apache.spark.SecurityManager 54 - Changing view acls to: OUY
2019-09-12 11:14:40,132  INFO org.apache.spark.SecurityManager 54 - Changing modify acls to: OUY
2019-09-12 11:14:40,133  INFO org.apache.spark.SecurityManager 54 - Changing view acls groups to: 
2019-09-12 11:14:40,135  INFO org.apache.spark.SecurityManager 54 - Changing modify acls groups to: 
2019-09-12 11:14:40,136  INFO org.apache.spark.SecurityManager 54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(OUY); groups with view permissions: Set(); users  with modify permissions: Set(OUY); groups with modify permissions: Set()
2019-09-12 11:14:40,905  INFO org.apache.spark.util.Utils 54 - Successfully started service 'sparkDriver' on port 5466.
2019-09-12 11:14:40,975  INFO org.apache.spark.SparkEnv 54 - Registering MapOutputTracker
2019-09-12 11:14:41,002  INFO org.apache.spark.SparkEnv 54 - Registering BlockManagerMaster
2019-09-12 11:14:41,007  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-09-12 11:14:41,008  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - BlockManagerMasterEndpoint up
2019-09-12 11:14:41,024  INFO org.apache.spark.storage.DiskBlockManager 54 - Created local directory at C:\Users\OUY\AppData\Local\Temp\blockmgr-2fc0e980-06ef-4e5f-b24e-3d1fddd78da5
2019-09-12 11:14:41,057  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore started with capacity 899.7 MB
2019-09-12 11:14:41,149  INFO org.apache.spark.SparkEnv 54 - Registering OutputCommitCoordinator
2019-09-12 11:14:41,278  INFO org.spark_project.jetty.util.log 192 - Logging initialized @4521ms
2019-09-12 11:14:41,346  INFO org.spark_project.jetty.server.Server 345 - jetty-9.3.z-SNAPSHOT
2019-09-12 11:14:41,364  INFO org.spark_project.jetty.server.Server 403 - Started @4609ms
2019-09-12 11:14:41,380  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2019-09-12 11:14:41,381  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
2019-09-12 11:14:41,390  INFO org.spark_project.jetty.server.AbstractConnector 270 - Started ServerConnector@4109f738{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 11:14:41,390  INFO org.apache.spark.util.Utils 54 - Successfully started service 'SparkUI' on port 4042.
2019-09-12 11:14:41,424  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@305f031{/jobs,null,AVAILABLE,@Spark}
2019-09-12 11:14:41,425  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@36b0fcd5{/jobs/json,null,AVAILABLE,@Spark}
2019-09-12 11:14:41,426  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@475835b1{/jobs/job,null,AVAILABLE,@Spark}
2019-09-12 11:14:41,427  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@716a7124{/jobs/job/json,null,AVAILABLE,@Spark}
2019-09-12 11:14:41,428  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@226642a5{/stages,null,AVAILABLE,@Spark}
2019-09-12 11:14:41,429  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5cc126dc{/stages/json,null,AVAILABLE,@Spark}
2019-09-12 11:14:41,430  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@72bd06ca{/stages/stage,null,AVAILABLE,@Spark}
2019-09-12 11:14:41,431  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@55795845{/stages/stage/json,null,AVAILABLE,@Spark}
2019-09-12 11:14:41,432  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@119f1f2a{/stages/pool,null,AVAILABLE,@Spark}
2019-09-12 11:14:41,434  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5b970f7{/stages/pool/json,null,AVAILABLE,@Spark}
2019-09-12 11:14:41,435  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@165b8a71{/storage,null,AVAILABLE,@Spark}
2019-09-12 11:14:41,435  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2f058b8a{/storage/json,null,AVAILABLE,@Spark}
2019-09-12 11:14:41,436  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3f2ef586{/storage/rdd,null,AVAILABLE,@Spark}
2019-09-12 11:14:41,437  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@76c7beb3{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-09-12 11:14:41,438  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2cf92cc7{/environment,null,AVAILABLE,@Spark}
2019-09-12 11:14:41,441  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7b139eab{/environment/json,null,AVAILABLE,@Spark}
2019-09-12 11:14:41,443  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@611df6e3{/executors,null,AVAILABLE,@Spark}
2019-09-12 11:14:41,444  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6273c5a4{/executors/json,null,AVAILABLE,@Spark}
2019-09-12 11:14:41,445  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@53e211ee{/executors/threadDump,null,AVAILABLE,@Spark}
2019-09-12 11:14:41,446  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3d8bbcdc{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-09-12 11:14:41,456  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@117e0fe5{/static,null,AVAILABLE,@Spark}
2019-09-12 11:14:41,457  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6c67e137{/,null,AVAILABLE,@Spark}
2019-09-12 11:14:41,460  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@191ae03f{/api,null,AVAILABLE,@Spark}
2019-09-12 11:14:41,461  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@172ca72b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-09-12 11:14:41,462  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@71e5f61d{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-09-12 11:14:41,465  INFO org.apache.spark.ui.SparkUI 54 - Bound SparkUI to 0.0.0.0, and started at http://10.6.1.14:4042
2019-09-12 11:14:41,630  INFO org.apache.spark.executor.Executor 54 - Starting executor ID driver on host localhost
2019-09-12 11:14:41,693  INFO org.apache.spark.util.Utils 54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 5479.
2019-09-12 11:14:41,694  INFO org.apache.spark.network.netty.NettyBlockTransferService 54 - Server created on 10.6.1.14:5479
2019-09-12 11:14:41,696  INFO org.apache.spark.storage.BlockManager 54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-09-12 11:14:41,706  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registering BlockManager BlockManagerId(driver, 10.6.1.14, 5479, None)
2019-09-12 11:14:41,713  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Registering block manager 10.6.1.14:5479 with 899.7 MB RAM, BlockManagerId(driver, 10.6.1.14, 5479, None)
2019-09-12 11:14:41,720  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registered BlockManager BlockManagerId(driver, 10.6.1.14, 5479, None)
2019-09-12 11:14:41,721  INFO org.apache.spark.storage.BlockManager 54 - Initialized BlockManager: BlockManagerId(driver, 10.6.1.14, 5479, None)
2019-09-12 11:14:42,042  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@721eb7df{/metrics/json,null,AVAILABLE,@Spark}
2019-09-12 11:14:42,149  INFO org.apache.spark.sql.internal.SharedState 54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/MyProject/splitCHword/spark-warehouse/').
2019-09-12 11:14:42,151  INFO org.apache.spark.sql.internal.SharedState 54 - Warehouse path is 'file:/D:/MyProject/splitCHword/spark-warehouse/'.
2019-09-12 11:14:42,161  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@16943e88{/SQL,null,AVAILABLE,@Spark}
2019-09-12 11:14:42,161  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@73d6d0c{/SQL/json,null,AVAILABLE,@Spark}
2019-09-12 11:14:42,163  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@d2387c8{/SQL/execution,null,AVAILABLE,@Spark}
2019-09-12 11:14:42,165  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1500e009{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-09-12 11:14:42,168  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@673bb956{/static/sql,null,AVAILABLE,@Spark}
2019-09-12 11:14:43,321  INFO org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef 54 - Registered StateStoreCoordinator endpoint
2019-09-12 11:14:43,686  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0 stored as values in memory (estimated size 219.8 KB, free 899.5 MB)
2019-09-12 11:14:43,790  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 11:14:43,796  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_0_piece0 in memory on 10.6.1.14:5479 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 11:14:43,802  INFO org.apache.spark.SparkContext 54 - Created broadcast 0 from load at SparkUtil.java:34
2019-09-12 11:14:44,226  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 11:14:44,228  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 11:14:44,244  INFO org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 413 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 326
2019-09-12 11:14:44,297  INFO org.apache.spark.SparkContext 54 - Starting job: load at SparkUtil.java:34
2019-09-12 11:14:44,318  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 0 (load at SparkUtil.java:34) with 1 output partitions
2019-09-12 11:14:44,318  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 0 (load at SparkUtil.java:34)
2019-09-12 11:14:44,319  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 11:14:44,320  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 11:14:44,327  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:34), which has no missing parents
2019-09-12 11:14:44,362  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1 stored as values in memory (estimated size 5.5 KB, free 899.5 MB)
2019-09-12 11:14:44,368  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 899.5 MB)
2019-09-12 11:14:44,370  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_1_piece0 in memory on 10.6.1.14:5479 (size: 3.4 KB, free: 899.7 MB)
2019-09-12 11:14:44,372  INFO org.apache.spark.SparkContext 54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-09-12 11:14:44,399  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:34) (first 15 tasks are for partitions Vector(0))
2019-09-12 11:14:44,401  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 0.0 with 1 tasks
2019-09-12 11:14:44,467  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes)
2019-09-12 11:14:44,484  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 0.0 (TID 0)
2019-09-12 11:14:44,589  INFO org.apache.spark.rdd.BinaryFileRDD 54 - Input split: Paths:/D:/MyProject/splitCHword/target/classes/test.csv:0+326
2019-09-12 11:14:44,780  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 0.0 (TID 0). 896 bytes result sent to driver
2019-09-12 11:14:44,789  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 0.0 (TID 0) in 342 ms on localhost (executor driver) (1/1)
2019-09-12 11:14:44,791  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-09-12 11:14:44,796  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 0 (load at SparkUtil.java:34) finished in 0.373 s
2019-09-12 11:14:44,805  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 0 finished: load at SparkUtil.java:34, took 0.507322 s
2019-09-12 11:14:45,356  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_1_piece0 on 10.6.1.14:5479 in memory (size: 3.4 KB, free: 899.7 MB)
2019-09-12 11:14:47,130  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Pruning directories with: 
2019-09-12 11:14:47,137  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Post-Scan Filters: 
2019-09-12 11:14:47,142  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Output Data Schema: struct<feature: string>
2019-09-12 11:14:47,151  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Pushed Filters: 
2019-09-12 11:14:47,356  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_0_piece0 on 10.6.1.14:5479 in memory (size: 20.7 KB, free: 899.7 MB)
2019-09-12 11:14:47,876  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 353.389352 ms
2019-09-12 11:14:47,892  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2 stored as values in memory (estimated size 220.5 KB, free 899.5 MB)
2019-09-12 11:14:47,921  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 11:14:47,926  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_2_piece0 in memory on 10.6.1.14:5479 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 11:14:47,927  INFO org.apache.spark.SparkContext 54 - Created broadcast 2 from show at SparkUtil.java:57
2019-09-12 11:14:47,943  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2019-09-12 11:14:48,017  INFO org.apache.spark.SparkContext 54 - Starting job: show at SparkUtil.java:57
2019-09-12 11:14:48,020  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 1 (show at SparkUtil.java:57) with 1 output partitions
2019-09-12 11:14:48,020  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 1 (show at SparkUtil.java:57)
2019-09-12 11:14:48,020  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 11:14:48,021  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 11:14:48,022  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:57), which has no missing parents
2019-09-12 11:14:48,048  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3 stored as values in memory (estimated size 15.4 KB, free 899.4 MB)
2019-09-12 11:14:48,091  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.3 KB, free 899.4 MB)
2019-09-12 11:14:48,092  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_3_piece0 in memory on 10.6.1.14:5479 (size: 7.3 KB, free: 899.7 MB)
2019-09-12 11:14:48,093  INFO org.apache.spark.SparkContext 54 - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2019-09-12 11:14:48,095  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:57) (first 15 tasks are for partitions Vector(0))
2019-09-12 11:14:48,095  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 1.0 with 1 tasks
2019-09-12 11:14:48,108  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5293 bytes)
2019-09-12 11:14:48,109  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 1.0 (TID 1)
2019-09-12 11:14:48,136  INFO org.apache.spark.sql.execution.datasources.FileScanRDD 54 - Reading File path: file:///D:/MyProject/splitCHword/target/classes/test.csv, range: 0-326, partition values: [empty row]
2019-09-12 11:14:48,164  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 19.000731 ms
2019-09-12 11:14:48,445  INFO org.ansj.util.MyStaticValue 106 - init version.number to env value is : 2.11.8
2019-09-12 11:14:48,445  INFO org.ansj.util.MyStaticValue 106 - init copyright.string to env value is : Copyright 2002-2016, LAMP/EPFL
2019-09-12 11:14:48,446  INFO org.ansj.util.MyStaticValue 106 - init osgi.version.number to env value is : 2.11.8.v20160304-115712-1706a37eb8
2019-09-12 11:14:48,446  INFO org.ansj.util.MyStaticValue 106 - init maven.version.number to env value is : 2.11.8
2019-09-12 11:14:48,450  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/ambiguity.dic
2019-09-12 11:14:48,450 ERROR org.ansj.library.AmbiguityLibrary 117 - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:D:\MyProject\splitCHword\library\ambiguity.dic not found or can not to read, path: library/ambiguity.dic
2019-09-12 11:14:48,452  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/default.dic
2019-09-12 11:14:48,452 ERROR org.ansj.library.DicLibrary 221 - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:D:\MyProject\splitCHword\library\default.dic not found or can not to read, path: library/default.dic
2019-09-12 11:14:49,660  INFO org.ansj.library.DATDictionary 90 - init core library ok use time : 1104
2019-09-12 11:14:50,141  INFO org.ansj.library.NgramLibrary 17 - init ngram ok use time :477
2019-09-12 11:14:50,154  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 1.0 (TID 1). 1429 bytes result sent to driver
2019-09-12 11:14:50,207  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 1.0 (TID 1) in 2110 ms on localhost (executor driver) (1/1)
2019-09-12 11:14:50,207  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-09-12 11:14:50,208  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 1 (show at SparkUtil.java:57) finished in 2.112 s
2019-09-12 11:14:50,209  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 1 finished: show at SparkUtil.java:57, took 2.191159 s
2019-09-12 11:14:50,236  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 12.614389 ms
2019-09-12 11:14:50,289  INFO org.apache.spark.SparkContext 54 - Invoking stop() from shutdown hook
2019-09-12 11:14:50,299  INFO org.spark_project.jetty.server.AbstractConnector 310 - Stopped Spark@4109f738{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 11:14:50,303  INFO org.apache.spark.ui.SparkUI 54 - Stopped Spark web UI at http://10.6.1.14:4042
2019-09-12 11:14:50,323  INFO org.apache.spark.MapOutputTrackerMasterEndpoint 54 - MapOutputTrackerMasterEndpoint stopped!
2019-09-12 11:14:50,386  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore cleared
2019-09-12 11:14:50,387  INFO org.apache.spark.storage.BlockManager 54 - BlockManager stopped
2019-09-12 11:14:50,392  INFO org.apache.spark.storage.BlockManagerMaster 54 - BlockManagerMaster stopped
2019-09-12 11:14:50,415  INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint 54 - OutputCommitCoordinator stopped!
2019-09-12 11:14:50,426  INFO org.apache.spark.SparkContext 54 - Successfully stopped SparkContext
2019-09-12 11:14:50,427  INFO org.apache.spark.util.ShutdownHookManager 54 - Shutdown hook called
2019-09-12 11:14:50,430  INFO org.apache.spark.util.ShutdownHookManager 54 - Deleting directory C:\Users\OUY\AppData\Local\Temp\spark-4cf0de8a-27fb-45a0-883a-c8c47dac3d80
2019-09-12 11:15:07,173  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-12 11:15:07,863  WARN org.apache.hadoop.util.NativeCodeLoader 62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-12 11:15:07,951 ERROR org.apache.hadoop.util.Shell 396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2430)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
	at com.datamodel.SparkUtil.CreateSession(SparkUtil.java:25)
	at com.datamodel.SparkUtil.readDataFromCsv(SparkUtil.java:30)
	at com.datamodel.SparkUtil.main(SparkUtil.java:43)
2019-09-12 11:15:08,127  INFO org.apache.spark.SparkContext 54 - Submitted application: splitCHWord
2019-09-12 11:15:08,172  INFO org.apache.spark.SecurityManager 54 - Changing view acls to: OUY
2019-09-12 11:15:08,173  INFO org.apache.spark.SecurityManager 54 - Changing modify acls to: OUY
2019-09-12 11:15:08,175  INFO org.apache.spark.SecurityManager 54 - Changing view acls groups to: 
2019-09-12 11:15:08,176  INFO org.apache.spark.SecurityManager 54 - Changing modify acls groups to: 
2019-09-12 11:15:08,177  INFO org.apache.spark.SecurityManager 54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(OUY); groups with view permissions: Set(); users  with modify permissions: Set(OUY); groups with modify permissions: Set()
2019-09-12 11:15:09,086  INFO org.apache.spark.util.Utils 54 - Successfully started service 'sparkDriver' on port 5511.
2019-09-12 11:15:09,142  INFO org.apache.spark.SparkEnv 54 - Registering MapOutputTracker
2019-09-12 11:15:09,166  INFO org.apache.spark.SparkEnv 54 - Registering BlockManagerMaster
2019-09-12 11:15:09,170  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-09-12 11:15:09,171  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - BlockManagerMasterEndpoint up
2019-09-12 11:15:09,186  INFO org.apache.spark.storage.DiskBlockManager 54 - Created local directory at C:\Users\OUY\AppData\Local\Temp\blockmgr-5aaddb28-88ec-462b-9ce7-f1d2b3a039eb
2019-09-12 11:15:09,213  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore started with capacity 899.7 MB
2019-09-12 11:15:09,281  INFO org.apache.spark.SparkEnv 54 - Registering OutputCommitCoordinator
2019-09-12 11:15:09,392  INFO org.spark_project.jetty.util.log 192 - Logging initialized @4576ms
2019-09-12 11:15:09,461  INFO org.spark_project.jetty.server.Server 345 - jetty-9.3.z-SNAPSHOT
2019-09-12 11:15:09,477  INFO org.spark_project.jetty.server.Server 403 - Started @4662ms
2019-09-12 11:15:09,493  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2019-09-12 11:15:09,494  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
2019-09-12 11:15:09,503  INFO org.spark_project.jetty.server.AbstractConnector 270 - Started ServerConnector@297ea53a{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 11:15:09,504  INFO org.apache.spark.util.Utils 54 - Successfully started service 'SparkUI' on port 4042.
2019-09-12 11:15:09,536  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@592e843a{/jobs,null,AVAILABLE,@Spark}
2019-09-12 11:15:09,537  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4fad94a7{/jobs/json,null,AVAILABLE,@Spark}
2019-09-12 11:15:09,538  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6326d182{/jobs/job,null,AVAILABLE,@Spark}
2019-09-12 11:15:09,540  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@77192705{/jobs/job/json,null,AVAILABLE,@Spark}
2019-09-12 11:15:09,541  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7e809b79{/stages,null,AVAILABLE,@Spark}
2019-09-12 11:15:09,542  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@625e134e{/stages/json,null,AVAILABLE,@Spark}
2019-09-12 11:15:09,542  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@89c10b7{/stages/stage,null,AVAILABLE,@Spark}
2019-09-12 11:15:09,544  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3d08f3f5{/stages/stage/json,null,AVAILABLE,@Spark}
2019-09-12 11:15:09,546  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1a1da881{/stages/pool,null,AVAILABLE,@Spark}
2019-09-12 11:15:09,547  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7fd4acee{/stages/pool/json,null,AVAILABLE,@Spark}
2019-09-12 11:15:09,548  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6175619b{/storage,null,AVAILABLE,@Spark}
2019-09-12 11:15:09,549  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@756cf158{/storage/json,null,AVAILABLE,@Spark}
2019-09-12 11:15:09,550  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@751d3241{/storage/rdd,null,AVAILABLE,@Spark}
2019-09-12 11:15:09,551  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@64337702{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-09-12 11:15:09,552  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@30ea8c23{/environment,null,AVAILABLE,@Spark}
2019-09-12 11:15:09,554  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4e76dac{/environment/json,null,AVAILABLE,@Spark}
2019-09-12 11:15:09,555  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5f2f577{/executors,null,AVAILABLE,@Spark}
2019-09-12 11:15:09,556  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5d465e4b{/executors/json,null,AVAILABLE,@Spark}
2019-09-12 11:15:09,557  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@41a90fa8{/executors/threadDump,null,AVAILABLE,@Spark}
2019-09-12 11:15:09,558  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@52500920{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-09-12 11:15:09,570  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@18a3962d{/static,null,AVAILABLE,@Spark}
2019-09-12 11:15:09,572  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2c9399a4{/,null,AVAILABLE,@Spark}
2019-09-12 11:15:09,573  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@9635fa{/api,null,AVAILABLE,@Spark}
2019-09-12 11:15:09,574  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5bda80bf{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-09-12 11:15:09,575  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2ce86164{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-09-12 11:15:09,578  INFO org.apache.spark.ui.SparkUI 54 - Bound SparkUI to 0.0.0.0, and started at http://10.6.1.14:4042
2019-09-12 11:15:09,782  INFO org.apache.spark.executor.Executor 54 - Starting executor ID driver on host localhost
2019-09-12 11:15:09,824  INFO org.apache.spark.util.Utils 54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 5524.
2019-09-12 11:15:09,825  INFO org.apache.spark.network.netty.NettyBlockTransferService 54 - Server created on 10.6.1.14:5524
2019-09-12 11:15:09,833  INFO org.apache.spark.storage.BlockManager 54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-09-12 11:15:09,838  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registering BlockManager BlockManagerId(driver, 10.6.1.14, 5524, None)
2019-09-12 11:15:09,844  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Registering block manager 10.6.1.14:5524 with 899.7 MB RAM, BlockManagerId(driver, 10.6.1.14, 5524, None)
2019-09-12 11:15:09,850  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registered BlockManager BlockManagerId(driver, 10.6.1.14, 5524, None)
2019-09-12 11:15:09,850  INFO org.apache.spark.storage.BlockManager 54 - Initialized BlockManager: BlockManagerId(driver, 10.6.1.14, 5524, None)
2019-09-12 11:15:10,078  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1ea9f009{/metrics/json,null,AVAILABLE,@Spark}
2019-09-12 11:15:10,171  INFO org.apache.spark.sql.internal.SharedState 54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/MyProject/splitCHword/spark-warehouse/').
2019-09-12 11:15:10,172  INFO org.apache.spark.sql.internal.SharedState 54 - Warehouse path is 'file:/D:/MyProject/splitCHword/spark-warehouse/'.
2019-09-12 11:15:10,209  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3ae66c85{/SQL,null,AVAILABLE,@Spark}
2019-09-12 11:15:10,216  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4604b900{/SQL/json,null,AVAILABLE,@Spark}
2019-09-12 11:15:10,227  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@367795c7{/SQL/execution,null,AVAILABLE,@Spark}
2019-09-12 11:15:10,242  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3956b302{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-09-12 11:15:10,246  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@13e9f2e2{/static/sql,null,AVAILABLE,@Spark}
2019-09-12 11:15:11,725  INFO org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef 54 - Registered StateStoreCoordinator endpoint
2019-09-12 11:15:12,054  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0 stored as values in memory (estimated size 219.8 KB, free 899.5 MB)
2019-09-12 11:15:12,142  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 11:15:12,148  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_0_piece0 in memory on 10.6.1.14:5524 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 11:15:12,153  INFO org.apache.spark.SparkContext 54 - Created broadcast 0 from load at SparkUtil.java:34
2019-09-12 11:15:12,521  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 11:15:12,523  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 11:15:12,540  INFO org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 413 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 326
2019-09-12 11:15:12,592  INFO org.apache.spark.SparkContext 54 - Starting job: load at SparkUtil.java:34
2019-09-12 11:15:12,613  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 0 (load at SparkUtil.java:34) with 1 output partitions
2019-09-12 11:15:12,614  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 0 (load at SparkUtil.java:34)
2019-09-12 11:15:12,614  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 11:15:12,616  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 11:15:12,622  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:34), which has no missing parents
2019-09-12 11:15:12,664  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1 stored as values in memory (estimated size 5.5 KB, free 899.5 MB)
2019-09-12 11:15:12,672  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 899.5 MB)
2019-09-12 11:15:12,674  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_1_piece0 in memory on 10.6.1.14:5524 (size: 3.4 KB, free: 899.7 MB)
2019-09-12 11:15:12,677  INFO org.apache.spark.SparkContext 54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-09-12 11:15:12,710  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:34) (first 15 tasks are for partitions Vector(0))
2019-09-12 11:15:12,712  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 0.0 with 1 tasks
2019-09-12 11:15:12,786  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes)
2019-09-12 11:15:12,801  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 0.0 (TID 0)
2019-09-12 11:15:12,891  INFO org.apache.spark.rdd.BinaryFileRDD 54 - Input split: Paths:/D:/MyProject/splitCHword/target/classes/test.csv:0+326
2019-09-12 11:15:13,076  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 0.0 (TID 0). 853 bytes result sent to driver
2019-09-12 11:15:13,083  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 0.0 (TID 0) in 318 ms on localhost (executor driver) (1/1)
2019-09-12 11:15:13,086  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-09-12 11:15:13,092  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 0 (load at SparkUtil.java:34) finished in 0.361 s
2019-09-12 11:15:13,099  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 0 finished: load at SparkUtil.java:34, took 0.507151 s
2019-09-12 11:15:13,733  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_0_piece0 on 10.6.1.14:5524 in memory (size: 20.7 KB, free: 899.7 MB)
2019-09-12 11:15:13,741  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_1_piece0 on 10.6.1.14:5524 in memory (size: 3.4 KB, free: 899.7 MB)
2019-09-12 11:15:15,292  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Pruning directories with: 
2019-09-12 11:15:15,296  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Post-Scan Filters: 
2019-09-12 11:15:15,300  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Output Data Schema: struct<feature: string>
2019-09-12 11:15:15,311  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Pushed Filters: 
2019-09-12 11:15:15,990  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 306.971898 ms
2019-09-12 11:15:16,004  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2 stored as values in memory (estimated size 220.5 KB, free 899.5 MB)
2019-09-12 11:15:16,029  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 11:15:16,033  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_2_piece0 in memory on 10.6.1.14:5524 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 11:15:16,035  INFO org.apache.spark.SparkContext 54 - Created broadcast 2 from show at SparkUtil.java:57
2019-09-12 11:15:16,052  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2019-09-12 11:15:16,114  INFO org.apache.spark.SparkContext 54 - Starting job: show at SparkUtil.java:57
2019-09-12 11:15:16,115  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 1 (show at SparkUtil.java:57) with 1 output partitions
2019-09-12 11:15:16,115  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 1 (show at SparkUtil.java:57)
2019-09-12 11:15:16,115  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 11:15:16,116  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 11:15:16,117  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:57), which has no missing parents
2019-09-12 11:15:16,142  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3 stored as values in memory (estimated size 15.4 KB, free 899.4 MB)
2019-09-12 11:15:16,147  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.3 KB, free 899.4 MB)
2019-09-12 11:15:16,148  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_3_piece0 in memory on 10.6.1.14:5524 (size: 7.3 KB, free: 899.7 MB)
2019-09-12 11:15:16,149  INFO org.apache.spark.SparkContext 54 - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2019-09-12 11:15:16,150  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:57) (first 15 tasks are for partitions Vector(0))
2019-09-12 11:15:16,150  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 1.0 with 1 tasks
2019-09-12 11:15:16,158  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5293 bytes)
2019-09-12 11:15:16,158  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 1.0 (TID 1)
2019-09-12 11:15:16,176  INFO org.apache.spark.sql.execution.datasources.FileScanRDD 54 - Reading File path: file:///D:/MyProject/splitCHword/target/classes/test.csv, range: 0-326, partition values: [empty row]
2019-09-12 11:15:16,197  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 16.821746 ms
2019-09-12 11:15:16,480  INFO org.ansj.util.MyStaticValue 106 - init version.number to env value is : 2.11.8
2019-09-12 11:15:16,480  INFO org.ansj.util.MyStaticValue 106 - init copyright.string to env value is : Copyright 2002-2016, LAMP/EPFL
2019-09-12 11:15:16,480  INFO org.ansj.util.MyStaticValue 106 - init osgi.version.number to env value is : 2.11.8.v20160304-115712-1706a37eb8
2019-09-12 11:15:16,481  INFO org.ansj.util.MyStaticValue 106 - init maven.version.number to env value is : 2.11.8
2019-09-12 11:15:16,484  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/ambiguity.dic
2019-09-12 11:15:16,485 ERROR org.ansj.library.AmbiguityLibrary 117 - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:D:\MyProject\splitCHword\library\ambiguity.dic not found or can not to read, path: library/ambiguity.dic
2019-09-12 11:15:16,486  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/default.dic
2019-09-12 11:15:16,486 ERROR org.ansj.library.DicLibrary 221 - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:D:\MyProject\splitCHword\library\default.dic not found or can not to read, path: library/default.dic
2019-09-12 11:15:17,636  INFO org.ansj.library.DATDictionary 90 - init core library ok use time : 1044
2019-09-12 11:15:18,169  INFO org.ansj.library.NgramLibrary 17 - init ngram ok use time :528
2019-09-12 11:15:18,181  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 1.0 (TID 1). 1429 bytes result sent to driver
2019-09-12 11:15:18,183  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 1.0 (TID 1) in 2032 ms on localhost (executor driver) (1/1)
2019-09-12 11:15:18,184  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-09-12 11:15:18,234  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 1 (show at SparkUtil.java:57) finished in 2.083 s
2019-09-12 11:15:18,235  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 1 finished: show at SparkUtil.java:57, took 2.121011 s
2019-09-12 11:15:18,267  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 16.675399 ms
2019-09-12 11:15:18,293  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Pruning directories with: 
2019-09-12 11:15:18,294  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Post-Scan Filters: 
2019-09-12 11:15:18,294  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Output Data Schema: struct<label: string, feature: string>
2019-09-12 11:15:18,294  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Pushed Filters: 
2019-09-12 11:15:18,303  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 6.436262 ms
2019-09-12 11:15:18,309  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_4 stored as values in memory (estimated size 220.5 KB, free 899.2 MB)
2019-09-12 11:15:18,331  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_4_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.2 MB)
2019-09-12 11:15:18,370  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_4_piece0 in memory on 10.6.1.14:5524 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 11:15:18,371  INFO org.apache.spark.SparkContext 54 - Created broadcast 4 from toJSON at SparkUtil.java:66
2019-09-12 11:15:18,372  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2019-09-12 11:15:18,736  INFO org.apache.spark.SparkContext 54 - Invoking stop() from shutdown hook
2019-09-12 11:15:18,742  INFO org.spark_project.jetty.server.AbstractConnector 310 - Stopped Spark@297ea53a{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 11:15:18,745  INFO org.apache.spark.ui.SparkUI 54 - Stopped Spark web UI at http://10.6.1.14:4042
2019-09-12 11:15:18,758  INFO org.apache.spark.MapOutputTrackerMasterEndpoint 54 - MapOutputTrackerMasterEndpoint stopped!
2019-09-12 11:15:18,799  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore cleared
2019-09-12 11:15:18,800  INFO org.apache.spark.storage.BlockManager 54 - BlockManager stopped
2019-09-12 11:15:18,802  INFO org.apache.spark.storage.BlockManagerMaster 54 - BlockManagerMaster stopped
2019-09-12 11:15:18,805  INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint 54 - OutputCommitCoordinator stopped!
2019-09-12 11:15:18,811  INFO org.apache.spark.SparkContext 54 - Successfully stopped SparkContext
2019-09-12 11:15:18,811  INFO org.apache.spark.util.ShutdownHookManager 54 - Shutdown hook called
2019-09-12 11:15:18,812  INFO org.apache.spark.util.ShutdownHookManager 54 - Deleting directory C:\Users\OUY\AppData\Local\Temp\spark-220f197f-8642-45aa-8a28-4d0e190bc8f4
2019-09-12 11:22:51,739  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-12 11:22:52,424  WARN org.apache.hadoop.util.NativeCodeLoader 62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-12 11:22:52,610 ERROR org.apache.hadoop.util.Shell 396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2430)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
	at com.datamodel.SparkUtil.CreateSession(SparkUtil.java:25)
	at com.datamodel.SparkUtil.readDataFromCsv(SparkUtil.java:30)
	at com.datamodel.SparkUtil.main(SparkUtil.java:43)
2019-09-12 11:22:52,755  INFO org.apache.spark.SparkContext 54 - Submitted application: splitCHWord
2019-09-12 11:22:52,790  INFO org.apache.spark.SecurityManager 54 - Changing view acls to: OUY
2019-09-12 11:22:52,791  INFO org.apache.spark.SecurityManager 54 - Changing modify acls to: OUY
2019-09-12 11:22:52,791  INFO org.apache.spark.SecurityManager 54 - Changing view acls groups to: 
2019-09-12 11:22:52,792  INFO org.apache.spark.SecurityManager 54 - Changing modify acls groups to: 
2019-09-12 11:22:52,793  INFO org.apache.spark.SecurityManager 54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(OUY); groups with view permissions: Set(); users  with modify permissions: Set(OUY); groups with modify permissions: Set()
2019-09-12 11:22:53,498  INFO org.apache.spark.util.Utils 54 - Successfully started service 'sparkDriver' on port 5598.
2019-09-12 11:22:53,545  INFO org.apache.spark.SparkEnv 54 - Registering MapOutputTracker
2019-09-12 11:22:53,577  INFO org.apache.spark.SparkEnv 54 - Registering BlockManagerMaster
2019-09-12 11:22:53,577  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-09-12 11:22:53,577  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - BlockManagerMasterEndpoint up
2019-09-12 11:22:53,592  INFO org.apache.spark.storage.DiskBlockManager 54 - Created local directory at C:\Users\OUY\AppData\Local\Temp\blockmgr-d257b9d6-5cf8-4ff9-9826-83a4b965f4ab
2019-09-12 11:22:53,623  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore started with capacity 899.7 MB
2019-09-12 11:22:53,686  INFO org.apache.spark.SparkEnv 54 - Registering OutputCommitCoordinator
2019-09-12 11:22:53,827  INFO org.spark_project.jetty.util.log 192 - Logging initialized @4154ms
2019-09-12 11:22:53,905  INFO org.spark_project.jetty.server.Server 345 - jetty-9.3.z-SNAPSHOT
2019-09-12 11:22:53,920  INFO org.spark_project.jetty.server.Server 403 - Started @4243ms
2019-09-12 11:22:53,936  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2019-09-12 11:22:53,936  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
2019-09-12 11:22:53,936  INFO org.spark_project.jetty.server.AbstractConnector 270 - Started ServerConnector@791933f7{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 11:22:53,936  INFO org.apache.spark.util.Utils 54 - Successfully started service 'SparkUI' on port 4042.
2019-09-12 11:22:53,983  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@592e843a{/jobs,null,AVAILABLE,@Spark}
2019-09-12 11:22:53,983  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4fad94a7{/jobs/json,null,AVAILABLE,@Spark}
2019-09-12 11:22:53,983  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6326d182{/jobs/job,null,AVAILABLE,@Spark}
2019-09-12 11:22:53,983  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@77192705{/jobs/job/json,null,AVAILABLE,@Spark}
2019-09-12 11:22:53,983  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7e809b79{/stages,null,AVAILABLE,@Spark}
2019-09-12 11:22:53,983  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@625e134e{/stages/json,null,AVAILABLE,@Spark}
2019-09-12 11:22:53,983  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@89c10b7{/stages/stage,null,AVAILABLE,@Spark}
2019-09-12 11:22:53,998  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3d08f3f5{/stages/stage/json,null,AVAILABLE,@Spark}
2019-09-12 11:22:53,998  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1a1da881{/stages/pool,null,AVAILABLE,@Spark}
2019-09-12 11:22:53,998  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7fd4acee{/stages/pool/json,null,AVAILABLE,@Spark}
2019-09-12 11:22:53,998  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6175619b{/storage,null,AVAILABLE,@Spark}
2019-09-12 11:22:53,998  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@756cf158{/storage/json,null,AVAILABLE,@Spark}
2019-09-12 11:22:53,998  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@751d3241{/storage/rdd,null,AVAILABLE,@Spark}
2019-09-12 11:22:53,998  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@64337702{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-09-12 11:22:53,998  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@30ea8c23{/environment,null,AVAILABLE,@Spark}
2019-09-12 11:22:53,998  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4e76dac{/environment/json,null,AVAILABLE,@Spark}
2019-09-12 11:22:53,998  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5f2f577{/executors,null,AVAILABLE,@Spark}
2019-09-12 11:22:53,998  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5d465e4b{/executors/json,null,AVAILABLE,@Spark}
2019-09-12 11:22:53,998  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@41a90fa8{/executors/threadDump,null,AVAILABLE,@Spark}
2019-09-12 11:22:54,014  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@52500920{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-09-12 11:22:54,014  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@18a3962d{/static,null,AVAILABLE,@Spark}
2019-09-12 11:22:54,014  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2c9399a4{/,null,AVAILABLE,@Spark}
2019-09-12 11:22:54,014  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@9635fa{/api,null,AVAILABLE,@Spark}
2019-09-12 11:22:54,014  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5bda80bf{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-09-12 11:22:54,014  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2ce86164{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-09-12 11:22:54,030  INFO org.apache.spark.ui.SparkUI 54 - Bound SparkUI to 0.0.0.0, and started at http://10.6.1.14:4042
2019-09-12 11:22:54,186  INFO org.apache.spark.executor.Executor 54 - Starting executor ID driver on host localhost
2019-09-12 11:22:54,217  INFO org.apache.spark.util.Utils 54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 5611.
2019-09-12 11:22:54,217  INFO org.apache.spark.network.netty.NettyBlockTransferService 54 - Server created on 10.6.1.14:5611
2019-09-12 11:22:54,217  INFO org.apache.spark.storage.BlockManager 54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-09-12 11:22:54,233  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registering BlockManager BlockManagerId(driver, 10.6.1.14, 5611, None)
2019-09-12 11:22:54,233  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Registering block manager 10.6.1.14:5611 with 899.7 MB RAM, BlockManagerId(driver, 10.6.1.14, 5611, None)
2019-09-12 11:22:54,248  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registered BlockManager BlockManagerId(driver, 10.6.1.14, 5611, None)
2019-09-12 11:22:54,248  INFO org.apache.spark.storage.BlockManager 54 - Initialized BlockManager: BlockManagerId(driver, 10.6.1.14, 5611, None)
2019-09-12 11:22:54,498  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1ea9f009{/metrics/json,null,AVAILABLE,@Spark}
2019-09-12 11:22:54,576  INFO org.apache.spark.sql.internal.SharedState 54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/MyProject/splitCHword/spark-warehouse/').
2019-09-12 11:22:54,576  INFO org.apache.spark.sql.internal.SharedState 54 - Warehouse path is 'file:/D:/MyProject/splitCHword/spark-warehouse/'.
2019-09-12 11:22:54,592  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4604b900{/SQL,null,AVAILABLE,@Spark}
2019-09-12 11:22:54,592  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@e36bb2a{/SQL/json,null,AVAILABLE,@Spark}
2019-09-12 11:22:54,592  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3956b302{/SQL/execution,null,AVAILABLE,@Spark}
2019-09-12 11:22:54,592  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1fd386c3{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-09-12 11:22:54,592  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@cd7f1ae{/static/sql,null,AVAILABLE,@Spark}
2019-09-12 11:22:55,795  INFO org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef 54 - Registered StateStoreCoordinator endpoint
2019-09-12 11:22:56,170  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0 stored as values in memory (estimated size 219.8 KB, free 899.5 MB)
2019-09-12 11:22:56,295  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 11:22:56,310  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_0_piece0 in memory on 10.6.1.14:5611 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 11:22:56,310  INFO org.apache.spark.SparkContext 54 - Created broadcast 0 from load at SparkUtil.java:34
2019-09-12 11:22:56,716  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 11:22:56,716  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 11:22:56,748  INFO org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 413 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 326
2019-09-12 11:22:56,826  INFO org.apache.spark.SparkContext 54 - Starting job: load at SparkUtil.java:34
2019-09-12 11:22:56,888  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 0 (load at SparkUtil.java:34) with 1 output partitions
2019-09-12 11:22:56,888  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 0 (load at SparkUtil.java:34)
2019-09-12 11:22:56,888  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 11:22:56,888  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 11:22:56,920  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:34), which has no missing parents
2019-09-12 11:22:56,998  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1 stored as values in memory (estimated size 5.5 KB, free 899.5 MB)
2019-09-12 11:22:56,998  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 899.5 MB)
2019-09-12 11:22:56,998  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_1_piece0 in memory on 10.6.1.14:5611 (size: 3.4 KB, free: 899.7 MB)
2019-09-12 11:22:56,998  INFO org.apache.spark.SparkContext 54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-09-12 11:22:57,045  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:34) (first 15 tasks are for partitions Vector(0))
2019-09-12 11:22:57,045  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 0.0 with 1 tasks
2019-09-12 11:22:57,123  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes)
2019-09-12 11:22:57,138  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 0.0 (TID 0)
2019-09-12 11:22:57,216  INFO org.apache.spark.rdd.BinaryFileRDD 54 - Input split: Paths:/D:/MyProject/splitCHword/target/classes/test.csv:0+326
2019-09-12 11:22:57,435  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 0.0 (TID 0). 853 bytes result sent to driver
2019-09-12 11:22:57,451  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 0.0 (TID 0) in 360 ms on localhost (executor driver) (1/1)
2019-09-12 11:22:57,451  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-09-12 11:22:57,451  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 0 (load at SparkUtil.java:34) finished in 0.375 s
2019-09-12 11:22:57,466  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 0 finished: load at SparkUtil.java:34, took 0.639846 s
2019-09-12 11:22:57,935  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_1_piece0 on 10.6.1.14:5611 in memory (size: 3.4 KB, free: 899.7 MB)
2019-09-12 11:22:59,247  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Pruning directories with: 
2019-09-12 11:22:59,247  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Post-Scan Filters: 
2019-09-12 11:22:59,263  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Output Data Schema: struct<feature: string>
2019-09-12 11:22:59,263  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Pushed Filters: 
2019-09-12 11:22:59,419  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_0_piece0 on 10.6.1.14:5611 in memory (size: 20.7 KB, free: 899.7 MB)
2019-09-12 11:22:59,903  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 292.558257 ms
2019-09-12 11:22:59,903  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2 stored as values in memory (estimated size 220.5 KB, free 899.5 MB)
2019-09-12 11:22:59,919  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 11:22:59,934  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_2_piece0 in memory on 10.6.1.14:5611 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 11:22:59,934  INFO org.apache.spark.SparkContext 54 - Created broadcast 2 from show at SparkUtil.java:57
2019-09-12 11:22:59,950  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2019-09-12 11:23:00,032  INFO org.apache.spark.SparkContext 54 - Starting job: show at SparkUtil.java:57
2019-09-12 11:23:00,033  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 1 (show at SparkUtil.java:57) with 1 output partitions
2019-09-12 11:23:00,033  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 1 (show at SparkUtil.java:57)
2019-09-12 11:23:00,036  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 11:23:00,037  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 11:23:00,038  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:57), which has no missing parents
2019-09-12 11:23:00,065  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3 stored as values in memory (estimated size 15.4 KB, free 899.4 MB)
2019-09-12 11:23:00,072  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.3 KB, free 899.4 MB)
2019-09-12 11:23:00,073  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_3_piece0 in memory on 10.6.1.14:5611 (size: 7.3 KB, free: 899.7 MB)
2019-09-12 11:23:00,074  INFO org.apache.spark.SparkContext 54 - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2019-09-12 11:23:00,075  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:57) (first 15 tasks are for partitions Vector(0))
2019-09-12 11:23:00,075  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 1.0 with 1 tasks
2019-09-12 11:23:00,086  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5293 bytes)
2019-09-12 11:23:00,087  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 1.0 (TID 1)
2019-09-12 11:23:00,109  INFO org.apache.spark.sql.execution.datasources.FileScanRDD 54 - Reading File path: file:///D:/MyProject/splitCHword/target/classes/test.csv, range: 0-326, partition values: [empty row]
2019-09-12 11:23:00,138  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 22.448621 ms
2019-09-12 11:23:00,414  INFO org.ansj.util.MyStaticValue 106 - init version.number to env value is : 2.11.8
2019-09-12 11:23:00,414  INFO org.ansj.util.MyStaticValue 106 - init copyright.string to env value is : Copyright 2002-2016, LAMP/EPFL
2019-09-12 11:23:00,415  INFO org.ansj.util.MyStaticValue 106 - init osgi.version.number to env value is : 2.11.8.v20160304-115712-1706a37eb8
2019-09-12 11:23:00,415  INFO org.ansj.util.MyStaticValue 106 - init maven.version.number to env value is : 2.11.8
2019-09-12 11:23:00,419  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/ambiguity.dic
2019-09-12 11:23:00,420 ERROR org.ansj.library.AmbiguityLibrary 117 - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:D:\MyProject\splitCHword\library\ambiguity.dic not found or can not to read, path: library/ambiguity.dic
2019-09-12 11:23:00,421  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/default.dic
2019-09-12 11:23:00,421 ERROR org.ansj.library.DicLibrary 221 - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:D:\MyProject\splitCHword\library\default.dic not found or can not to read, path: library/default.dic
2019-09-12 11:23:01,516  INFO org.ansj.library.DATDictionary 90 - init core library ok use time : 994
2019-09-12 11:23:02,125  INFO org.ansj.library.NgramLibrary 17 - init ngram ok use time :609
2019-09-12 11:23:02,141  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 1.0 (TID 1). 1386 bytes result sent to driver
2019-09-12 11:23:02,141  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 1.0 (TID 1) in 2065 ms on localhost (executor driver) (1/1)
2019-09-12 11:23:02,187  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-09-12 11:23:02,187  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 1 (show at SparkUtil.java:57) finished in 2.108 s
2019-09-12 11:23:02,187  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 1 finished: show at SparkUtil.java:57, took 2.169317 s
2019-09-12 11:23:02,219  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 13.090549 ms
2019-09-12 11:23:02,250  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Pruning directories with: 
2019-09-12 11:23:02,250  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Post-Scan Filters: 
2019-09-12 11:23:02,250  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Output Data Schema: struct<label: string, feature: string>
2019-09-12 11:23:02,250  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Pushed Filters: 
2019-09-12 11:23:02,250  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 7.441061 ms
2019-09-12 11:23:02,265  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_4 stored as values in memory (estimated size 220.5 KB, free 899.2 MB)
2019-09-12 11:23:02,281  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_4_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.2 MB)
2019-09-12 11:23:02,281  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_4_piece0 in memory on 10.6.1.14:5611 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 11:23:02,281  INFO org.apache.spark.SparkContext 54 - Created broadcast 4 from toJSON at SparkUtil.java:62
2019-09-12 11:23:02,281  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2019-09-12 11:23:02,640  INFO org.apache.spark.SparkContext 54 - Invoking stop() from shutdown hook
2019-09-12 11:23:02,656  INFO org.spark_project.jetty.server.AbstractConnector 310 - Stopped Spark@791933f7{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 11:23:02,656  INFO org.apache.spark.ui.SparkUI 54 - Stopped Spark web UI at http://10.6.1.14:4042
2019-09-12 11:23:02,672  INFO org.apache.spark.MapOutputTrackerMasterEndpoint 54 - MapOutputTrackerMasterEndpoint stopped!
2019-09-12 11:23:02,719  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore cleared
2019-09-12 11:23:02,719  INFO org.apache.spark.storage.BlockManager 54 - BlockManager stopped
2019-09-12 11:23:02,719  INFO org.apache.spark.storage.BlockManagerMaster 54 - BlockManagerMaster stopped
2019-09-12 11:23:02,719  INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint 54 - OutputCommitCoordinator stopped!
2019-09-12 11:23:02,734  INFO org.apache.spark.SparkContext 54 - Successfully stopped SparkContext
2019-09-12 11:23:02,734  INFO org.apache.spark.util.ShutdownHookManager 54 - Shutdown hook called
2019-09-12 11:23:02,734  INFO org.apache.spark.util.ShutdownHookManager 54 - Deleting directory C:\Users\OUY\AppData\Local\Temp\spark-2be75dea-8674-45f2-a2b9-40ddc3e6a577
2019-09-12 11:23:34,769  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-12 11:23:35,425  WARN org.apache.hadoop.util.NativeCodeLoader 62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-12 11:23:35,519 ERROR org.apache.hadoop.util.Shell 396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2430)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
	at com.datamodel.SparkUtil.CreateSession(SparkUtil.java:25)
	at com.datamodel.SparkUtil.readDataFromCsv(SparkUtil.java:30)
	at com.datamodel.SparkUtil.main(SparkUtil.java:43)
2019-09-12 11:23:35,629  INFO org.apache.spark.SparkContext 54 - Submitted application: splitCHWord
2019-09-12 11:23:35,660  INFO org.apache.spark.SecurityManager 54 - Changing view acls to: OUY
2019-09-12 11:23:35,660  INFO org.apache.spark.SecurityManager 54 - Changing modify acls to: OUY
2019-09-12 11:23:35,660  INFO org.apache.spark.SecurityManager 54 - Changing view acls groups to: 
2019-09-12 11:23:35,660  INFO org.apache.spark.SecurityManager 54 - Changing modify acls groups to: 
2019-09-12 11:23:35,660  INFO org.apache.spark.SecurityManager 54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(OUY); groups with view permissions: Set(); users  with modify permissions: Set(OUY); groups with modify permissions: Set()
2019-09-12 11:23:36,378  INFO org.apache.spark.util.Utils 54 - Successfully started service 'sparkDriver' on port 5637.
2019-09-12 11:23:36,425  INFO org.apache.spark.SparkEnv 54 - Registering MapOutputTracker
2019-09-12 11:23:36,441  INFO org.apache.spark.SparkEnv 54 - Registering BlockManagerMaster
2019-09-12 11:23:36,456  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-09-12 11:23:36,456  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - BlockManagerMasterEndpoint up
2019-09-12 11:23:36,456  INFO org.apache.spark.storage.DiskBlockManager 54 - Created local directory at C:\Users\OUY\AppData\Local\Temp\blockmgr-e5260caf-d1a7-4f51-9a77-4bd65451a322
2019-09-12 11:23:36,488  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore started with capacity 899.7 MB
2019-09-12 11:23:36,550  INFO org.apache.spark.SparkEnv 54 - Registering OutputCommitCoordinator
2019-09-12 11:23:36,644  INFO org.spark_project.jetty.util.log 192 - Logging initialized @3900ms
2019-09-12 11:23:36,706  INFO org.spark_project.jetty.server.Server 345 - jetty-9.3.z-SNAPSHOT
2019-09-12 11:23:36,722  INFO org.spark_project.jetty.server.Server 403 - Started @3982ms
2019-09-12 11:23:36,738  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2019-09-12 11:23:36,738  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
2019-09-12 11:23:36,738  INFO org.spark_project.jetty.server.AbstractConnector 270 - Started ServerConnector@7c55f2c{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 11:23:36,738  INFO org.apache.spark.util.Utils 54 - Successfully started service 'SparkUI' on port 4042.
2019-09-12 11:23:36,769  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@68759011{/jobs,null,AVAILABLE,@Spark}
2019-09-12 11:23:36,784  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@fd0e5b6{/jobs/json,null,AVAILABLE,@Spark}
2019-09-12 11:23:36,784  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@36b0fcd5{/jobs/job,null,AVAILABLE,@Spark}
2019-09-12 11:23:36,784  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6326d182{/jobs/job/json,null,AVAILABLE,@Spark}
2019-09-12 11:23:36,784  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@716a7124{/stages,null,AVAILABLE,@Spark}
2019-09-12 11:23:36,784  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@226642a5{/stages/json,null,AVAILABLE,@Spark}
2019-09-12 11:23:36,784  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5cc126dc{/stages/stage,null,AVAILABLE,@Spark}
2019-09-12 11:23:36,784  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5dbe30be{/stages/stage/json,null,AVAILABLE,@Spark}
2019-09-12 11:23:36,784  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@55795845{/stages/pool,null,AVAILABLE,@Spark}
2019-09-12 11:23:36,784  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@119f1f2a{/stages/pool/json,null,AVAILABLE,@Spark}
2019-09-12 11:23:36,784  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5b970f7{/storage,null,AVAILABLE,@Spark}
2019-09-12 11:23:36,784  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@165b8a71{/storage/json,null,AVAILABLE,@Spark}
2019-09-12 11:23:36,784  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2f058b8a{/storage/rdd,null,AVAILABLE,@Spark}
2019-09-12 11:23:36,784  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3f2ef586{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-09-12 11:23:36,784  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@76c7beb3{/environment,null,AVAILABLE,@Spark}
2019-09-12 11:23:36,800  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2cf92cc7{/environment/json,null,AVAILABLE,@Spark}
2019-09-12 11:23:36,800  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7b139eab{/executors,null,AVAILABLE,@Spark}
2019-09-12 11:23:36,800  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@611df6e3{/executors/json,null,AVAILABLE,@Spark}
2019-09-12 11:23:36,800  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6273c5a4{/executors/threadDump,null,AVAILABLE,@Spark}
2019-09-12 11:23:36,800  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@53e211ee{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-09-12 11:23:36,800  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3d8bbcdc{/static,null,AVAILABLE,@Spark}
2019-09-12 11:23:36,800  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@41fe9859{/,null,AVAILABLE,@Spark}
2019-09-12 11:23:36,816  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6c67e137{/api,null,AVAILABLE,@Spark}
2019-09-12 11:23:36,816  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2d10e0b1{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-09-12 11:23:36,816  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@172ca72b{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-09-12 11:23:36,816  INFO org.apache.spark.ui.SparkUI 54 - Bound SparkUI to 0.0.0.0, and started at http://10.6.1.14:4042
2019-09-12 11:23:36,988  INFO org.apache.spark.executor.Executor 54 - Starting executor ID driver on host localhost
2019-09-12 11:23:37,041  INFO org.apache.spark.util.Utils 54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 5650.
2019-09-12 11:23:37,042  INFO org.apache.spark.network.netty.NettyBlockTransferService 54 - Server created on 10.6.1.14:5650
2019-09-12 11:23:37,045  INFO org.apache.spark.storage.BlockManager 54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-09-12 11:23:37,053  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registering BlockManager BlockManagerId(driver, 10.6.1.14, 5650, None)
2019-09-12 11:23:37,058  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Registering block manager 10.6.1.14:5650 with 899.7 MB RAM, BlockManagerId(driver, 10.6.1.14, 5650, None)
2019-09-12 11:23:37,061  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registered BlockManager BlockManagerId(driver, 10.6.1.14, 5650, None)
2019-09-12 11:23:37,062  INFO org.apache.spark.storage.BlockManager 54 - Initialized BlockManager: BlockManagerId(driver, 10.6.1.14, 5650, None)
2019-09-12 11:23:37,327  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3e1162e7{/metrics/json,null,AVAILABLE,@Spark}
2019-09-12 11:23:37,410  INFO org.apache.spark.sql.internal.SharedState 54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/MyProject/splitCHword/spark-warehouse/').
2019-09-12 11:23:37,411  INFO org.apache.spark.sql.internal.SharedState 54 - Warehouse path is 'file:/D:/MyProject/splitCHword/spark-warehouse/'.
2019-09-12 11:23:37,420  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@47dd778{/SQL,null,AVAILABLE,@Spark}
2019-09-12 11:23:37,420  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@36a7abe1{/SQL/json,null,AVAILABLE,@Spark}
2019-09-12 11:23:37,422  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3ae66c85{/SQL/execution,null,AVAILABLE,@Spark}
2019-09-12 11:23:37,423  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4604b900{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-09-12 11:23:37,425  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@d2387c8{/static/sql,null,AVAILABLE,@Spark}
2019-09-12 11:23:38,723  INFO org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef 54 - Registered StateStoreCoordinator endpoint
2019-09-12 11:23:39,004  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0 stored as values in memory (estimated size 219.8 KB, free 899.5 MB)
2019-09-12 11:23:39,082  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 11:23:39,098  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_0_piece0 in memory on 10.6.1.14:5650 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 11:23:39,098  INFO org.apache.spark.SparkContext 54 - Created broadcast 0 from load at SparkUtil.java:34
2019-09-12 11:23:39,426  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 11:23:39,426  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 11:23:39,442  INFO org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 413 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 326
2019-09-12 11:23:39,488  INFO org.apache.spark.SparkContext 54 - Starting job: load at SparkUtil.java:34
2019-09-12 11:23:39,504  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 0 (load at SparkUtil.java:34) with 1 output partitions
2019-09-12 11:23:39,504  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 0 (load at SparkUtil.java:34)
2019-09-12 11:23:39,504  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 11:23:39,504  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 11:23:39,535  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:34), which has no missing parents
2019-09-12 11:23:39,551  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1 stored as values in memory (estimated size 5.5 KB, free 899.5 MB)
2019-09-12 11:23:39,551  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 899.5 MB)
2019-09-12 11:23:39,551  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_1_piece0 in memory on 10.6.1.14:5650 (size: 3.4 KB, free: 899.7 MB)
2019-09-12 11:23:39,551  INFO org.apache.spark.SparkContext 54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-09-12 11:23:39,582  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:34) (first 15 tasks are for partitions Vector(0))
2019-09-12 11:23:39,582  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 0.0 with 1 tasks
2019-09-12 11:23:39,629  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes)
2019-09-12 11:23:39,660  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 0.0 (TID 0)
2019-09-12 11:23:39,738  INFO org.apache.spark.rdd.BinaryFileRDD 54 - Input split: Paths:/D:/MyProject/splitCHword/target/classes/test.csv:0+326
2019-09-12 11:23:39,910  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 0.0 (TID 0). 853 bytes result sent to driver
2019-09-12 11:23:39,910  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 0.0 (TID 0) in 297 ms on localhost (executor driver) (1/1)
2019-09-12 11:23:39,926  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-09-12 11:23:39,926  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 0 (load at SparkUtil.java:34) finished in 0.328 s
2019-09-12 11:23:39,926  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 0 finished: load at SparkUtil.java:34, took 0.442131 s
2019-09-12 11:23:40,379  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_0_piece0 on 10.6.1.14:5650 in memory (size: 20.7 KB, free: 899.7 MB)
2019-09-12 11:23:40,379  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_1_piece0 on 10.6.1.14:5650 in memory (size: 3.4 KB, free: 899.7 MB)
2019-09-12 11:23:41,738  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Pruning directories with: 
2019-09-12 11:23:41,738  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Post-Scan Filters: 
2019-09-12 11:23:41,738  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Output Data Schema: struct<feature: string>
2019-09-12 11:23:41,754  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Pushed Filters: 
2019-09-12 11:23:42,425  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 327.539347 ms
2019-09-12 11:23:42,472  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2 stored as values in memory (estimated size 220.5 KB, free 899.5 MB)
2019-09-12 11:23:42,503  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 11:23:42,503  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_2_piece0 in memory on 10.6.1.14:5650 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 11:23:42,519  INFO org.apache.spark.SparkContext 54 - Created broadcast 2 from show at SparkUtil.java:57
2019-09-12 11:23:42,535  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2019-09-12 11:23:42,597  INFO org.apache.spark.SparkContext 54 - Starting job: show at SparkUtil.java:57
2019-09-12 11:23:42,597  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 1 (show at SparkUtil.java:57) with 1 output partitions
2019-09-12 11:23:42,597  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 1 (show at SparkUtil.java:57)
2019-09-12 11:23:42,597  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 11:23:42,597  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 11:23:42,597  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:57), which has no missing parents
2019-09-12 11:23:42,613  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3 stored as values in memory (estimated size 15.4 KB, free 899.4 MB)
2019-09-12 11:23:42,628  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.3 KB, free 899.4 MB)
2019-09-12 11:23:42,628  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_3_piece0 in memory on 10.6.1.14:5650 (size: 7.3 KB, free: 899.7 MB)
2019-09-12 11:23:42,628  INFO org.apache.spark.SparkContext 54 - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2019-09-12 11:23:42,628  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:57) (first 15 tasks are for partitions Vector(0))
2019-09-12 11:23:42,628  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 1.0 with 1 tasks
2019-09-12 11:23:42,628  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5293 bytes)
2019-09-12 11:23:42,644  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 1.0 (TID 1)
2019-09-12 11:23:42,675  INFO org.apache.spark.sql.execution.datasources.FileScanRDD 54 - Reading File path: file:///D:/MyProject/splitCHword/target/classes/test.csv, range: 0-326, partition values: [empty row]
2019-09-12 11:23:42,722  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 41.109299 ms
2019-09-12 11:23:43,019  INFO org.ansj.util.MyStaticValue 106 - init version.number to env value is : 2.11.8
2019-09-12 11:23:43,019  INFO org.ansj.util.MyStaticValue 106 - init copyright.string to env value is : Copyright 2002-2016, LAMP/EPFL
2019-09-12 11:23:43,019  INFO org.ansj.util.MyStaticValue 106 - init osgi.version.number to env value is : 2.11.8.v20160304-115712-1706a37eb8
2019-09-12 11:23:43,019  INFO org.ansj.util.MyStaticValue 106 - init maven.version.number to env value is : 2.11.8
2019-09-12 11:23:43,019  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/ambiguity.dic
2019-09-12 11:23:43,019 ERROR org.ansj.library.AmbiguityLibrary 117 - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:D:\MyProject\splitCHword\library\ambiguity.dic not found or can not to read, path: library/ambiguity.dic
2019-09-12 11:23:43,019  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/default.dic
2019-09-12 11:23:43,019 ERROR org.ansj.library.DicLibrary 221 - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:D:\MyProject\splitCHword\library\default.dic not found or can not to read, path: library/default.dic
2019-09-12 11:23:44,315  INFO org.ansj.library.DATDictionary 90 - init core library ok use time : 1202
2019-09-12 11:23:44,768  INFO org.ansj.library.NgramLibrary 17 - init ngram ok use time :453
2019-09-12 11:23:44,768  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 1.0 (TID 1). 1429 bytes result sent to driver
2019-09-12 11:23:44,784  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 1.0 (TID 1) in 2156 ms on localhost (executor driver) (1/1)
2019-09-12 11:23:44,784  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-09-12 11:23:44,784  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 1 (show at SparkUtil.java:57) finished in 2.156 s
2019-09-12 11:23:44,784  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 1 finished: show at SparkUtil.java:57, took 2.189504 s
2019-09-12 11:23:44,800  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 8.016206 ms
2019-09-12 11:23:44,831  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Pruning directories with: 
2019-09-12 11:23:44,831  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Post-Scan Filters: 
2019-09-12 11:23:44,831  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Output Data Schema: struct<label: string, feature: string>
2019-09-12 11:23:44,831  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Pushed Filters: 
2019-09-12 11:23:44,831  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 6.630394 ms
2019-09-12 11:23:44,847  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_4 stored as values in memory (estimated size 220.5 KB, free 899.2 MB)
2019-09-12 11:23:44,862  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_4_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.2 MB)
2019-09-12 11:23:44,862  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_4_piece0 in memory on 10.6.1.14:5650 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 11:23:44,862  INFO org.apache.spark.SparkContext 54 - Created broadcast 4 from toJSON at SparkUtil.java:62
2019-09-12 11:23:44,862  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2019-09-12 11:23:45,206  INFO org.apache.spark.SparkContext 54 - Invoking stop() from shutdown hook
2019-09-12 11:23:45,221  INFO org.spark_project.jetty.server.AbstractConnector 310 - Stopped Spark@7c55f2c{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 11:23:45,221  INFO org.apache.spark.ui.SparkUI 54 - Stopped Spark web UI at http://10.6.1.14:4042
2019-09-12 11:23:45,237  INFO org.apache.spark.MapOutputTrackerMasterEndpoint 54 - MapOutputTrackerMasterEndpoint stopped!
2019-09-12 11:23:45,268  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore cleared
2019-09-12 11:23:45,268  INFO org.apache.spark.storage.BlockManager 54 - BlockManager stopped
2019-09-12 11:23:45,268  INFO org.apache.spark.storage.BlockManagerMaster 54 - BlockManagerMaster stopped
2019-09-12 11:23:45,284  INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint 54 - OutputCommitCoordinator stopped!
2019-09-12 11:23:45,284  INFO org.apache.spark.SparkContext 54 - Successfully stopped SparkContext
2019-09-12 11:23:45,284  INFO org.apache.spark.util.ShutdownHookManager 54 - Shutdown hook called
2019-09-12 11:23:45,284  INFO org.apache.spark.util.ShutdownHookManager 54 - Deleting directory C:\Users\OUY\AppData\Local\Temp\spark-be7df00e-6a41-41b3-b8d7-846b0709edec
2019-09-12 11:32:16,827  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-12 11:32:17,515  WARN org.apache.hadoop.util.NativeCodeLoader 62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-12 11:32:17,608 ERROR org.apache.hadoop.util.Shell 396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2430)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
	at com.datamodel.SparkUtil.CreateSession(SparkUtil.java:25)
	at com.datamodel.SparkUtil.readDataFromCsv(SparkUtil.java:30)
	at com.datamodel.SparkUtil.main(SparkUtil.java:43)
2019-09-12 11:32:17,764  INFO org.apache.spark.SparkContext 54 - Submitted application: splitCHWord
2019-09-12 11:32:17,796  INFO org.apache.spark.SecurityManager 54 - Changing view acls to: OUY
2019-09-12 11:32:17,796  INFO org.apache.spark.SecurityManager 54 - Changing modify acls to: OUY
2019-09-12 11:32:17,796  INFO org.apache.spark.SecurityManager 54 - Changing view acls groups to: 
2019-09-12 11:32:17,796  INFO org.apache.spark.SecurityManager 54 - Changing modify acls groups to: 
2019-09-12 11:32:17,796  INFO org.apache.spark.SecurityManager 54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(OUY); groups with view permissions: Set(); users  with modify permissions: Set(OUY); groups with modify permissions: Set()
2019-09-12 11:32:18,670  INFO org.apache.spark.util.Utils 54 - Successfully started service 'sparkDriver' on port 5724.
2019-09-12 11:32:18,717  INFO org.apache.spark.SparkEnv 54 - Registering MapOutputTracker
2019-09-12 11:32:18,749  INFO org.apache.spark.SparkEnv 54 - Registering BlockManagerMaster
2019-09-12 11:32:18,749  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-09-12 11:32:18,749  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - BlockManagerMasterEndpoint up
2019-09-12 11:32:18,764  INFO org.apache.spark.storage.DiskBlockManager 54 - Created local directory at C:\Users\OUY\AppData\Local\Temp\blockmgr-b3ad736c-3676-47b8-be48-5d75459db7af
2019-09-12 11:32:18,780  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore started with capacity 899.7 MB
2019-09-12 11:32:18,842  INFO org.apache.spark.SparkEnv 54 - Registering OutputCommitCoordinator
2019-09-12 11:32:18,983  INFO org.spark_project.jetty.util.log 192 - Logging initialized @4204ms
2019-09-12 11:32:19,061  INFO org.spark_project.jetty.server.Server 345 - jetty-9.3.z-SNAPSHOT
2019-09-12 11:32:19,077  INFO org.spark_project.jetty.server.Server 403 - Started @4305ms
2019-09-12 11:32:19,108  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2019-09-12 11:32:19,108  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
2019-09-12 11:32:19,108  INFO org.spark_project.jetty.server.AbstractConnector 270 - Started ServerConnector@297ea53a{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 11:32:19,108  INFO org.apache.spark.util.Utils 54 - Successfully started service 'SparkUI' on port 4042.
2019-09-12 11:32:19,139  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@592e843a{/jobs,null,AVAILABLE,@Spark}
2019-09-12 11:32:19,139  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4fad94a7{/jobs/json,null,AVAILABLE,@Spark}
2019-09-12 11:32:19,139  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6326d182{/jobs/job,null,AVAILABLE,@Spark}
2019-09-12 11:32:19,139  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@77192705{/jobs/job/json,null,AVAILABLE,@Spark}
2019-09-12 11:32:19,139  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7e809b79{/stages,null,AVAILABLE,@Spark}
2019-09-12 11:32:19,139  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@625e134e{/stages/json,null,AVAILABLE,@Spark}
2019-09-12 11:32:19,139  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@89c10b7{/stages/stage,null,AVAILABLE,@Spark}
2019-09-12 11:32:19,155  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3d08f3f5{/stages/stage/json,null,AVAILABLE,@Spark}
2019-09-12 11:32:19,155  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1a1da881{/stages/pool,null,AVAILABLE,@Spark}
2019-09-12 11:32:19,155  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7fd4acee{/stages/pool/json,null,AVAILABLE,@Spark}
2019-09-12 11:32:19,155  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6175619b{/storage,null,AVAILABLE,@Spark}
2019-09-12 11:32:19,155  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@756cf158{/storage/json,null,AVAILABLE,@Spark}
2019-09-12 11:32:19,155  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@751d3241{/storage/rdd,null,AVAILABLE,@Spark}
2019-09-12 11:32:19,155  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@64337702{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-09-12 11:32:19,155  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@30ea8c23{/environment,null,AVAILABLE,@Spark}
2019-09-12 11:32:19,155  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4e76dac{/environment/json,null,AVAILABLE,@Spark}
2019-09-12 11:32:19,155  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5f2f577{/executors,null,AVAILABLE,@Spark}
2019-09-12 11:32:19,155  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5d465e4b{/executors/json,null,AVAILABLE,@Spark}
2019-09-12 11:32:19,155  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@41a90fa8{/executors/threadDump,null,AVAILABLE,@Spark}
2019-09-12 11:32:19,155  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@52500920{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-09-12 11:32:19,170  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@18a3962d{/static,null,AVAILABLE,@Spark}
2019-09-12 11:32:19,170  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2c9399a4{/,null,AVAILABLE,@Spark}
2019-09-12 11:32:19,170  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@9635fa{/api,null,AVAILABLE,@Spark}
2019-09-12 11:32:19,170  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5bda80bf{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-09-12 11:32:19,170  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2ce86164{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-09-12 11:32:19,170  INFO org.apache.spark.ui.SparkUI 54 - Bound SparkUI to 0.0.0.0, and started at http://10.6.1.14:4042
2019-09-12 11:32:19,342  INFO org.apache.spark.executor.Executor 54 - Starting executor ID driver on host localhost
2019-09-12 11:32:19,373  INFO org.apache.spark.util.Utils 54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 5738.
2019-09-12 11:32:19,373  INFO org.apache.spark.network.netty.NettyBlockTransferService 54 - Server created on 10.6.1.14:5738
2019-09-12 11:32:19,373  INFO org.apache.spark.storage.BlockManager 54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-09-12 11:32:19,389  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registering BlockManager BlockManagerId(driver, 10.6.1.14, 5738, None)
2019-09-12 11:32:19,389  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Registering block manager 10.6.1.14:5738 with 899.7 MB RAM, BlockManagerId(driver, 10.6.1.14, 5738, None)
2019-09-12 11:32:19,389  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registered BlockManager BlockManagerId(driver, 10.6.1.14, 5738, None)
2019-09-12 11:32:19,389  INFO org.apache.spark.storage.BlockManager 54 - Initialized BlockManager: BlockManagerId(driver, 10.6.1.14, 5738, None)
2019-09-12 11:32:19,670  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1ea9f009{/metrics/json,null,AVAILABLE,@Spark}
2019-09-12 11:32:19,748  INFO org.apache.spark.sql.internal.SharedState 54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/MyProject/splitCHword/spark-warehouse/').
2019-09-12 11:32:19,748  INFO org.apache.spark.sql.internal.SharedState 54 - Warehouse path is 'file:/D:/MyProject/splitCHword/spark-warehouse/'.
2019-09-12 11:32:19,764  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3ae66c85{/SQL,null,AVAILABLE,@Spark}
2019-09-12 11:32:19,764  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4604b900{/SQL/json,null,AVAILABLE,@Spark}
2019-09-12 11:32:19,764  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@367795c7{/SQL/execution,null,AVAILABLE,@Spark}
2019-09-12 11:32:19,764  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3956b302{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-09-12 11:32:19,764  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@13e9f2e2{/static/sql,null,AVAILABLE,@Spark}
2019-09-12 11:32:20,826  INFO org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef 54 - Registered StateStoreCoordinator endpoint
2019-09-12 11:32:21,092  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0 stored as values in memory (estimated size 219.8 KB, free 899.5 MB)
2019-09-12 11:32:21,232  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 11:32:21,232  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_0_piece0 in memory on 10.6.1.14:5738 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 11:32:21,248  INFO org.apache.spark.SparkContext 54 - Created broadcast 0 from load at SparkUtil.java:34
2019-09-12 11:32:21,639  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 11:32:21,639  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 11:32:21,654  INFO org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 413 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 326
2019-09-12 11:32:21,701  INFO org.apache.spark.SparkContext 54 - Starting job: load at SparkUtil.java:34
2019-09-12 11:32:21,717  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 0 (load at SparkUtil.java:34) with 1 output partitions
2019-09-12 11:32:21,717  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 0 (load at SparkUtil.java:34)
2019-09-12 11:32:21,717  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 11:32:21,717  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 11:32:21,732  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:34), which has no missing parents
2019-09-12 11:32:21,748  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1 stored as values in memory (estimated size 5.5 KB, free 899.5 MB)
2019-09-12 11:32:21,748  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 899.5 MB)
2019-09-12 11:32:21,764  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_1_piece0 in memory on 10.6.1.14:5738 (size: 3.4 KB, free: 899.7 MB)
2019-09-12 11:32:21,764  INFO org.apache.spark.SparkContext 54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-09-12 11:32:21,779  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:34) (first 15 tasks are for partitions Vector(0))
2019-09-12 11:32:21,779  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 0.0 with 1 tasks
2019-09-12 11:32:21,842  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes)
2019-09-12 11:32:21,857  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 0.0 (TID 0)
2019-09-12 11:32:21,951  INFO org.apache.spark.rdd.BinaryFileRDD 54 - Input split: Paths:/D:/MyProject/splitCHword/target/classes/test.csv:0+326
2019-09-12 11:32:22,153  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 0.0 (TID 0). 853 bytes result sent to driver
2019-09-12 11:32:22,161  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 0.0 (TID 0) in 334 ms on localhost (executor driver) (1/1)
2019-09-12 11:32:22,163  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-09-12 11:32:22,169  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 0 (load at SparkUtil.java:34) finished in 0.357 s
2019-09-12 11:32:22,174  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 0 finished: load at SparkUtil.java:34, took 0.463223 s
2019-09-12 11:32:22,385  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_0_piece0 on 10.6.1.14:5738 in memory (size: 20.7 KB, free: 899.7 MB)
2019-09-12 11:32:22,393  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_1_piece0 on 10.6.1.14:5738 in memory (size: 3.4 KB, free: 899.7 MB)
2019-09-12 11:32:24,063  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Pruning directories with: 
2019-09-12 11:32:24,063  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Post-Scan Filters: 
2019-09-12 11:32:24,063  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Output Data Schema: struct<feature: string>
2019-09-12 11:32:24,078  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Pushed Filters: 
2019-09-12 11:32:24,734  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 283.084985 ms
2019-09-12 11:32:24,750  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2 stored as values in memory (estimated size 220.5 KB, free 899.5 MB)
2019-09-12 11:32:24,766  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 11:32:24,766  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_2_piece0 in memory on 10.6.1.14:5738 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 11:32:24,766  INFO org.apache.spark.SparkContext 54 - Created broadcast 2 from show at SparkUtil.java:57
2019-09-12 11:32:24,781  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2019-09-12 11:32:24,844  INFO org.apache.spark.SparkContext 54 - Starting job: show at SparkUtil.java:57
2019-09-12 11:32:24,844  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 1 (show at SparkUtil.java:57) with 1 output partitions
2019-09-12 11:32:24,844  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 1 (show at SparkUtil.java:57)
2019-09-12 11:32:24,844  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 11:32:24,844  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 11:32:24,844  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:57), which has no missing parents
2019-09-12 11:32:24,859  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3 stored as values in memory (estimated size 15.4 KB, free 899.4 MB)
2019-09-12 11:32:24,875  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.3 KB, free 899.4 MB)
2019-09-12 11:32:24,875  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_3_piece0 in memory on 10.6.1.14:5738 (size: 7.3 KB, free: 899.7 MB)
2019-09-12 11:32:24,875  INFO org.apache.spark.SparkContext 54 - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2019-09-12 11:32:24,875  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:57) (first 15 tasks are for partitions Vector(0))
2019-09-12 11:32:24,875  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 1.0 with 1 tasks
2019-09-12 11:32:24,891  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5293 bytes)
2019-09-12 11:32:24,891  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 1.0 (TID 1)
2019-09-12 11:32:24,906  INFO org.apache.spark.sql.execution.datasources.FileScanRDD 54 - Reading File path: file:///D:/MyProject/splitCHword/target/classes/test.csv, range: 0-326, partition values: [empty row]
2019-09-12 11:32:24,922  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 11.076258 ms
2019-09-12 11:32:25,156  INFO org.ansj.util.MyStaticValue 106 - init version.number to env value is : 2.11.8
2019-09-12 11:32:25,156  INFO org.ansj.util.MyStaticValue 106 - init copyright.string to env value is : Copyright 2002-2016, LAMP/EPFL
2019-09-12 11:32:25,156  INFO org.ansj.util.MyStaticValue 106 - init osgi.version.number to env value is : 2.11.8.v20160304-115712-1706a37eb8
2019-09-12 11:32:25,156  INFO org.ansj.util.MyStaticValue 106 - init maven.version.number to env value is : 2.11.8
2019-09-12 11:32:25,172  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/ambiguity.dic
2019-09-12 11:32:25,172 ERROR org.ansj.library.AmbiguityLibrary 117 - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:D:\MyProject\splitCHword\library\ambiguity.dic not found or can not to read, path: library/ambiguity.dic
2019-09-12 11:32:25,172  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/default.dic
2019-09-12 11:32:25,172 ERROR org.ansj.library.DicLibrary 221 - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:D:\MyProject\splitCHword\library\default.dic not found or can not to read, path: library/default.dic
2019-09-12 11:32:26,281  INFO org.ansj.library.DATDictionary 90 - init core library ok use time : 1015
2019-09-12 11:32:26,796  INFO org.ansj.library.NgramLibrary 17 - init ngram ok use time :500
2019-09-12 11:32:26,812  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 1.0 (TID 1). 1343 bytes result sent to driver
2019-09-12 11:32:26,812  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 1.0 (TID 1) in 1937 ms on localhost (executor driver) (1/1)
2019-09-12 11:32:26,812  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-09-12 11:32:26,812  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 1 (show at SparkUtil.java:57) finished in 1.937 s
2019-09-12 11:32:26,828  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 1 finished: show at SparkUtil.java:57, took 1.977519 s
2019-09-12 11:32:26,859  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 16.441586 ms
2019-09-12 11:32:26,906  INFO org.apache.spark.SparkContext 54 - Invoking stop() from shutdown hook
2019-09-12 11:32:26,906  INFO org.spark_project.jetty.server.AbstractConnector 310 - Stopped Spark@297ea53a{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 11:32:26,906  INFO org.apache.spark.ui.SparkUI 54 - Stopped Spark web UI at http://10.6.1.14:4042
2019-09-12 11:32:26,921  INFO org.apache.spark.MapOutputTrackerMasterEndpoint 54 - MapOutputTrackerMasterEndpoint stopped!
2019-09-12 11:32:26,953  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore cleared
2019-09-12 11:32:26,953  INFO org.apache.spark.storage.BlockManager 54 - BlockManager stopped
2019-09-12 11:32:26,968  INFO org.apache.spark.storage.BlockManagerMaster 54 - BlockManagerMaster stopped
2019-09-12 11:32:26,968  INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint 54 - OutputCommitCoordinator stopped!
2019-09-12 11:32:26,968  INFO org.apache.spark.SparkContext 54 - Successfully stopped SparkContext
2019-09-12 11:32:26,968  INFO org.apache.spark.util.ShutdownHookManager 54 - Shutdown hook called
2019-09-12 11:32:26,968  INFO org.apache.spark.util.ShutdownHookManager 54 - Deleting directory C:\Users\OUY\AppData\Local\Temp\spark-68690765-4881-40ee-a297-5afeda1c8aa0
2019-09-12 11:33:46,422  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-12 11:33:47,094  WARN org.apache.hadoop.util.NativeCodeLoader 62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-12 11:33:47,172 ERROR org.apache.hadoop.util.Shell 396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2430)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
	at com.datamodel.SparkUtil.CreateSession(SparkUtil.java:25)
	at com.datamodel.SparkUtil.readDataFromCsv(SparkUtil.java:30)
	at com.datamodel.SparkUtil.main(SparkUtil.java:43)
2019-09-12 11:33:47,328  INFO org.apache.spark.SparkContext 54 - Submitted application: splitCHWord
2019-09-12 11:33:47,359  INFO org.apache.spark.SecurityManager 54 - Changing view acls to: OUY
2019-09-12 11:33:47,359  INFO org.apache.spark.SecurityManager 54 - Changing modify acls to: OUY
2019-09-12 11:33:47,359  INFO org.apache.spark.SecurityManager 54 - Changing view acls groups to: 
2019-09-12 11:33:47,359  INFO org.apache.spark.SecurityManager 54 - Changing modify acls groups to: 
2019-09-12 11:33:47,359  INFO org.apache.spark.SecurityManager 54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(OUY); groups with view permissions: Set(); users  with modify permissions: Set(OUY); groups with modify permissions: Set()
2019-09-12 11:33:48,078  INFO org.apache.spark.util.Utils 54 - Successfully started service 'sparkDriver' on port 5769.
2019-09-12 11:33:48,140  INFO org.apache.spark.SparkEnv 54 - Registering MapOutputTracker
2019-09-12 11:33:48,156  INFO org.apache.spark.SparkEnv 54 - Registering BlockManagerMaster
2019-09-12 11:33:48,156  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-09-12 11:33:48,156  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - BlockManagerMasterEndpoint up
2019-09-12 11:33:48,171  INFO org.apache.spark.storage.DiskBlockManager 54 - Created local directory at C:\Users\OUY\AppData\Local\Temp\blockmgr-5270f947-79b3-45d2-ae02-4cbd77dcbd3c
2019-09-12 11:33:48,187  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore started with capacity 899.7 MB
2019-09-12 11:33:48,250  INFO org.apache.spark.SparkEnv 54 - Registering OutputCommitCoordinator
2019-09-12 11:33:48,359  INFO org.spark_project.jetty.util.log 192 - Logging initialized @4135ms
2019-09-12 11:33:48,421  INFO org.spark_project.jetty.server.Server 345 - jetty-9.3.z-SNAPSHOT
2019-09-12 11:33:48,437  INFO org.spark_project.jetty.server.Server 403 - Started @4218ms
2019-09-12 11:33:48,453  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2019-09-12 11:33:48,453  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
2019-09-12 11:33:48,468  INFO org.spark_project.jetty.server.AbstractConnector 270 - Started ServerConnector@297ea53a{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 11:33:48,468  INFO org.apache.spark.util.Utils 54 - Successfully started service 'SparkUI' on port 4042.
2019-09-12 11:33:48,484  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@592e843a{/jobs,null,AVAILABLE,@Spark}
2019-09-12 11:33:48,484  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4fad94a7{/jobs/json,null,AVAILABLE,@Spark}
2019-09-12 11:33:48,484  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6326d182{/jobs/job,null,AVAILABLE,@Spark}
2019-09-12 11:33:48,499  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@77192705{/jobs/job/json,null,AVAILABLE,@Spark}
2019-09-12 11:33:48,499  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7e809b79{/stages,null,AVAILABLE,@Spark}
2019-09-12 11:33:48,499  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@625e134e{/stages/json,null,AVAILABLE,@Spark}
2019-09-12 11:33:48,499  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@89c10b7{/stages/stage,null,AVAILABLE,@Spark}
2019-09-12 11:33:48,499  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3d08f3f5{/stages/stage/json,null,AVAILABLE,@Spark}
2019-09-12 11:33:48,499  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1a1da881{/stages/pool,null,AVAILABLE,@Spark}
2019-09-12 11:33:48,499  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7fd4acee{/stages/pool/json,null,AVAILABLE,@Spark}
2019-09-12 11:33:48,499  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6175619b{/storage,null,AVAILABLE,@Spark}
2019-09-12 11:33:48,499  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@756cf158{/storage/json,null,AVAILABLE,@Spark}
2019-09-12 11:33:48,499  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@751d3241{/storage/rdd,null,AVAILABLE,@Spark}
2019-09-12 11:33:48,499  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@64337702{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-09-12 11:33:48,499  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@30ea8c23{/environment,null,AVAILABLE,@Spark}
2019-09-12 11:33:48,499  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4e76dac{/environment/json,null,AVAILABLE,@Spark}
2019-09-12 11:33:48,499  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5f2f577{/executors,null,AVAILABLE,@Spark}
2019-09-12 11:33:48,515  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5d465e4b{/executors/json,null,AVAILABLE,@Spark}
2019-09-12 11:33:48,515  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@41a90fa8{/executors/threadDump,null,AVAILABLE,@Spark}
2019-09-12 11:33:48,515  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@52500920{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-09-12 11:33:48,515  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@18a3962d{/static,null,AVAILABLE,@Spark}
2019-09-12 11:33:48,515  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2c9399a4{/,null,AVAILABLE,@Spark}
2019-09-12 11:33:48,531  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@9635fa{/api,null,AVAILABLE,@Spark}
2019-09-12 11:33:48,531  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5bda80bf{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-09-12 11:33:48,531  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2ce86164{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-09-12 11:33:48,531  INFO org.apache.spark.ui.SparkUI 54 - Bound SparkUI to 0.0.0.0, and started at http://10.6.1.14:4042
2019-09-12 11:33:48,687  INFO org.apache.spark.executor.Executor 54 - Starting executor ID driver on host localhost
2019-09-12 11:33:48,718  INFO org.apache.spark.util.Utils 54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 5782.
2019-09-12 11:33:48,734  INFO org.apache.spark.network.netty.NettyBlockTransferService 54 - Server created on 10.6.1.14:5782
2019-09-12 11:33:48,734  INFO org.apache.spark.storage.BlockManager 54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-09-12 11:33:48,734  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registering BlockManager BlockManagerId(driver, 10.6.1.14, 5782, None)
2019-09-12 11:33:48,749  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Registering block manager 10.6.1.14:5782 with 899.7 MB RAM, BlockManagerId(driver, 10.6.1.14, 5782, None)
2019-09-12 11:33:48,749  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registered BlockManager BlockManagerId(driver, 10.6.1.14, 5782, None)
2019-09-12 11:33:48,749  INFO org.apache.spark.storage.BlockManager 54 - Initialized BlockManager: BlockManagerId(driver, 10.6.1.14, 5782, None)
2019-09-12 11:33:49,046  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1ea9f009{/metrics/json,null,AVAILABLE,@Spark}
2019-09-12 11:33:49,140  INFO org.apache.spark.sql.internal.SharedState 54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/MyProject/splitCHword/spark-warehouse/').
2019-09-12 11:33:49,140  INFO org.apache.spark.sql.internal.SharedState 54 - Warehouse path is 'file:/D:/MyProject/splitCHword/spark-warehouse/'.
2019-09-12 11:33:49,156  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3ae66c85{/SQL,null,AVAILABLE,@Spark}
2019-09-12 11:33:49,156  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4604b900{/SQL/json,null,AVAILABLE,@Spark}
2019-09-12 11:33:49,156  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@367795c7{/SQL/execution,null,AVAILABLE,@Spark}
2019-09-12 11:33:49,156  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3956b302{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-09-12 11:33:49,156  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@13e9f2e2{/static/sql,null,AVAILABLE,@Spark}
2019-09-12 11:33:50,390  INFO org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef 54 - Registered StateStoreCoordinator endpoint
2019-09-12 11:33:50,655  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0 stored as values in memory (estimated size 219.8 KB, free 899.5 MB)
2019-09-12 11:33:50,827  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 11:33:50,843  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_0_piece0 in memory on 10.6.1.14:5782 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 11:33:50,843  INFO org.apache.spark.SparkContext 54 - Created broadcast 0 from load at SparkUtil.java:34
2019-09-12 11:33:51,311  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 11:33:51,311  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 11:33:51,327  INFO org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 413 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 326
2019-09-12 11:33:51,389  INFO org.apache.spark.SparkContext 54 - Starting job: load at SparkUtil.java:34
2019-09-12 11:33:51,405  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 0 (load at SparkUtil.java:34) with 1 output partitions
2019-09-12 11:33:51,405  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 0 (load at SparkUtil.java:34)
2019-09-12 11:33:51,405  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 11:33:51,405  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 11:33:51,405  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:34), which has no missing parents
2019-09-12 11:33:51,436  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1 stored as values in memory (estimated size 5.5 KB, free 899.5 MB)
2019-09-12 11:33:51,436  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 899.5 MB)
2019-09-12 11:33:51,436  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_1_piece0 in memory on 10.6.1.14:5782 (size: 3.4 KB, free: 899.7 MB)
2019-09-12 11:33:51,436  INFO org.apache.spark.SparkContext 54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-09-12 11:33:51,468  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:34) (first 15 tasks are for partitions Vector(0))
2019-09-12 11:33:51,468  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 0.0 with 1 tasks
2019-09-12 11:33:51,530  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes)
2019-09-12 11:33:51,546  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 0.0 (TID 0)
2019-09-12 11:33:51,639  INFO org.apache.spark.rdd.BinaryFileRDD 54 - Input split: Paths:/D:/MyProject/splitCHword/target/classes/test.csv:0+326
2019-09-12 11:33:51,811  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 0.0 (TID 0). 853 bytes result sent to driver
2019-09-12 11:33:51,811  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 0.0 (TID 0) in 312 ms on localhost (executor driver) (1/1)
2019-09-12 11:33:51,811  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-09-12 11:33:51,827  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 0 (load at SparkUtil.java:34) finished in 0.344 s
2019-09-12 11:33:51,827  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 0 finished: load at SparkUtil.java:34, took 0.443181 s
2019-09-12 11:33:52,264  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_1_piece0 on 10.6.1.14:5782 in memory (size: 3.4 KB, free: 899.7 MB)
2019-09-12 11:33:52,264  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_0_piece0 on 10.6.1.14:5782 in memory (size: 20.7 KB, free: 899.7 MB)
2019-09-12 11:33:53,545  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Pruning directories with: 
2019-09-12 11:33:53,545  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Post-Scan Filters: 
2019-09-12 11:33:53,545  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Output Data Schema: struct<feature: string>
2019-09-12 11:33:53,545  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Pushed Filters: 
2019-09-12 11:33:54,326  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 430.626192 ms
2019-09-12 11:33:54,342  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2 stored as values in memory (estimated size 220.5 KB, free 899.5 MB)
2019-09-12 11:33:54,389  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 11:33:54,389  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_2_piece0 in memory on 10.6.1.14:5782 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 11:33:54,404  INFO org.apache.spark.SparkContext 54 - Created broadcast 2 from show at SparkUtil.java:57
2019-09-12 11:33:54,436  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2019-09-12 11:33:54,592  INFO org.apache.spark.SparkContext 54 - Starting job: show at SparkUtil.java:57
2019-09-12 11:33:54,592  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 1 (show at SparkUtil.java:57) with 1 output partitions
2019-09-12 11:33:54,592  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 1 (show at SparkUtil.java:57)
2019-09-12 11:33:54,592  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 11:33:54,592  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 11:33:54,607  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:57), which has no missing parents
2019-09-12 11:33:54,639  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3 stored as values in memory (estimated size 15.4 KB, free 899.4 MB)
2019-09-12 11:33:54,639  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.3 KB, free 899.4 MB)
2019-09-12 11:33:54,654  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_3_piece0 in memory on 10.6.1.14:5782 (size: 7.3 KB, free: 899.7 MB)
2019-09-12 11:33:54,654  INFO org.apache.spark.SparkContext 54 - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2019-09-12 11:33:54,654  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:57) (first 15 tasks are for partitions Vector(0))
2019-09-12 11:33:54,654  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 1.0 with 1 tasks
2019-09-12 11:33:54,670  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5293 bytes)
2019-09-12 11:33:54,670  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 1.0 (TID 1)
2019-09-12 11:33:54,685  INFO org.apache.spark.sql.execution.datasources.FileScanRDD 54 - Reading File path: file:///D:/MyProject/splitCHword/target/classes/test.csv, range: 0-326, partition values: [empty row]
2019-09-12 11:33:54,717  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 20.087023 ms
2019-09-12 11:33:54,982  INFO org.ansj.util.MyStaticValue 106 - init version.number to env value is : 2.11.8
2019-09-12 11:33:54,982  INFO org.ansj.util.MyStaticValue 106 - init copyright.string to env value is : Copyright 2002-2016, LAMP/EPFL
2019-09-12 11:33:54,982  INFO org.ansj.util.MyStaticValue 106 - init osgi.version.number to env value is : 2.11.8.v20160304-115712-1706a37eb8
2019-09-12 11:33:54,982  INFO org.ansj.util.MyStaticValue 106 - init maven.version.number to env value is : 2.11.8
2019-09-12 11:33:54,982  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/ambiguity.dic
2019-09-12 11:33:54,982 ERROR org.ansj.library.AmbiguityLibrary 117 - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:D:\MyProject\splitCHword\library\ambiguity.dic not found or can not to read, path: library/ambiguity.dic
2019-09-12 11:33:54,982  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/default.dic
2019-09-12 11:33:54,982 ERROR org.ansj.library.DicLibrary 221 - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:D:\MyProject\splitCHword\library\default.dic not found or can not to read, path: library/default.dic
2019-09-12 11:33:56,029  INFO org.ansj.library.DATDictionary 90 - init core library ok use time : 953
2019-09-12 11:33:56,513  INFO org.ansj.library.NgramLibrary 17 - init ngram ok use time :484
2019-09-12 11:33:56,513  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 1.0 (TID 1). 1429 bytes result sent to driver
2019-09-12 11:33:56,513  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 1.0 (TID 1) in 1859 ms on localhost (executor driver) (1/1)
2019-09-12 11:33:56,513  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-09-12 11:33:56,513  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 1 (show at SparkUtil.java:57) finished in 1.859 s
2019-09-12 11:33:56,513  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 1 finished: show at SparkUtil.java:57, took 1.927026 s
2019-09-12 11:33:56,544  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 11.096737 ms
2019-09-12 11:33:56,576  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Pruning directories with: 
2019-09-12 11:33:56,576  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Post-Scan Filters: 
2019-09-12 11:33:56,576  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Output Data Schema: struct<feature: string>
2019-09-12 11:33:56,576  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Pushed Filters: 
2019-09-12 11:33:56,591  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_4 stored as values in memory (estimated size 220.5 KB, free 899.2 MB)
2019-09-12 11:33:56,607  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_4_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.2 MB)
2019-09-12 11:33:56,623  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_4_piece0 in memory on 10.6.1.14:5782 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 11:33:56,623  INFO org.apache.spark.SparkContext 54 - Created broadcast 4 from show at SparkUtil.java:65
2019-09-12 11:33:56,623  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2019-09-12 11:33:56,623  INFO org.apache.spark.SparkContext 54 - Starting job: show at SparkUtil.java:65
2019-09-12 11:33:56,638  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 2 (show at SparkUtil.java:65) with 1 output partitions
2019-09-12 11:33:56,638  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 2 (show at SparkUtil.java:65)
2019-09-12 11:33:56,638  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 11:33:56,638  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 11:33:56,638  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 2 (MapPartitionsRDD[9] at show at SparkUtil.java:65), which has no missing parents
2019-09-12 11:33:56,638  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_5 stored as values in memory (estimated size 15.4 KB, free 899.2 MB)
2019-09-12 11:33:56,638  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.3 KB, free 899.2 MB)
2019-09-12 11:33:56,638  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_5_piece0 in memory on 10.6.1.14:5782 (size: 7.3 KB, free: 899.6 MB)
2019-09-12 11:33:56,638  INFO org.apache.spark.SparkContext 54 - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
2019-09-12 11:33:56,638  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at show at SparkUtil.java:65) (first 15 tasks are for partitions Vector(0))
2019-09-12 11:33:56,638  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 2.0 with 1 tasks
2019-09-12 11:33:56,638  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 5293 bytes)
2019-09-12 11:33:56,638  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 2.0 (TID 2)
2019-09-12 11:33:56,654  INFO org.apache.spark.sql.execution.datasources.FileScanRDD 54 - Reading File path: file:///D:/MyProject/splitCHword/target/classes/test.csv, range: 0-326, partition values: [empty row]
2019-09-12 11:33:56,654  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 2.0 (TID 2). 1300 bytes result sent to driver
2019-09-12 11:33:56,654  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 2.0 (TID 2) in 16 ms on localhost (executor driver) (1/1)
2019-09-12 11:33:56,654  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-09-12 11:33:56,654  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 2 (show at SparkUtil.java:65) finished in 0.016 s
2019-09-12 11:33:56,654  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 2 finished: show at SparkUtil.java:65, took 0.031565 s
2019-09-12 11:33:56,669  INFO org.apache.spark.SparkContext 54 - Invoking stop() from shutdown hook
2019-09-12 11:33:56,669  INFO org.spark_project.jetty.server.AbstractConnector 310 - Stopped Spark@297ea53a{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 11:33:56,669  INFO org.apache.spark.ui.SparkUI 54 - Stopped Spark web UI at http://10.6.1.14:4042
2019-09-12 11:33:56,685  INFO org.apache.spark.MapOutputTrackerMasterEndpoint 54 - MapOutputTrackerMasterEndpoint stopped!
2019-09-12 11:33:56,732  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore cleared
2019-09-12 11:33:56,732  INFO org.apache.spark.storage.BlockManager 54 - BlockManager stopped
2019-09-12 11:33:56,748  INFO org.apache.spark.storage.BlockManagerMaster 54 - BlockManagerMaster stopped
2019-09-12 11:33:56,748  INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint 54 - OutputCommitCoordinator stopped!
2019-09-12 11:33:56,748  INFO org.apache.spark.SparkContext 54 - Successfully stopped SparkContext
2019-09-12 11:33:56,748  INFO org.apache.spark.util.ShutdownHookManager 54 - Shutdown hook called
2019-09-12 11:33:56,763  INFO org.apache.spark.util.ShutdownHookManager 54 - Deleting directory C:\Users\OUY\AppData\Local\Temp\spark-3d5427ed-c736-494b-8edc-1e7034c06cb4
2019-09-12 11:35:34,377  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-12 11:35:34,996  WARN org.apache.hadoop.util.NativeCodeLoader 62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-12 11:35:35,090 ERROR org.apache.hadoop.util.Shell 396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2430)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
	at com.datamodel.SparkUtil.CreateSession(SparkUtil.java:25)
	at com.datamodel.SparkUtil.readDataFromCsv(SparkUtil.java:30)
	at com.datamodel.SparkUtil.main(SparkUtil.java:43)
2019-09-12 11:35:35,215  INFO org.apache.spark.SparkContext 54 - Submitted application: splitCHWord
2019-09-12 11:35:35,246  INFO org.apache.spark.SecurityManager 54 - Changing view acls to: OUY
2019-09-12 11:35:35,246  INFO org.apache.spark.SecurityManager 54 - Changing modify acls to: OUY
2019-09-12 11:35:35,246  INFO org.apache.spark.SecurityManager 54 - Changing view acls groups to: 
2019-09-12 11:35:35,262  INFO org.apache.spark.SecurityManager 54 - Changing modify acls groups to: 
2019-09-12 11:35:35,262  INFO org.apache.spark.SecurityManager 54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(OUY); groups with view permissions: Set(); users  with modify permissions: Set(OUY); groups with modify permissions: Set()
2019-09-12 11:35:35,965  INFO org.apache.spark.util.Utils 54 - Successfully started service 'sparkDriver' on port 5810.
2019-09-12 11:35:36,043  INFO org.apache.spark.SparkEnv 54 - Registering MapOutputTracker
2019-09-12 11:35:36,059  INFO org.apache.spark.SparkEnv 54 - Registering BlockManagerMaster
2019-09-12 11:35:36,074  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-09-12 11:35:36,074  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - BlockManagerMasterEndpoint up
2019-09-12 11:35:36,090  INFO org.apache.spark.storage.DiskBlockManager 54 - Created local directory at C:\Users\OUY\AppData\Local\Temp\blockmgr-54114178-6257-4553-8e4e-243708320edd
2019-09-12 11:35:36,121  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore started with capacity 899.7 MB
2019-09-12 11:35:36,199  INFO org.apache.spark.SparkEnv 54 - Registering OutputCommitCoordinator
2019-09-12 11:35:36,309  INFO org.spark_project.jetty.util.log 192 - Logging initialized @4086ms
2019-09-12 11:35:36,356  INFO org.spark_project.jetty.server.Server 345 - jetty-9.3.z-SNAPSHOT
2019-09-12 11:35:36,371  INFO org.spark_project.jetty.server.Server 403 - Started @4165ms
2019-09-12 11:35:36,387  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2019-09-12 11:35:36,387  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
2019-09-12 11:35:36,402  INFO org.spark_project.jetty.server.AbstractConnector 270 - Started ServerConnector@4052274f{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 11:35:36,402  INFO org.apache.spark.util.Utils 54 - Successfully started service 'SparkUI' on port 4042.
2019-09-12 11:35:36,434  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7e242b4d{/jobs,null,AVAILABLE,@Spark}
2019-09-12 11:35:36,434  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4eed46ee{/jobs/json,null,AVAILABLE,@Spark}
2019-09-12 11:35:36,434  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4fad94a7{/jobs/job,null,AVAILABLE,@Spark}
2019-09-12 11:35:36,434  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5241cf67{/jobs/job/json,null,AVAILABLE,@Spark}
2019-09-12 11:35:36,434  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@77192705{/stages,null,AVAILABLE,@Spark}
2019-09-12 11:35:36,449  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7e809b79{/stages/json,null,AVAILABLE,@Spark}
2019-09-12 11:35:36,449  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@625e134e{/stages/stage,null,AVAILABLE,@Spark}
2019-09-12 11:35:36,449  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4fe89c24{/stages/stage/json,null,AVAILABLE,@Spark}
2019-09-12 11:35:36,449  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3d08f3f5{/stages/pool,null,AVAILABLE,@Spark}
2019-09-12 11:35:36,449  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1a1da881{/stages/pool/json,null,AVAILABLE,@Spark}
2019-09-12 11:35:36,449  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7fd4acee{/storage,null,AVAILABLE,@Spark}
2019-09-12 11:35:36,449  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6175619b{/storage/json,null,AVAILABLE,@Spark}
2019-09-12 11:35:36,449  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@756cf158{/storage/rdd,null,AVAILABLE,@Spark}
2019-09-12 11:35:36,449  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@751d3241{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-09-12 11:35:36,449  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@64337702{/environment,null,AVAILABLE,@Spark}
2019-09-12 11:35:36,449  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@30ea8c23{/environment/json,null,AVAILABLE,@Spark}
2019-09-12 11:35:36,465  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4e76dac{/executors,null,AVAILABLE,@Spark}
2019-09-12 11:35:36,465  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5f2f577{/executors/json,null,AVAILABLE,@Spark}
2019-09-12 11:35:36,465  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5d465e4b{/executors/threadDump,null,AVAILABLE,@Spark}
2019-09-12 11:35:36,465  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@41a90fa8{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-09-12 11:35:36,465  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@52500920{/static,null,AVAILABLE,@Spark}
2019-09-12 11:35:36,465  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5ac86ba5{/,null,AVAILABLE,@Spark}
2019-09-12 11:35:36,480  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2c9399a4{/api,null,AVAILABLE,@Spark}
2019-09-12 11:35:36,480  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1c98290c{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-09-12 11:35:36,480  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5bda80bf{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-09-12 11:35:36,480  INFO org.apache.spark.ui.SparkUI 54 - Bound SparkUI to 0.0.0.0, and started at http://10.6.1.14:4042
2019-09-12 11:35:36,637  INFO org.apache.spark.executor.Executor 54 - Starting executor ID driver on host localhost
2019-09-12 11:35:36,684  INFO org.apache.spark.util.Utils 54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 5823.
2019-09-12 11:35:36,684  INFO org.apache.spark.network.netty.NettyBlockTransferService 54 - Server created on 10.6.1.14:5823
2019-09-12 11:35:36,699  INFO org.apache.spark.storage.BlockManager 54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-09-12 11:35:36,699  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registering BlockManager BlockManagerId(driver, 10.6.1.14, 5823, None)
2019-09-12 11:35:36,699  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Registering block manager 10.6.1.14:5823 with 899.7 MB RAM, BlockManagerId(driver, 10.6.1.14, 5823, None)
2019-09-12 11:35:36,715  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registered BlockManager BlockManagerId(driver, 10.6.1.14, 5823, None)
2019-09-12 11:35:36,715  INFO org.apache.spark.storage.BlockManager 54 - Initialized BlockManager: BlockManagerId(driver, 10.6.1.14, 5823, None)
2019-09-12 11:35:36,996  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@669d2b1b{/metrics/json,null,AVAILABLE,@Spark}
2019-09-12 11:35:37,090  INFO org.apache.spark.sql.internal.SharedState 54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/MyProject/splitCHword/spark-warehouse/').
2019-09-12 11:35:37,090  INFO org.apache.spark.sql.internal.SharedState 54 - Warehouse path is 'file:/D:/MyProject/splitCHword/spark-warehouse/'.
2019-09-12 11:35:37,105  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4604b900{/SQL,null,AVAILABLE,@Spark}
2019-09-12 11:35:37,105  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@e36bb2a{/SQL/json,null,AVAILABLE,@Spark}
2019-09-12 11:35:37,105  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3956b302{/SQL/execution,null,AVAILABLE,@Spark}
2019-09-12 11:35:37,105  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1fd386c3{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-09-12 11:35:37,105  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@cd7f1ae{/static/sql,null,AVAILABLE,@Spark}
2019-09-12 11:35:38,121  INFO org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef 54 - Registered StateStoreCoordinator endpoint
2019-09-12 11:35:38,402  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0 stored as values in memory (estimated size 219.8 KB, free 899.5 MB)
2019-09-12 11:35:38,480  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 11:35:38,480  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_0_piece0 in memory on 10.6.1.14:5823 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 11:35:38,496  INFO org.apache.spark.SparkContext 54 - Created broadcast 0 from load at SparkUtil.java:34
2019-09-12 11:35:38,808  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 11:35:38,808  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 11:35:38,824  INFO org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 413 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 326
2019-09-12 11:35:38,902  INFO org.apache.spark.SparkContext 54 - Starting job: load at SparkUtil.java:34
2019-09-12 11:35:38,917  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 0 (load at SparkUtil.java:34) with 1 output partitions
2019-09-12 11:35:38,917  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 0 (load at SparkUtil.java:34)
2019-09-12 11:35:38,917  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 11:35:38,917  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 11:35:38,917  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:34), which has no missing parents
2019-09-12 11:35:38,933  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1 stored as values in memory (estimated size 5.5 KB, free 899.5 MB)
2019-09-12 11:35:38,949  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 899.5 MB)
2019-09-12 11:35:38,949  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_1_piece0 in memory on 10.6.1.14:5823 (size: 3.4 KB, free: 899.7 MB)
2019-09-12 11:35:38,949  INFO org.apache.spark.SparkContext 54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-09-12 11:35:38,980  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:34) (first 15 tasks are for partitions Vector(0))
2019-09-12 11:35:38,980  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 0.0 with 1 tasks
2019-09-12 11:35:39,027  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes)
2019-09-12 11:35:39,042  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 0.0 (TID 0)
2019-09-12 11:35:39,120  INFO org.apache.spark.rdd.BinaryFileRDD 54 - Input split: Paths:/D:/MyProject/splitCHword/target/classes/test.csv:0+326
2019-09-12 11:35:39,324  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 0.0 (TID 0). 853 bytes result sent to driver
2019-09-12 11:35:39,324  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 0.0 (TID 0) in 313 ms on localhost (executor driver) (1/1)
2019-09-12 11:35:39,324  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-09-12 11:35:39,324  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 0 (load at SparkUtil.java:34) finished in 0.328 s
2019-09-12 11:35:39,339  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 0 finished: load at SparkUtil.java:34, took 0.441408 s
2019-09-12 11:35:39,761  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_1_piece0 on 10.6.1.14:5823 in memory (size: 3.4 KB, free: 899.7 MB)
2019-09-12 11:35:41,042  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Pruning directories with: 
2019-09-12 11:35:41,042  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Post-Scan Filters: 
2019-09-12 11:35:41,042  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Output Data Schema: struct<feature: string>
2019-09-12 11:35:41,058  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Pushed Filters: 
2019-09-12 11:35:41,229  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_0_piece0 on 10.6.1.14:5823 in memory (size: 20.7 KB, free: 899.7 MB)
2019-09-12 11:35:41,698  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 294.674522 ms
2019-09-12 11:35:41,714  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2 stored as values in memory (estimated size 220.5 KB, free 899.5 MB)
2019-09-12 11:35:41,729  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 11:35:41,729  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_2_piece0 in memory on 10.6.1.14:5823 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 11:35:41,729  INFO org.apache.spark.SparkContext 54 - Created broadcast 2 from show at SparkUtil.java:57
2019-09-12 11:35:41,745  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2019-09-12 11:35:41,807  INFO org.apache.spark.SparkContext 54 - Starting job: show at SparkUtil.java:57
2019-09-12 11:35:41,807  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 1 (show at SparkUtil.java:57) with 1 output partitions
2019-09-12 11:35:41,807  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 1 (show at SparkUtil.java:57)
2019-09-12 11:35:41,807  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 11:35:41,807  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 11:35:41,807  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:57), which has no missing parents
2019-09-12 11:35:41,839  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3 stored as values in memory (estimated size 15.4 KB, free 899.4 MB)
2019-09-12 11:35:41,839  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.3 KB, free 899.4 MB)
2019-09-12 11:35:41,839  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_3_piece0 in memory on 10.6.1.14:5823 (size: 7.3 KB, free: 899.7 MB)
2019-09-12 11:35:41,839  INFO org.apache.spark.SparkContext 54 - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2019-09-12 11:35:41,839  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:57) (first 15 tasks are for partitions Vector(0))
2019-09-12 11:35:41,839  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 1.0 with 1 tasks
2019-09-12 11:35:41,854  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5293 bytes)
2019-09-12 11:35:41,854  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 1.0 (TID 1)
2019-09-12 11:35:41,870  INFO org.apache.spark.sql.execution.datasources.FileScanRDD 54 - Reading File path: file:///D:/MyProject/splitCHword/target/classes/test.csv, range: 0-326, partition values: [empty row]
2019-09-12 11:35:41,885  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 12.861856 ms
2019-09-12 11:35:42,151  INFO org.ansj.util.MyStaticValue 106 - init version.number to env value is : 2.11.8
2019-09-12 11:35:42,151  INFO org.ansj.util.MyStaticValue 106 - init copyright.string to env value is : Copyright 2002-2016, LAMP/EPFL
2019-09-12 11:35:42,151  INFO org.ansj.util.MyStaticValue 106 - init osgi.version.number to env value is : 2.11.8.v20160304-115712-1706a37eb8
2019-09-12 11:35:42,151  INFO org.ansj.util.MyStaticValue 106 - init maven.version.number to env value is : 2.11.8
2019-09-12 11:35:42,167  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/ambiguity.dic
2019-09-12 11:35:42,167 ERROR org.ansj.library.AmbiguityLibrary 117 - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:D:\MyProject\splitCHword\library\ambiguity.dic not found or can not to read, path: library/ambiguity.dic
2019-09-12 11:35:42,167  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/default.dic
2019-09-12 11:35:42,167 ERROR org.ansj.library.DicLibrary 221 - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:D:\MyProject\splitCHword\library\default.dic not found or can not to read, path: library/default.dic
2019-09-12 11:35:43,182  INFO org.ansj.library.DATDictionary 90 - init core library ok use time : 922
2019-09-12 11:35:43,604  INFO org.ansj.library.NgramLibrary 17 - init ngram ok use time :422
2019-09-12 11:35:43,619  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 1.0 (TID 1). 1386 bytes result sent to driver
2019-09-12 11:35:43,619  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 1.0 (TID 1) in 1780 ms on localhost (executor driver) (1/1)
2019-09-12 11:35:43,619  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-09-12 11:35:43,619  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 1 (show at SparkUtil.java:57) finished in 1.780 s
2019-09-12 11:35:43,619  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 1 finished: show at SparkUtil.java:57, took 1.806145 s
2019-09-12 11:35:43,635  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 8.997966 ms
2019-09-12 11:35:43,666  INFO org.apache.spark.SparkContext 54 - Invoking stop() from shutdown hook
2019-09-12 11:35:43,682  INFO org.spark_project.jetty.server.AbstractConnector 310 - Stopped Spark@4052274f{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 11:35:43,682  INFO org.apache.spark.ui.SparkUI 54 - Stopped Spark web UI at http://10.6.1.14:4042
2019-09-12 11:35:43,698  INFO org.apache.spark.MapOutputTrackerMasterEndpoint 54 - MapOutputTrackerMasterEndpoint stopped!
2019-09-12 11:35:43,744  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore cleared
2019-09-12 11:35:43,744  INFO org.apache.spark.storage.BlockManager 54 - BlockManager stopped
2019-09-12 11:35:43,744  INFO org.apache.spark.storage.BlockManagerMaster 54 - BlockManagerMaster stopped
2019-09-12 11:35:43,744  INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint 54 - OutputCommitCoordinator stopped!
2019-09-12 11:35:43,744  INFO org.apache.spark.SparkContext 54 - Successfully stopped SparkContext
2019-09-12 11:35:43,744  INFO org.apache.spark.util.ShutdownHookManager 54 - Shutdown hook called
2019-09-12 11:35:43,760  INFO org.apache.spark.util.ShutdownHookManager 54 - Deleting directory C:\Users\OUY\AppData\Local\Temp\spark-620c85b6-5ec6-4ec2-885b-61b026592181
2019-09-12 14:12:55,617  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-12 14:12:56,261  WARN org.apache.hadoop.util.NativeCodeLoader 62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-12 14:12:56,347 ERROR org.apache.hadoop.util.Shell 396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2430)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
	at com.datamodel.SparkUtil.CreateSession(SparkUtil.java:29)
	at com.datamodel.SparkUtil.readDataFromCsv(SparkUtil.java:34)
	at com.datamodel.SparkUtil.main(SparkUtil.java:54)
2019-09-12 14:12:56,465  INFO org.apache.spark.SparkContext 54 - Submitted application: splitCHWord
2019-09-12 14:12:56,501  INFO org.apache.spark.SecurityManager 54 - Changing view acls to: OUY
2019-09-12 14:12:56,502  INFO org.apache.spark.SecurityManager 54 - Changing modify acls to: OUY
2019-09-12 14:12:56,504  INFO org.apache.spark.SecurityManager 54 - Changing view acls groups to: 
2019-09-12 14:12:56,505  INFO org.apache.spark.SecurityManager 54 - Changing modify acls groups to: 
2019-09-12 14:12:56,507  INFO org.apache.spark.SecurityManager 54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(OUY); groups with view permissions: Set(); users  with modify permissions: Set(OUY); groups with modify permissions: Set()
2019-09-12 14:12:57,235  INFO org.apache.spark.util.Utils 54 - Successfully started service 'sparkDriver' on port 7265.
2019-09-12 14:12:57,296  INFO org.apache.spark.SparkEnv 54 - Registering MapOutputTracker
2019-09-12 14:12:57,318  INFO org.apache.spark.SparkEnv 54 - Registering BlockManagerMaster
2019-09-12 14:12:57,323  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-09-12 14:12:57,323  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - BlockManagerMasterEndpoint up
2019-09-12 14:12:57,336  INFO org.apache.spark.storage.DiskBlockManager 54 - Created local directory at C:\Users\OUY\AppData\Local\Temp\blockmgr-f9d6f074-63fa-4683-ac2e-a3de29700e26
2019-09-12 14:12:57,361  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore started with capacity 899.7 MB
2019-09-12 14:12:57,429  INFO org.apache.spark.SparkEnv 54 - Registering OutputCommitCoordinator
2019-09-12 14:12:57,532  INFO org.spark_project.jetty.util.log 192 - Logging initialized @4179ms
2019-09-12 14:12:57,601  INFO org.spark_project.jetty.server.Server 345 - jetty-9.3.z-SNAPSHOT
2019-09-12 14:12:57,617  INFO org.spark_project.jetty.server.Server 403 - Started @4266ms
2019-09-12 14:12:57,639  INFO org.spark_project.jetty.server.AbstractConnector 270 - Started ServerConnector@1643d68f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-09-12 14:12:57,639  INFO org.apache.spark.util.Utils 54 - Successfully started service 'SparkUI' on port 4040.
2019-09-12 14:12:57,670  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@410ae9a3{/jobs,null,AVAILABLE,@Spark}
2019-09-12 14:12:57,672  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3c0fae6c{/jobs/json,null,AVAILABLE,@Spark}
2019-09-12 14:12:57,672  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@52b56a3e{/jobs/job,null,AVAILABLE,@Spark}
2019-09-12 14:12:57,674  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@36b0fcd5{/jobs/job/json,null,AVAILABLE,@Spark}
2019-09-12 14:12:57,675  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@475835b1{/stages,null,AVAILABLE,@Spark}
2019-09-12 14:12:57,677  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5241cf67{/stages/json,null,AVAILABLE,@Spark}
2019-09-12 14:12:57,678  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@77192705{/stages/stage,null,AVAILABLE,@Spark}
2019-09-12 14:12:57,680  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@625e134e{/stages/stage/json,null,AVAILABLE,@Spark}
2019-09-12 14:12:57,681  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@89c10b7{/stages/pool,null,AVAILABLE,@Spark}
2019-09-12 14:12:57,682  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4fe89c24{/stages/pool/json,null,AVAILABLE,@Spark}
2019-09-12 14:12:57,683  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3d08f3f5{/storage,null,AVAILABLE,@Spark}
2019-09-12 14:12:57,684  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1a1da881{/storage/json,null,AVAILABLE,@Spark}
2019-09-12 14:12:57,685  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7fd4acee{/storage/rdd,null,AVAILABLE,@Spark}
2019-09-12 14:12:57,686  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6175619b{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-09-12 14:12:57,687  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@756cf158{/environment,null,AVAILABLE,@Spark}
2019-09-12 14:12:57,689  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@751d3241{/environment/json,null,AVAILABLE,@Spark}
2019-09-12 14:12:57,690  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@64337702{/executors,null,AVAILABLE,@Spark}
2019-09-12 14:12:57,691  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@30ea8c23{/executors/json,null,AVAILABLE,@Spark}
2019-09-12 14:12:57,693  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4e76dac{/executors/threadDump,null,AVAILABLE,@Spark}
2019-09-12 14:12:57,694  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5f2f577{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-09-12 14:12:57,706  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5d465e4b{/static,null,AVAILABLE,@Spark}
2019-09-12 14:12:57,707  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@70dd7e15{/,null,AVAILABLE,@Spark}
2019-09-12 14:12:57,708  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@35beb15e{/api,null,AVAILABLE,@Spark}
2019-09-12 14:12:57,709  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@9635fa{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-09-12 14:12:57,710  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@63c5efee{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-09-12 14:12:57,713  INFO org.apache.spark.ui.SparkUI 54 - Bound SparkUI to 0.0.0.0, and started at http://10.6.1.14:4040
2019-09-12 14:12:57,887  INFO org.apache.spark.executor.Executor 54 - Starting executor ID driver on host localhost
2019-09-12 14:12:57,927  INFO org.apache.spark.util.Utils 54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 7278.
2019-09-12 14:12:57,928  INFO org.apache.spark.network.netty.NettyBlockTransferService 54 - Server created on 10.6.1.14:7278
2019-09-12 14:12:57,931  INFO org.apache.spark.storage.BlockManager 54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-09-12 14:12:57,933  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registering BlockManager BlockManagerId(driver, 10.6.1.14, 7278, None)
2019-09-12 14:12:57,944  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Registering block manager 10.6.1.14:7278 with 899.7 MB RAM, BlockManagerId(driver, 10.6.1.14, 7278, None)
2019-09-12 14:12:57,949  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registered BlockManager BlockManagerId(driver, 10.6.1.14, 7278, None)
2019-09-12 14:12:57,950  INFO org.apache.spark.storage.BlockManager 54 - Initialized BlockManager: BlockManagerId(driver, 10.6.1.14, 7278, None)
2019-09-12 14:12:58,253  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@79c3f01f{/metrics/json,null,AVAILABLE,@Spark}
2019-09-12 14:12:58,341  INFO org.apache.spark.sql.internal.SharedState 54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/MyProject/splitCHword/spark-warehouse/').
2019-09-12 14:12:58,342  INFO org.apache.spark.sql.internal.SharedState 54 - Warehouse path is 'file:/D:/MyProject/splitCHword/spark-warehouse/'.
2019-09-12 14:12:58,351  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@15515c51{/SQL,null,AVAILABLE,@Spark}
2019-09-12 14:12:58,353  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@64a896b0{/SQL/json,null,AVAILABLE,@Spark}
2019-09-12 14:12:58,354  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@16943e88{/SQL/execution,null,AVAILABLE,@Spark}
2019-09-12 14:12:58,354  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@73d6d0c{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-09-12 14:12:58,357  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3956b302{/static/sql,null,AVAILABLE,@Spark}
2019-09-12 14:12:59,420  INFO org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef 54 - Registered StateStoreCoordinator endpoint
2019-09-12 14:12:59,746  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0 stored as values in memory (estimated size 219.8 KB, free 899.5 MB)
2019-09-12 14:12:59,886  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 14:12:59,892  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_0_piece0 in memory on 10.6.1.14:7278 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 14:12:59,897  INFO org.apache.spark.SparkContext 54 - Created broadcast 0 from load at SparkUtil.java:38
2019-09-12 14:13:00,389  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 14:13:00,392  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 14:13:00,407  INFO org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 413 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 326
2019-09-12 14:13:00,465  INFO org.apache.spark.SparkContext 54 - Starting job: load at SparkUtil.java:38
2019-09-12 14:13:00,505  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 0 (load at SparkUtil.java:38) with 1 output partitions
2019-09-12 14:13:00,506  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 0 (load at SparkUtil.java:38)
2019-09-12 14:13:00,507  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 14:13:00,508  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 14:13:00,514  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:38), which has no missing parents
2019-09-12 14:13:00,536  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1 stored as values in memory (estimated size 5.5 KB, free 899.5 MB)
2019-09-12 14:13:00,544  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 899.5 MB)
2019-09-12 14:13:00,546  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_1_piece0 in memory on 10.6.1.14:7278 (size: 3.4 KB, free: 899.7 MB)
2019-09-12 14:13:00,547  INFO org.apache.spark.SparkContext 54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-09-12 14:13:00,574  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at SparkUtil.java:38) (first 15 tasks are for partitions Vector(0))
2019-09-12 14:13:00,575  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 0.0 with 1 tasks
2019-09-12 14:13:00,642  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes)
2019-09-12 14:13:00,655  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 0.0 (TID 0)
2019-09-12 14:13:00,753  INFO org.apache.spark.rdd.BinaryFileRDD 54 - Input split: Paths:/D:/MyProject/splitCHword/target/classes/test.csv:0+326
2019-09-12 14:13:00,984  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 0.0 (TID 0). 853 bytes result sent to driver
2019-09-12 14:13:00,994  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 0.0 (TID 0) in 367 ms on localhost (executor driver) (1/1)
2019-09-12 14:13:00,997  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-09-12 14:13:01,003  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 0 (load at SparkUtil.java:38) finished in 0.401 s
2019-09-12 14:13:01,011  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 0 finished: load at SparkUtil.java:38, took 0.545554 s
2019-09-12 14:13:01,636  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_1_piece0 on 10.6.1.14:7278 in memory (size: 3.4 KB, free: 899.7 MB)
2019-09-12 14:13:02,708  WARN org.apache.spark.sql.SparkSession$Builder 66 - Using an existing SparkSession; some configuration may not take effect.
2019-09-12 14:13:02,719  INFO org.apache.spark.sql.execution.SparkSqlParser 54 - Parsing command: splitCHword(feature)
2019-09-12 14:13:03,420  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_0_piece0 on 10.6.1.14:7278 in memory (size: 20.7 KB, free: 899.7 MB)
2019-09-12 14:13:03,518  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Pruning directories with: 
2019-09-12 14:13:03,522  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Post-Scan Filters: 
2019-09-12 14:13:03,527  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Output Data Schema: struct<label: string, feature: string>
2019-09-12 14:13:03,539  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Pushed Filters: 
2019-09-12 14:13:04,047  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 292.394417 ms
2019-09-12 14:13:04,058  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2 stored as values in memory (estimated size 220.5 KB, free 899.5 MB)
2019-09-12 14:13:04,081  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 14:13:04,099  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_2_piece0 in memory on 10.6.1.14:7278 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 14:13:04,101  INFO org.apache.spark.SparkContext 54 - Created broadcast 2 from show at SparkUtil.java:70
2019-09-12 14:13:04,119  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2019-09-12 14:13:04,204  INFO org.apache.spark.SparkContext 54 - Starting job: show at SparkUtil.java:70
2019-09-12 14:13:04,205  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 1 (show at SparkUtil.java:70) with 1 output partitions
2019-09-12 14:13:04,205  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 1 (show at SparkUtil.java:70)
2019-09-12 14:13:04,205  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 14:13:04,206  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 14:13:04,207  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:70), which has no missing parents
2019-09-12 14:13:04,236  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3 stored as values in memory (estimated size 13.1 KB, free 899.5 MB)
2019-09-12 14:13:04,259  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.8 KB, free 899.4 MB)
2019-09-12 14:13:04,261  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_3_piece0 in memory on 10.6.1.14:7278 (size: 6.8 KB, free: 899.7 MB)
2019-09-12 14:13:04,261  INFO org.apache.spark.SparkContext 54 - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2019-09-12 14:13:04,263  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at SparkUtil.java:70) (first 15 tasks are for partitions Vector(0))
2019-09-12 14:13:04,263  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 1.0 with 1 tasks
2019-09-12 14:13:04,273  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5293 bytes)
2019-09-12 14:13:04,274  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 1.0 (TID 1)
2019-09-12 14:13:04,303  INFO org.apache.spark.sql.execution.datasources.FileScanRDD 54 - Reading File path: file:///D:/MyProject/splitCHword/target/classes/test.csv, range: 0-326, partition values: [empty row]
2019-09-12 14:13:04,341  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 31.122321 ms
2019-09-12 14:13:04,628  INFO org.ansj.util.MyStaticValue 106 - init version.number to env value is : 2.11.8
2019-09-12 14:13:04,628  INFO org.ansj.util.MyStaticValue 106 - init copyright.string to env value is : Copyright 2002-2016, LAMP/EPFL
2019-09-12 14:13:04,628  INFO org.ansj.util.MyStaticValue 106 - init osgi.version.number to env value is : 2.11.8.v20160304-115712-1706a37eb8
2019-09-12 14:13:04,628  INFO org.ansj.util.MyStaticValue 106 - init maven.version.number to env value is : 2.11.8
2019-09-12 14:13:04,632  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/ambiguity.dic
2019-09-12 14:13:04,633 ERROR org.ansj.library.AmbiguityLibrary 117 - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:D:\MyProject\splitCHword\library\ambiguity.dic not found or can not to read, path: library/ambiguity.dic
2019-09-12 14:13:04,635  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/default.dic
2019-09-12 14:13:04,635 ERROR org.ansj.library.DicLibrary 221 - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:D:\MyProject\splitCHword\library\default.dic not found or can not to read, path: library/default.dic
2019-09-12 14:13:05,698  INFO org.ansj.library.DATDictionary 90 - init core library ok use time : 953
2019-09-12 14:13:06,221  INFO org.ansj.library.NgramLibrary 17 - init ngram ok use time :520
2019-09-12 14:13:06,234  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 1.0 (TID 1). 1657 bytes result sent to driver
2019-09-12 14:13:06,295  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 1.0 (TID 1) in 2030 ms on localhost (executor driver) (1/1)
2019-09-12 14:13:06,296  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-09-12 14:13:06,296  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 1 (show at SparkUtil.java:70) finished in 2.032 s
2019-09-12 14:13:06,297  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 1 finished: show at SparkUtil.java:70, took 2.092789 s
2019-09-12 14:13:06,328  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 13.812468 ms
2019-09-12 14:13:06,351  INFO org.apache.spark.SparkContext 54 - Invoking stop() from shutdown hook
2019-09-12 14:13:06,360  INFO org.spark_project.jetty.server.AbstractConnector 310 - Stopped Spark@1643d68f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-09-12 14:13:06,364  INFO org.apache.spark.ui.SparkUI 54 - Stopped Spark web UI at http://10.6.1.14:4040
2019-09-12 14:13:06,382  INFO org.apache.spark.MapOutputTrackerMasterEndpoint 54 - MapOutputTrackerMasterEndpoint stopped!
2019-09-12 14:13:06,427  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore cleared
2019-09-12 14:13:06,427  INFO org.apache.spark.storage.BlockManager 54 - BlockManager stopped
2019-09-12 14:13:06,430  INFO org.apache.spark.storage.BlockManagerMaster 54 - BlockManagerMaster stopped
2019-09-12 14:13:06,434  INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint 54 - OutputCommitCoordinator stopped!
2019-09-12 14:13:06,442  INFO org.apache.spark.SparkContext 54 - Successfully stopped SparkContext
2019-09-12 14:13:06,442  INFO org.apache.spark.util.ShutdownHookManager 54 - Shutdown hook called
2019-09-12 14:13:06,443  INFO org.apache.spark.util.ShutdownHookManager 54 - Deleting directory C:\Users\OUY\AppData\Local\Temp\spark-3c565c83-da4d-4550-9a2a-34d13d9040a3
2019-09-12 15:59:32,130  INFO org.apache.spark.SparkContext 54 - Running Spark version 2.2.0
2019-09-12 15:59:32,813  WARN org.apache.hadoop.util.NativeCodeLoader 62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-12 15:59:32,898 ERROR org.apache.hadoop.util.Shell 396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2430)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2430)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
	at com.datamodel.SplitCHword.main(SplitCHword.java:40)
2019-09-12 15:59:33,039  INFO org.apache.spark.SparkContext 54 - Submitted application: splitCHWord
2019-09-12 15:59:33,080  INFO org.apache.spark.SecurityManager 54 - Changing view acls to: OUY
2019-09-12 15:59:33,081  INFO org.apache.spark.SecurityManager 54 - Changing modify acls to: OUY
2019-09-12 15:59:33,085  INFO org.apache.spark.SecurityManager 54 - Changing view acls groups to: 
2019-09-12 15:59:33,086  INFO org.apache.spark.SecurityManager 54 - Changing modify acls groups to: 
2019-09-12 15:59:33,088  INFO org.apache.spark.SecurityManager 54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(OUY); groups with view permissions: Set(); users  with modify permissions: Set(OUY); groups with modify permissions: Set()
2019-09-12 15:59:33,928  INFO org.apache.spark.util.Utils 54 - Successfully started service 'sparkDriver' on port 9175.
2019-09-12 15:59:33,989  INFO org.apache.spark.SparkEnv 54 - Registering MapOutputTracker
2019-09-12 15:59:34,010  INFO org.apache.spark.SparkEnv 54 - Registering BlockManagerMaster
2019-09-12 15:59:34,014  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-09-12 15:59:34,014  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - BlockManagerMasterEndpoint up
2019-09-12 15:59:34,027  INFO org.apache.spark.storage.DiskBlockManager 54 - Created local directory at C:\Users\OUY\AppData\Local\Temp\blockmgr-c2a0d131-3875-472e-bc02-0346aca6c00f
2019-09-12 15:59:34,052  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore started with capacity 899.7 MB
2019-09-12 15:59:34,142  INFO org.apache.spark.SparkEnv 54 - Registering OutputCommitCoordinator
2019-09-12 15:59:34,306  INFO org.spark_project.jetty.util.log 192 - Logging initialized @4574ms
2019-09-12 15:59:34,420  INFO org.spark_project.jetty.server.Server 345 - jetty-9.3.z-SNAPSHOT
2019-09-12 15:59:34,445  INFO org.spark_project.jetty.server.Server 403 - Started @4716ms
2019-09-12 15:59:34,473  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2019-09-12 15:59:34,475  WARN org.apache.spark.util.Utils 66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
2019-09-12 15:59:34,488  INFO org.spark_project.jetty.server.AbstractConnector 270 - Started ServerConnector@2ed3b1f5{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 15:59:34,489  INFO org.apache.spark.util.Utils 54 - Successfully started service 'SparkUI' on port 4042.
2019-09-12 15:59:34,542  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@44c79f32{/jobs,null,AVAILABLE,@Spark}
2019-09-12 15:59:34,544  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@72bd06ca{/jobs/json,null,AVAILABLE,@Spark}
2019-09-12 15:59:34,855  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5dbe30be{/jobs/job,null,AVAILABLE,@Spark}
2019-09-12 15:59:34,859  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3d08f3f5{/jobs/job/json,null,AVAILABLE,@Spark}
2019-09-12 15:59:34,860  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1a1da881{/stages,null,AVAILABLE,@Spark}
2019-09-12 15:59:34,862  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@7fd4acee{/stages/json,null,AVAILABLE,@Spark}
2019-09-12 15:59:34,869  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@6175619b{/stages/stage,null,AVAILABLE,@Spark}
2019-09-12 15:59:34,871  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@751d3241{/stages/stage/json,null,AVAILABLE,@Spark}
2019-09-12 15:59:34,874  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@64337702{/stages/pool,null,AVAILABLE,@Spark}
2019-09-12 15:59:34,875  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@30ea8c23{/stages/pool/json,null,AVAILABLE,@Spark}
2019-09-12 15:59:34,876  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4e76dac{/storage,null,AVAILABLE,@Spark}
2019-09-12 15:59:34,878  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5f2f577{/storage/json,null,AVAILABLE,@Spark}
2019-09-12 15:59:34,879  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@5d465e4b{/storage/rdd,null,AVAILABLE,@Spark}
2019-09-12 15:59:34,883  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@41a90fa8{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-09-12 15:59:34,884  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@52500920{/environment,null,AVAILABLE,@Spark}
2019-09-12 15:59:34,887  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@18a3962d{/environment/json,null,AVAILABLE,@Spark}
2019-09-12 15:59:34,889  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2a65bb85{/executors,null,AVAILABLE,@Spark}
2019-09-12 15:59:34,890  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@4f936da8{/executors/json,null,AVAILABLE,@Spark}
2019-09-12 15:59:34,891  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@452ba1db{/executors/threadDump,null,AVAILABLE,@Spark}
2019-09-12 15:59:34,893  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@76a36b71{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-09-12 15:59:34,909  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@f9d87b{/static,null,AVAILABLE,@Spark}
2019-09-12 15:59:34,910  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@2ce86164{/,null,AVAILABLE,@Spark}
2019-09-12 15:59:34,912  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@51df223b{/api,null,AVAILABLE,@Spark}
2019-09-12 15:59:34,913  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@60fa3495{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-09-12 15:59:34,914  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@79e18e38{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-09-12 15:59:34,918  INFO org.apache.spark.ui.SparkUI 54 - Bound SparkUI to 0.0.0.0, and started at http://10.6.1.14:4042
2019-09-12 15:59:35,090  INFO org.apache.spark.executor.Executor 54 - Starting executor ID driver on host localhost
2019-09-12 15:59:35,159  INFO org.apache.spark.util.Utils 54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 9188.
2019-09-12 15:59:35,169  INFO org.apache.spark.network.netty.NettyBlockTransferService 54 - Server created on 10.6.1.14:9188
2019-09-12 15:59:35,172  INFO org.apache.spark.storage.BlockManager 54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-09-12 15:59:35,174  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registering BlockManager BlockManagerId(driver, 10.6.1.14, 9188, None)
2019-09-12 15:59:35,180  INFO org.apache.spark.storage.BlockManagerMasterEndpoint 54 - Registering block manager 10.6.1.14:9188 with 899.7 MB RAM, BlockManagerId(driver, 10.6.1.14, 9188, None)
2019-09-12 15:59:35,187  INFO org.apache.spark.storage.BlockManagerMaster 54 - Registered BlockManager BlockManagerId(driver, 10.6.1.14, 9188, None)
2019-09-12 15:59:35,188  INFO org.apache.spark.storage.BlockManager 54 - Initialized BlockManager: BlockManagerId(driver, 10.6.1.14, 9188, None)
2019-09-12 15:59:35,463  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@15b986cd{/metrics/json,null,AVAILABLE,@Spark}
2019-09-12 15:59:35,559  INFO org.apache.spark.sql.internal.SharedState 54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/MyProject/splitCHword/spark-warehouse/').
2019-09-12 15:59:35,560  INFO org.apache.spark.sql.internal.SharedState 54 - Warehouse path is 'file:/D:/MyProject/splitCHword/spark-warehouse/'.
2019-09-12 15:59:35,569  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@1500e009{/SQL,null,AVAILABLE,@Spark}
2019-09-12 15:59:35,570  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@edf4f36{/SQL/json,null,AVAILABLE,@Spark}
2019-09-12 15:59:35,572  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@cd7f1ae{/SQL/execution,null,AVAILABLE,@Spark}
2019-09-12 15:59:35,573  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@3c4bc9fc{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-09-12 15:59:35,576  INFO org.spark_project.jetty.server.handler.ContextHandler 781 - Started o.s.j.s.ServletContextHandler@c827db{/static/sql,null,AVAILABLE,@Spark}
2019-09-12 15:59:36,671  INFO org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef 54 - Registered StateStoreCoordinator endpoint
2019-09-12 15:59:36,965  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0 stored as values in memory (estimated size 219.8 KB, free 899.5 MB)
2019-09-12 15:59:37,088  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 15:59:37,093  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_0_piece0 in memory on 10.6.1.14:9188 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 15:59:37,099  INFO org.apache.spark.SparkContext 54 - Created broadcast 0 from load at SplitCHword.java:47
2019-09-12 15:59:37,468  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 15:59:37,470  INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat 281 - Total input paths to process : 1
2019-09-12 15:59:37,485  INFO org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 413 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 326
2019-09-12 15:59:37,546  INFO org.apache.spark.SparkContext 54 - Starting job: load at SplitCHword.java:47
2019-09-12 15:59:37,566  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 0 (load at SplitCHword.java:47) with 1 output partitions
2019-09-12 15:59:37,567  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 0 (load at SplitCHword.java:47)
2019-09-12 15:59:37,567  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 15:59:37,569  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 15:59:37,575  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 0 (MapPartitionsRDD[2] at load at SplitCHword.java:47), which has no missing parents
2019-09-12 15:59:37,600  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1 stored as values in memory (estimated size 5.5 KB, free 899.5 MB)
2019-09-12 15:59:37,607  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 899.5 MB)
2019-09-12 15:59:37,609  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_1_piece0 in memory on 10.6.1.14:9188 (size: 3.4 KB, free: 899.7 MB)
2019-09-12 15:59:37,613  INFO org.apache.spark.SparkContext 54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-09-12 15:59:37,641  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at SplitCHword.java:47) (first 15 tasks are for partitions Vector(0))
2019-09-12 15:59:37,643  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 0.0 with 1 tasks
2019-09-12 15:59:37,711  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes)
2019-09-12 15:59:37,725  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 0.0 (TID 0)
2019-09-12 15:59:37,818  INFO org.apache.spark.rdd.BinaryFileRDD 54 - Input split: Paths:/D:/MyProject/splitCHword/target/classes/test.csv:0+326
2019-09-12 15:59:38,043  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 0.0 (TID 0). 896 bytes result sent to driver
2019-09-12 15:59:38,055  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 0.0 (TID 0) in 363 ms on localhost (executor driver) (1/1)
2019-09-12 15:59:38,059  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-09-12 15:59:38,063  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 0 (load at SplitCHword.java:47) finished in 0.399 s
2019-09-12 15:59:38,069  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 0 finished: load at SplitCHword.java:47, took 0.522665 s
2019-09-12 15:59:38,560  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_1_piece0 on 10.6.1.14:9188 in memory (size: 3.4 KB, free: 899.7 MB)
2019-09-12 15:59:39,825  INFO org.apache.spark.sql.execution.SparkSqlParser 54 - Parsing command: splitCHword(feature)
2019-09-12 15:59:40,422  INFO org.apache.spark.storage.BlockManagerInfo 54 - Removed broadcast_0_piece0 on 10.6.1.14:9188 in memory (size: 20.7 KB, free: 899.7 MB)
2019-09-12 15:59:40,573  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Pruning directories with: 
2019-09-12 15:59:40,580  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Post-Scan Filters: 
2019-09-12 15:59:40,585  INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy 54 - Output Data Schema: struct<label: string, feature: string>
2019-09-12 15:59:40,603  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Pushed Filters: 
2019-09-12 15:59:41,118  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 288.547594 ms
2019-09-12 15:59:41,129  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2 stored as values in memory (estimated size 220.5 KB, free 899.5 MB)
2019-09-12 15:59:41,149  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.7 KB, free 899.5 MB)
2019-09-12 15:59:41,154  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_2_piece0 in memory on 10.6.1.14:9188 (size: 20.7 KB, free: 899.7 MB)
2019-09-12 15:59:41,156  INFO org.apache.spark.SparkContext 54 - Created broadcast 2 from show at SplitCHword.java:53
2019-09-12 15:59:41,177  INFO org.apache.spark.sql.execution.FileSourceScanExec 54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2019-09-12 15:59:41,245  INFO org.apache.spark.SparkContext 54 - Starting job: show at SplitCHword.java:53
2019-09-12 15:59:41,247  INFO org.apache.spark.scheduler.DAGScheduler 54 - Got job 1 (show at SplitCHword.java:53) with 1 output partitions
2019-09-12 15:59:41,247  INFO org.apache.spark.scheduler.DAGScheduler 54 - Final stage: ResultStage 1 (show at SplitCHword.java:53)
2019-09-12 15:59:41,247  INFO org.apache.spark.scheduler.DAGScheduler 54 - Parents of final stage: List()
2019-09-12 15:59:41,247  INFO org.apache.spark.scheduler.DAGScheduler 54 - Missing parents: List()
2019-09-12 15:59:41,248  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at SplitCHword.java:53), which has no missing parents
2019-09-12 15:59:41,274  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3 stored as values in memory (estimated size 13.1 KB, free 899.5 MB)
2019-09-12 15:59:41,278  INFO org.apache.spark.storage.memory.MemoryStore 54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.8 KB, free 899.4 MB)
2019-09-12 15:59:41,279  INFO org.apache.spark.storage.BlockManagerInfo 54 - Added broadcast_3_piece0 in memory on 10.6.1.14:9188 (size: 6.8 KB, free: 899.7 MB)
2019-09-12 15:59:41,280  INFO org.apache.spark.SparkContext 54 - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2019-09-12 15:59:41,281  INFO org.apache.spark.scheduler.DAGScheduler 54 - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at SplitCHword.java:53) (first 15 tasks are for partitions Vector(0))
2019-09-12 15:59:41,281  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Adding task set 1.0 with 1 tasks
2019-09-12 15:59:41,287  INFO org.apache.spark.scheduler.TaskSetManager 54 - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5293 bytes)
2019-09-12 15:59:41,288  INFO org.apache.spark.executor.Executor 54 - Running task 0.0 in stage 1.0 (TID 1)
2019-09-12 15:59:41,313  INFO org.apache.spark.sql.execution.datasources.FileScanRDD 54 - Reading File path: file:///D:/MyProject/splitCHword/target/classes/test.csv, range: 0-326, partition values: [empty row]
2019-09-12 15:59:41,349  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 29.425895 ms
2019-09-12 15:59:41,608  INFO org.ansj.util.MyStaticValue 106 - init version.number to env value is : 2.11.8
2019-09-12 15:59:41,608  INFO org.ansj.util.MyStaticValue 106 - init copyright.string to env value is : Copyright 2002-2016, LAMP/EPFL
2019-09-12 15:59:41,608  INFO org.ansj.util.MyStaticValue 106 - init osgi.version.number to env value is : 2.11.8.v20160304-115712-1706a37eb8
2019-09-12 15:59:41,608  INFO org.ansj.util.MyStaticValue 106 - init maven.version.number to env value is : 2.11.8
2019-09-12 15:59:41,612  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/ambiguity.dic
2019-09-12 15:59:41,613 ERROR org.ansj.library.AmbiguityLibrary 117 - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:D:\MyProject\splitCHword\library\ambiguity.dic not found or can not to read, path: library/ambiguity.dic
2019-09-12 15:59:41,615  INFO org.ansj.dic.impl.File2Stream 25 - path to stream library/default.dic
2019-09-12 15:59:41,615 ERROR org.ansj.library.DicLibrary 221 - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:D:\MyProject\splitCHword\library\default.dic not found or can not to read, path: library/default.dic
2019-09-12 15:59:42,698  INFO org.ansj.library.DATDictionary 90 - init core library ok use time : 977
2019-09-12 15:59:43,170  INFO org.ansj.library.NgramLibrary 17 - init ngram ok use time :469
2019-09-12 15:59:43,180  INFO org.apache.spark.executor.Executor 54 - Finished task 0.0 in stage 1.0 (TID 1). 1700 bytes result sent to driver
2019-09-12 15:59:43,182  INFO org.apache.spark.scheduler.TaskSetManager 54 - Finished task 0.0 in stage 1.0 (TID 1) in 1900 ms on localhost (executor driver) (1/1)
2019-09-12 15:59:43,182  INFO org.apache.spark.scheduler.TaskSchedulerImpl 54 - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-09-12 15:59:43,183  INFO org.apache.spark.scheduler.DAGScheduler 54 - ResultStage 1 (show at SplitCHword.java:53) finished in 1.899 s
2019-09-12 15:59:43,186  INFO org.apache.spark.scheduler.DAGScheduler 54 - Job 1 finished: show at SplitCHword.java:53, took 1.939794 s
2019-09-12 15:59:43,221  INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator 54 - Code generated in 16.789319 ms
2019-09-12 15:59:43,247  INFO org.apache.spark.SparkContext 54 - Invoking stop() from shutdown hook
2019-09-12 15:59:43,253  INFO org.spark_project.jetty.server.AbstractConnector 310 - Stopped Spark@2ed3b1f5{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
2019-09-12 15:59:43,256  INFO org.apache.spark.ui.SparkUI 54 - Stopped Spark web UI at http://10.6.1.14:4042
2019-09-12 15:59:43,273  INFO org.apache.spark.MapOutputTrackerMasterEndpoint 54 - MapOutputTrackerMasterEndpoint stopped!
2019-09-12 15:59:43,333  INFO org.apache.spark.storage.memory.MemoryStore 54 - MemoryStore cleared
2019-09-12 15:59:43,334  INFO org.apache.spark.storage.BlockManager 54 - BlockManager stopped
2019-09-12 15:59:43,336  INFO org.apache.spark.storage.BlockManagerMaster 54 - BlockManagerMaster stopped
2019-09-12 15:59:43,340  INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint 54 - OutputCommitCoordinator stopped!
2019-09-12 15:59:43,345  INFO org.apache.spark.SparkContext 54 - Successfully stopped SparkContext
2019-09-12 15:59:43,347  INFO org.apache.spark.util.ShutdownHookManager 54 - Shutdown hook called
2019-09-12 15:59:43,348  INFO org.apache.spark.util.ShutdownHookManager 54 - Deleting directory C:\Users\OUY\AppData\Local\Temp\spark-db8f4b3b-f8ff-41d4-ad9c-914ab1fb9b34
